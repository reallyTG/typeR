package_name, function_name, parameter_position, outer_alternative, inner_alternative, type, count
"tokenizers", "check_input", -1, 0, 0, "null", 1
"tokenizers", "check_input", 0, 0, 0, "character[]", 1
"tokenizers", "check_input", 0, 0, 1, "list<character>", 1
"tokenizers", "chunk_individual_text", -1, 0, 0, "list<character>", 1
"tokenizers", "chunk_individual_text", 0, 0, 0, "character", 1
"tokenizers", "chunk_individual_text", 1, 0, 0, "double", 1
"tokenizers", "chunk_individual_text", 2, 0, 0, "character", 1
"tokenizers", "chunk_individual_text", 3, 0, 0, "...", 1
"tokenizers", "chunk_text", -1, 0, 0, "list<character>", 1
"tokenizers", "chunk_text", 0, 0, 0, "character", 1
"tokenizers", "chunk_text", 1, 0, 0, "double", 1
"tokenizers", "chunk_text", 2, 0, 0, "character", 1
"tokenizers", "chunk_text", 3, 0, 0, "...", 1
"tokenizers", "count_characters", -1, 0, 0, "integer", 1
"tokenizers", "count_characters", 0, 0, 0, "character", 1
"tokenizers", "count_sentences", -1, 0, 0, "integer", 1
"tokenizers", "count_sentences", 0, 0, 0, "character", 1
"tokenizers", "count_words", -1, 0, 0, "integer", 1
"tokenizers", "count_words", 0, 0, 0, "character", 1
"tokenizers", "generate_ngrams_batch", -1, 0, 0, "list<character[]>", 1
"tokenizers", "generate_ngrams_batch", 0, 0, 0, "list<character[]>", 1
"tokenizers", "generate_ngrams_batch", 1, 0, 0, "double", 1
"tokenizers", "generate_ngrams_batch", 2, 0, 0, "double", 1
"tokenizers", "generate_ngrams_batch", 3, 0, 0, "character[]", 1
"tokenizers", "generate_ngrams_batch", 4, 0, 0, "character", 1
"tokenizers", "simplify_list", -1, 0, 0, "character[]", 1
"tokenizers", "simplify_list", -1, 0, 1, "list<character[]>", 1
"tokenizers", "simplify_list", 0, 0, 0, "list<character[]>", 1
"tokenizers", "simplify_list", 1, 0, 0, "logical", 1
"tokenizers", "skip_ngrams_vectorised", -1, 0, 0, "list<character[]>", 1
"tokenizers", "skip_ngrams_vectorised", 0, 0, 0, "list<character[]>", 1
"tokenizers", "skip_ngrams_vectorised", 1, 0, 0, "list<double[]>", 1
"tokenizers", "skip_ngrams_vectorised", 2, 0, 0, "character[]", 1
"tokenizers", "tokenize_characters", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_characters", 0, 0, 0, "character", 1
"tokenizers", "tokenize_characters", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_characters", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_characters", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_characters.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_characters.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_characters.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_characters.default", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_characters.default", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_character_shingles", 0, 0, 0, "character", 1
"tokenizers", "tokenize_character_shingles", 1, 0, 0, "double", 1
"tokenizers", "tokenize_character_shingles", 2, 0, 0, "any", 1
"tokenizers", "tokenize_character_shingles", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_character_shingles.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_character_shingles.default", 1, 0, 0, "double", 1
"tokenizers", "tokenize_character_shingles.default", 2, 0, 0, "double", 1
"tokenizers", "tokenize_character_shingles.default", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles.default", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_character_shingles.default", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_lines", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_lines", 0, 0, 0, "character", 1
"tokenizers", "tokenize_lines", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_lines.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_lines.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_lines.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_ngrams", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_ngrams", 0, 0, 0, "character[]", 1
"tokenizers", "tokenize_ngrams", 0, 0, 1, "list<character>", 1
"tokenizers", "tokenize_ngrams", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_ngrams", 2, 0, 0, "double", 1
"tokenizers", "tokenize_ngrams", 3, 0, 0, "any", 1
"tokenizers", "tokenize_ngrams", 4, 0, 0, "any", 1
"tokenizers", "tokenize_ngrams", 5, 0, 0, "character", 1
"tokenizers", "tokenize_ngrams", 6, 0, 0, "logical", 1
"tokenizers", "tokenize_ngrams.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_ngrams.default", 0, 0, 0, "character[]", 1
"tokenizers", "tokenize_ngrams.default", 0, 0, 1, "list<character>", 1
"tokenizers", "tokenize_ngrams.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_ngrams.default", 2, 0, 0, "double", 1
"tokenizers", "tokenize_ngrams.default", 3, 0, 0, "double", 1
"tokenizers", "tokenize_ngrams.default", 4, 0, 0, "character[]", 1
"tokenizers", "tokenize_ngrams.default", 5, 0, 0, "character", 1
"tokenizers", "tokenize_ngrams.default", 6, 0, 0, "logical", 1
"tokenizers", "tokenize_paragraphs", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_paragraphs", 0, 0, 0, "character", 1
"tokenizers", "tokenize_paragraphs", 1, 0, 0, "character", 1
"tokenizers", "tokenize_paragraphs", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_paragraphs.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_paragraphs.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_paragraphs.default", 1, 0, 0, "character", 1
"tokenizers", "tokenize_paragraphs.default", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_ptb", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_ptb", 0, 0, 0, "list<character>", 1
"tokenizers", "tokenize_ptb", 0, 0, 1, "character[]", 1
"tokenizers", "tokenize_ptb", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_ptb", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_ptb.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_ptb.default", 0, 0, 0, "list<character>", 1
"tokenizers", "tokenize_ptb.default", 0, 0, 1, "character[]", 1
"tokenizers", "tokenize_ptb.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_ptb.default", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_sentences", 0, 0, 0, "character", 1
"tokenizers", "tokenize_sentences", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_sentences.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_sentences.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences.default", 2, 0, 0, "logical", 1
"tokenizers", "tokenize_sentences.default", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_skip_ngrams", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_skip_ngrams", 0, 0, 0, "character", 1
"tokenizers", "tokenize_skip_ngrams", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_skip_ngrams", 2, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams", 3, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams", 4, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams", 5, 0, 0, "any", 1
"tokenizers", "tokenize_skip_ngrams", 6, 0, 0, "logical", 1
"tokenizers", "tokenize_skip_ngrams.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_skip_ngrams.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_skip_ngrams.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_skip_ngrams.default", 2, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams.default", 3, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams.default", 4, 0, 0, "double", 1
"tokenizers", "tokenize_skip_ngrams.default", 5, 0, 0, "character[]", 1
"tokenizers", "tokenize_skip_ngrams.default", 6, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_tweets", 0, 0, 0, "character", 1
"tokenizers", "tokenize_tweets", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets", 2, 0, 0, "null", 1
"tokenizers", "tokenize_tweets", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_tweets.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_tweets.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets.default", 2, 0, 0, "null", 1
"tokenizers", "tokenize_tweets.default", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets.default", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_tweets.default", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_words", -1, 0, 0, "character[]", 1
"tokenizers", "tokenize_words", -1, 0, 1, "list<character[]>", 1
"tokenizers", "tokenize_words", 0, 0, 0, "character[]", 1
"tokenizers", "tokenize_words", 0, 0, 1, "list<character>", 1
"tokenizers", "tokenize_words", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_words", 2, 0, 0, "null", 1
"tokenizers", "tokenize_words", 2, 0, 1, "character[]", 1
"tokenizers", "tokenize_words", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_words", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_words", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_words.default", -1, 0, 0, "character[]", 1
"tokenizers", "tokenize_words.default", -1, 0, 1, "list<character[]>", 1
"tokenizers", "tokenize_words.default", 0, 0, 0, "character[]", 1
"tokenizers", "tokenize_words.default", 0, 0, 1, "list<character>", 1
"tokenizers", "tokenize_words.default", 1, 0, 0, "logical", 1
"tokenizers", "tokenize_words.default", 2, 0, 0, "null", 1
"tokenizers", "tokenize_words.default", 2, 0, 1, "character[]", 1
"tokenizers", "tokenize_words.default", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_words.default", 4, 0, 0, "logical", 1
"tokenizers", "tokenize_words.default", 5, 0, 0, "logical", 1
"tokenizers", "tokenize_word_stems", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_word_stems", 0, 0, 0, "character", 1
"tokenizers", "tokenize_word_stems", 1, 0, 0, "character", 1
"tokenizers", "tokenize_word_stems", 2, 0, 0, "null", 1
"tokenizers", "tokenize_word_stems", 3, 0, 0, "logical", 1
"tokenizers", "tokenize_word_stems.default", -1, 0, 0, "list<character[]>", 1
"tokenizers", "tokenize_word_stems.default", 0, 0, 0, "character", 1
"tokenizers", "tokenize_word_stems.default", 1, 0, 0, "character", 1
"tokenizers", "tokenize_word_stems.default", 2, 0, 0, "null", 1
"tokenizers", "tokenize_word_stems.default", 3, 0, 0, "logical", 1
