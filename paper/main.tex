\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings,xspace,wrapfig}
\lstset{language=R}
\usepackage{my_style}
\definecolor{LightGray}{rgb}{.95,.95,.95}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\graphicspath{ {./plots/} }

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray},
 % numbers=left
}

\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={env, equal, c, runif, trace, args},
  breaklines=true
}

\lstdefinestyle{Rin}{ %
  style=R,
  numberstyle=none,
  basicstyle=\normalsize\ttfamily,
  breaklines=false
}

\newcommand{\code}[1]{\lstinline|#1|\xspace}
\newcommand{\genthat}{{\sc Genthat}\xspace}

% Macros for type names?
\newcommand{\reals}{$\mathtt {\rm I\!R}$\xspace}
\newcommand{\realv}{$\overline{\rm I\!R}$\xspace}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A Large-scale Study of Polymorphism in R}

\newcommand{\PACKAGES}{11,463\xspace}
\newcommand{\PROGRAMMERS}{?\xspace}
\newcommand{\PERCENTCRAN}{83\%\xspace}
\newcommand{\CRANTOTAL}{13,841\xspace}
\newcommand{\RLOC}{15,050,267\xspace}
\newcommand{\CLOC}{9,373,542\xspace}
\newcommand{\YEARS}{?\xspace}
\newcommand{\INDEXCOINCIDENCE}{3,499\xspace}
\newcommand{\TOTALINDEXY}{6,561\xspace}
\newcommand{\INDEXYPERC}{53\%\xspace}		% TODO Not an accurate count.
\newcommand{\DATAPKGS}{206\xspace}
\newcommand{\DATAPKGSPERC}{1.5\%\xspace}
\newcommand{\METAARGCOUNT}{7,051\xspace}
\newcommand{\NUMPOLYTYPEMONOCLASS}{1,357\xspace}

\newcommand{\attr}[2]{\ensuremath{#1_{\mathtt{#2}}}\xspace}
\newcommand{\attrclass}[3]{\ensuremath{#1^{\mathtt{#3}}_{\mathtt{#2}}}\xspace}
\renewcommand{\to}{\ensuremath{\rightarrow}\xspace}
\newcommand{\D}{\ensuremath{\small\vec{\mathtt D}}\xspace} % Double
\newcommand{\I}{\ensuremath{\small\vec{\mathtt I}}\xspace} % Integer
\renewcommand{\C}{\ensuremath{\small\vec{\mathtt C}}\xspace} % Character
\renewcommand{\L}{\ensuremath{\small\vec{\mathtt L}}\xspace} % Logical
\newcommand{\R}{\ensuremath{\small\vec{\mathtt R}}\xspace} % Raw
\newcommand{\X}{\ensuremath{\small\vec{\mathtt X}}\xspace} % Complex
\newcommand{\Y}{\ensuremath{\small\vec{\mathtt Y}}\xspace} % Symbol
\newcommand{\sY}{\ensuremath{\small{\mathtt Y}}\xspace} % Symbol
\newcommand{\sS}{\ensuremath{\small{\mathtt S}}\xspace} % S4
\newcommand{\sF}{\ensuremath{\small{\mathtt F}}\xspace} % Closure
\newcommand{\sE}{\ensuremath{\small{\mathtt E}}\xspace} % Env
\renewcommand{\R}{\ensuremath{\small\vec{\mathtt R}}\xspace} % Raw
\newcommand{\sN}{\ensuremath{\small{\mathtt N}}\xspace}     % Null
\renewcommand{\l}{\ensuremath{\small\underline{\mathtt ?}}\xspace}     % List
\newcommand{\sD}{\ensuremath{\small{\mathtt D}}\xspace} % Double
\newcommand{\sI}{\ensuremath{\small{\mathtt I}}\xspace} % Integer
\newcommand{\sC}{\ensuremath{\small{\mathtt C}}\xspace} % Character
\newcommand{\sL}{\ensuremath{\small{\mathtt L}}\xspace} % Logical
\newcommand{\sX}{\ensuremath{\small{\mathtt X}}\xspace} % Complex
\newcommand{\sR}{\ensuremath{\small{\mathtt R}}\xspace} % Raw
\newcommand{\ANY}{\ensuremath{\small{\mathtt ?}}\xspace}     % Any
\newcommand{\lT}[1]{\ensuremath{\small\underline{\mathtt #1}}\xspace}     % Any

\begin{abstract}
The R programming language is widely used in a variety of scientific domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in \RLOC lines of R
code spread among some \PACKAGES packages, written over a
period of over \YEARS years by \PROGRAMMERS programmers.  We perform a dynamic
analysis, leveraging tests and use-cases, to determine the level of
polymorphism that is present in the code. We do this for several potential
notions of types. Our result suggest that polymorphism is important in some
key parts of the system but that relatively simple type annotations could be
used to capture most of the interesting cases.

\end{abstract}

\maketitle

\section{Introduction}

Our community builds, improves, and reasons about programming languages.  To
make design decisions that benefit most users, we need to understand the
language we are working with as well as the real-world needs it
answers. Often, we, as researchers, can appeal to our intuition as many
languages are intended to be general purpose and appeal to users with some
computer science training. Unfortunately, these intuitions don't always
apply to domain-specific languages, languages designed for and by a specific
group of users to solve very specific needs. This is the case of the data
science language R.

R and its ancestor S are languages designed, implemented and maintained by
statisticians. Originally they were designed as glue languages, languages
that would allow to read data into vectors and call statistical routines
written in Fortran. Over three decades, the languages became widely used
across many fields of science and in industry for data analysis and data
visualization; with time additional features were added.  Modern R, as a
linguistic object of study, is fascinating. It is a vectorized, dynamically
typed, lazy functional language with limited side-effects, extensive
reflective facilities and retrofitted object-oriented programming support.

Many of the design decisions that gave us R were intended to foster an
interactive, exploratory, programming style. This includes, to name a few,
the lack of type annotations on variables and functions, the ability to use
syntactic shortcut, and the automatic conversion between data types.  While
these choices have led to a language that is surprisingly easy to use by
beginners --many data science programs do not teach the language itself but
simply introduce some of its key libraries-- they have also created a
language where almost all computations yield a numeric result and where
errors can go undetected.

One way to increase assurance in the results obtained when using R would be
to add type annotations to functions and variable declarations. These
annotations could then be used, either statically or (more likely)
dynamically, to catch mismatches between expected and provided data values.
The nature of R is such that it is unlikely to be ever fully statically
checked, furthermore end users may not be willing to write types when
carrying out exploratory programming tasks. So, we are looking for an
optional type system that would allow us to capture as much of behavior of
library functions as possible while remaining easy to understand for
end-users and library developers alike.

This papers is a data-driven study of what a type system for the R language
could look like. Longer term, our intention is to propose changes to
language, but for any changes to be accepted by the user community, they
must clearly benefit the language without endangering backwards
compatibility. Our goal is thus to find a compromise between simplicity and
usefulness; the proposed type system should cover most common programming
idioms while remaining easy to use. In order to do this, we need to
understand the degree of polymorphism present in R code, that is to say, how
programmers leverage the dynamic nature of R to write code that can accept
arguments of different types.  This understanding will drive our design.

We propose to capture the degree of polymorphism present in R by the means
of a dynamic analysis of widely used libraries. For each function call we
can record the types of its arguments and of its return value. This allows
us to observe how many different combination of types are accepted by any
given function. Unlike many other languages, R has a carefully curated
software repository called CRAN. To be deposited in CRAN, a package must
come with sample dataset, tests and executable use-cases. As part of normal
operations these tests are run regularly and failing packages are removed.
This allowed us to have access to \PACKAGES libraries and about an order of
magnitude more runnable scripts that exercise those libraries.

The contributions of this paper are thus as follows:
\begin{itemize}
\item A large-scale analysis of the polymorphism present in function
  signatures of \PACKAGES widely used and actively maintained R packages.
\item A tracing and analysis pipeline that extends a previously published
  test generation tool named \genthat.
\item Manual analysis of 100 functions to validate the dynamic analysis
  results.
\end{itemize}

One threat to validity of our work is that we rely on dynamic analysis, so
our conclusions are only as good as the coverage of the possibly function
behaviors. Previous work~\cite{issta18}, reported that running all the
scripts that come with CRAN packages gives, on average, 68\% test coverage.
We attempted to mitigate the threat coming from the fact that only part of
the code is being exercised by manual analysis. It would be reasonable to
ask for confirmation of the data by static analysis of the code, but sound
static analysis of R is difficult because of the extensive use of reflective
features such as \code{eval} and of the ability to redefine the meaning of
operators such as \code{+} and \code{if}.  Another threat to validity is
that we only have access to code that has been deposited in the CRAN
repository. While this may bias our findings towards code written to be
reusable and, possibly, better engineered than typical user code. This is
also the code that would most benefit from type annotations.

\newpage  %%Leave here

\section{The R Programming Language}\label{sec:rlang}

Over the last decade, the R Project has become a key tool for implementing
sophisticated data analysis algorithms in fields ranging from Computational
Biology~\cite{R05} to Political Science~\cite{R:Keele:2008}. At the heart of
the R project is a \emph{vectorized, dynamic, lazy, functional,
  object-oriented} programming language with a rather unusual combination of
features~\cite{ecoop12} designed to ease learning by non-programmer and
enable rapid development of new statistical methods.  The language, commonly
referred to as R was designed in 1993 by Ross Ihaka and Robert
Gentleman~\cite{R96} as a successor to S~\cite{S88}.  First released in
1995, under a GNU license, R rapidly became the lingua franca for
statistical data analysis. Today, there are over 13,000 R packages available
from repositories such as CRAN and Bioconductor.  With 55 R user groups
world-wide, Smith~\cite{eco11} estimates that there are over 2 million
end-users.

As an introduction to R, consider the code snippet in Fig.~\ref{sample} from
a top-level interaction where the user defines a function \code{normSum}
that accepts vectors of integers, logicals, doubles and complex values and
normalizes the vector with respect to its sum and rounds the results. The
function definition does not require type annotations, and all operations
transparently work on vectors of any length and different types.

\begin{figure}[!hb]{\small
\begin{lstlisting}[style=R]
> normSum <- function( m )  round( m / sum(m), 2)
> normSum(c(1L,3L,6L))
[1] 0.1 0.3 0.6
> normSum(c(1.1,3.3,6.6))
[1] 0.1 0.3 0.6
> normSum(c(1.6,3.3,6.1))
[1] 0.15 0.30 0.55
> normSum(complex(r=rnorm(3),i=rnorm(3)))
[1] 0.49+0.21i 0.30-0.18i 0.22-0.03i
\end{lstlisting}}
\caption{Sample R code}\label{sample}
\end{figure}

In R, function can be called with named parameters, R support variable
argument lists, and arguments can have default values. Putting all of these
together consider the following declaration:

\begin{lstlisting}[style=R]
f <- function(x, ..., y=3) x + y
\end{lstlisting}

\noindent
Function \k{f} can be called with a single argument \code{f(3)}, with named
argument \code{f(y=4,x=2)} and with a variable number of arguments,
\code{f(1,2,3,4,y=5)}, all of these calls will return \code{6}.

R has a number of features that are not crucial to the present
discussion. We will mention some of them here for completeness.  In R, data
structures are reference counted and have copy-on-write semantics, thus the
assignment \code{x[12]<-3} results in an update to a copy of \code{x} unless
the reference count on that object is 1.  This semantics gives R a
functional flavor while allowing updating in place within loops (the first
update copies, subsequent updates are performed on the copy). Arguments to
functions are evaluated only when needed, they are bundled in so-called
promises which package the original expression (as an AST), its environment
as well as the result of evaluating the expression. Promises can be
leveraged for meta-programming as it is possible to retrieve the text of a
promise and evaluate that in a different environment.

\subsection{Types of Data}

Before attempting to define a type system for R, we should understand the
different kinds of values that programs operate on.  As we will see
different notions of type may emerge depending on how granular we want to
be.

\renewcommand{\k}[1]{{\tt #1}\xspace}

R has one builtin notion of type that can be queried by the \k{typeof}
function. Over the years, programmers have found the need for a richer type
structure and have added attributes. The best way to think of attributes is
as an optional map from name to values that can be attached to any object.
Attributes are used to encode various type structures. They can be queried
with functions such as \k{attributes} and \k{class}.

\begin{wrapfigure}{r}{6.1cm}
\footnotesize\begin{tabular}{l|c|l@{}}\hline
\multicolumn{3}{l}{\bf Vectorized data types:}  \\\hline
\k{logical}   & \L & vector of boolean values\\
\k{integer}   & \I & vector of 32 bit integer values\\
\k{double}    & \D & vector of 64 bit floating points\\
\k{complex}   & \X & vector of complex values\\
\k{character} & \C & vector of strings values\\
\k{raw}       & \R & vector of bytes\\
\k{list}      & \l & vector of values of any type\\\hline
\multicolumn{3}{l}{\bf Scalar data types:}\\\hline
\k{NULL}      & \sN &  singleton null value\\
\k{S4}        & \sS &  instance of a S4 class \\
\k{closure}   & \sF & a function with its environment\\
\k{environment}&\sE &  a mapping from symbol to value \\\hline
\multicolumn{3}{l}{\bf Implementation data types:}\\\hline
\multicolumn{3}{l}{\k{special},
\k{builtin},
\k{symbol} (\sY),
\k{pairlist},
\k{promise}}\\
\multicolumn{3}{l}{
\k{language},
\k{char},
\k{...},
\k{any},
\k{expression},
}\\
\multicolumn{3}{l}{
\k{externalprt},
\k{bytecode},
\k{weakref}}\\\hline
\end{tabular}\caption{Builtin Types}\label{types}\end{wrapfigure}

Figure~\ref{types} lists all of the builtin types that are provided by the
language. They are the possible return values of \k{typeof}. There is no
intrinsic notion of subtyping in R. But, in many context a \k{logical} will
convert to \k{integer}, and an \k{integer} will convert to \k{double}.  Some
off conversion can occur in corner cases, such as \k{1<"2"} holds and
\k{c(1,2)[1.6]} returns the first element of the vector, as the double is
converted to an integer. R does not distinguish between scalars and vectors
(they are all vectors), so \code{typeof(5) ==} \code{typeof(c(5)) ==
  typeof(c(5,5))} \code{ == "double"}. Finally all vectorized data types have a
distinguished missing value denoted by \code{NA}. The default type of
\code{NA} is \k{logical}. We can see that \code{typeof(NA)=="logical"}, but
NA inhabits every type: \code{typeof(c(1,NA)[2])=="double"}.

With one exception all vectorized data types are monomorphic, the exception
is the \k{list} type which can hold values of any other type including
\k{list}. For all monomorphic data types, attempting to store a value of a
different type will cause a conversion. Either the value is converted to the
type of vector, or the vector is converted to the type of the value.

Scalar data types include the distinguished \k{NULL} value, which is also of
type \k{NULL}, instance of classes written using the S4 object system,
closures and environments.  The implementation of R has a number of other
types that are mostly not used by user code, they are listed in
Figure~\ref{types} for reference.

The addition of attributes lets programmers extend the set of types by
tagging data with user-defined attributes. For example, one could define a
vector of four values, \code{x<-c(1,2,3,4)} and then attach the attribute
\k{dim} with a pair of numbers as value: \code{attr(x,"dim")<-c(2,2)}.  From
that point, arithmetic functions will treat \k{x} as a 2x2 matrix. Another
attribute that can be set is the \k{class}.  This attribute can be bound to
a list of class names. For instance, \code{class(x)<-"human"}, set the class
of \k{x} to be \k{human}.  Attributes are thus used for object-oriented
programming. The S3 object system support single dispatch on the class of
the first argument of a function, whereas the S4 object system allows
multiple dispatch (on all arguments). Some of the most widely used data
type, such as data frames, leverage attributes. A data frame, for instance,
is a list of vectors with a class and a column name attribute.

\paragraph{Summary.} The most common values in R computations are vectorized
types. R programs do not have a way to constrain values to be scalar.
\k{NULL} is sometimes used to represent the case when no value is
available. \k{NA} is used within vector to represent missing observations.
Attributes can decorate values and are used as building blocks for
object-oriented programming. A potential type system for R could focus only
on the builtin types, if one wanted to strive for simplicity, or it could
try to capture attributes at the risk of increased complexity.

\newpage
\section{Corpus}\label{sec:corpus}

In this section, we present our dataset. The R language aims to accommodate
data analysts; their workflows start with data import, followed by cleaning,
and then by steps of modeling, transformation and visualization. Often, the
code of these analysis pipelines resides, together with the data and
results, in notebooks. Few notebooks are publicly shared, and when they are,
the data isn't. For this reason our analysis focuses on packages which
bundle reusable units of R code with documentation, sample data and
use-cases.

We focus on packages hosted on the \emph{Comprehensive R Archive Network} or
CRAN.  With over 13,000 packages, CRAN is the largest repository of software
written in R. It is experiencing sustained growth with an average of size
new packages a day~\cite{LIgges2017}.  Unlike sites like GitHub, CRAN is a
\emph{curated} collection: A package is only accepted to CRAN if it abides
by a number of well-formedness rules.  Most relevant for our purposes,
packages must have data, examples, vignettes and tests, all of which must
successfully run. From our perspective this means that each package in CRAN
comes with several executable scripts that exercise some of its
functionality.  Notable exceptions to this rule are packages only containing
data, which have no runnable code but are referenced by other packages.
Only \DATAPKGS packages had no executable code, accounting for \DATAPKGSPERC
of CRAN.

The corpus used in this paper is a subset of CRAN. We retained packages that
could be run by our infrastructure in less than one hour. This corpus
consists of \PACKAGES packages, accounting for some \PERCENTCRAN of all
packages.  These packages have a total of \RLOC lines of R code and \CLOC
lines of C code. Figure~\ref{allcloc} shows a per-package breakdown of the
size of each package sorted by increasing numbers of lines of R. The figure
suggests that there is little correlation between the C/C++ parts and how
many lines of R a package contains. The median size of a package is 541
lines of R code and the largest package has 86K LOC. Typically, C/C++ code
is used to implement performance critical portions of the code. The majority
of package, 8,375 to be precise, have no C or C++ code.  The remaining 3,078
packages, have a median 572 lines of C and the largest package has 385,839
lines of C.

\begin{figure}[!b]\begin{center}
\includegraphics[width=.9\textwidth]{linesofrandccode}
\caption{Lines of code (log scal); for each package, R is above 0, and C/C++
  below}\label{allcloc}\end{center}
\end{figure}


For each package, we extracted all executable code snippets from
documentation, vignettes and tests and ran them independently recording all
calls to R functions.  It is noteworthy that in order to run the scripts in
one package, it is often necessary to load a number of other packages.
In~\cite{issta18}, the authors estimated code coverage to be around 68\%
when including reverse dependencies.  As our infrastructure adds overhead to
script execution, coupled with the fact that some scripts take inordinate
amounts of time to run, this is why we limited our analysis of any given
package to one hour.

% https://www.r-pkg.org/downloaded
\begin{figure}[!th]{\footnotesize\begin{tabular}{@{}r||l|r|r|r|r|r@{}}\hline
\bf Package & \bf Description & \bf R LOC &\bf C LOC &\bf Scripts & \bf Calls Observed & \bf Calls Recorded \\
\hline
\tt Rcpp  & Seamless C++ integration & 2.2K & 4.2K & 25 & 55K & 340 \\
\tt rlang & Functions for 'Tidyverse' & 7.0K & 6.1K & 122 & 3,924K & 8,422 \\
\tt glue  & Interpreted string literals & 0.3K & 0.3K & 8 & 4K & 145 \\
\tt tibble & Simple data frames & 2.0K & 0.3K & 16 & 1,332K & 6,367 \\
\tt stringi &  String processing & 1.5K & 515K & 64 & 923K & 873 \\
\tt ggplot2 & Data visualisations & 14K & 0 & 130 & 153K & 4,608 \\
\tt dplyr  &  Data manipulation & 4.5K & 4.7K & 78 & 233K & 3,099 \\
\tt pillar & Formatting for columns & 1.4K & 0 & 13 & 803K & 1,514 \\
\tt R6 & Classes w. ref. semantics & 0.7K & 0 & 2 & 1K & 330 \\
\tt stringr & String operations & 0.5K & 0 & 32 & 1,764K & 534 \\
\end{tabular}}\caption{10 Most Downloaded Packages.}\label{most}
\end{figure}

Figure~\ref{most} shows the ten most downloaded CRAN packages.  For each
one, we list how many lines of R and C/C++ the packages contains.  We show
the number of scripts that could be extracted from the package. Each script
corresponds to either one use-case or a set of unit tests. We print the
number of function calls that were observed by our infastructure, and the
number of unique signatures that were recorded.

Our infrastructure only retains unique argument/return combinations. Thus,
while we observe large number of functions being called with different
values, the types of these function call are often similar. Over the entire
corpus, we can see the relation between observed and recorded calls in
Figure~\ref{recorded}.  The median number of observed calls is 82 and
maximum is 19 million.  The median number of recorded signatures is 16 and
the maxium is 8,422. These numbers are skewed by a number of scripts doing
very few calls before plunging into C code.

\begin{figure}[htbp]\begin{center}
\includegraphics[width=.9\textwidth]{recordsbypkg}
\caption{Numbers of Recorded Function Invocations in the Analyzed Corpus}
\label{recorded}\end{center}
\end{figure}

\newpage
\section{Methods}

In this section, we detail our methodology for collecting data.  Our aim is
to observe arguments and return values of function calls, and from these
generalize possible type signatures for the called functions.  We base our
infrastructure on an open source tool called \genthat whose purpose is to
generate unit tests for R libraries~\cite{issta18}.  \genthat achieves this
by synthesizing unit tests from recorded function argument and return
values, comparing the test against existing ones to avoid generating tests
which do not increase code coverage.  To suit our purposes, we change the
existing tool in two main ways: we record \emph{shapes} rather than values,
and ignore the code coverage optimization phase.  Both changes are
beneficial for scalability, allowing us to trace far more calls.


To illustrate our approach, consider the following script which adds a
double to an integer, and then creates a matrix from a vector, finally
adding them together to get a new matrix.

\begin{figure}[!hb]
\begin{tabular}{ll}\begin{minipage}{5cm}
{\small\begin{lstlisting}[style=R]
> 1 + 2L
[1] 3
> x <- c(1,2,3,4)
> attr(x,"dim") <- c(2,2)
> x + c(1,2)
     [,1] [,2]
[1,]    2    4
[2,]    4    6
\end{lstlisting}}
\end{minipage} &
\begin{minipage}{8cm}\small
\begin{tabular}{rl}
\tt `+`: &\tt \sD \sI \to \sD \\
\tt c:& \sD \sD \sD \sD \to \D\\
\tt `<-`: &\tt \sY \D \to \D\\
\tt c: & \tt \sD \sD \to \D\\
\tt `attr<-`:&\tt  \sY \C \D \to \\
\tt c: &\tt \sD \sD \to \D\\
\tt `+`: &\tt \attr\D{dim=\D} \D \to \attr\D{dim=\D}
\end{tabular}
\end{minipage}
\end{tabular}
\caption{Example script and recorded signature}\label{example}\end{figure}

The functions being called here are \k{`+`}, \k{`<-`}, \k{c} and
\k{`attr<-`}.  The shapes we would expect to record are as follows:
In the above we abreviate types, \k{double}, \k{integer}, \k{symbol} and
\k{character}. For vectorized types we record their length and for all types
we record attributes with some of their values. Looking at the signatures
observed for addition, it is clear that the function is polymorphic as it
starts with the addition of two scalar numbers of different types, and then
adds a matrix to a vector returning a matrix.

\subsection{Implementation}

A high-level description of the workflow of our tool for one package
retrieved from CRAN is as follows:

\begin{enumerate}
\item {\bf Exec generation:} All runnable code in the package is extracted
  from its tests, examples and vignettes. The code snippets are combined
  into a single file.
\item {\bf Installation:} All packages that are required for execution of
  the current package are downloaded and installed.
\item {\bf Instrumentation:} As code is loaded into the R, every function
  definition is instrumented with an \code{on-exit} hook which is invoked
  when the function returns either normally or through an exception.
\item {\bf Recording:} When a hook is called, arguments and return value are
  inspected. We record \k{typeof}, \k{class} and \k{attributes} recursively.
  For \code{list} values, an extra bit of analysis is performed to record
  the element type.
\item {\bf Writing:} Unique signatures are recorded to file with information
  about which package triggered the recording.
\end{enumerate}

Our recording mechanism does not remember the order in which arguments were
passed, nor does it record which arguments were not passed (and for which
default values were used). If an argument was not passed, and no default
value was specified, we record it as \emph{missing}. In R, trying to use
such a missing value results in an error. For practical reasons, we do not
record the contents of environments. These can be used as hash tables and
may be big and are quite likely different from one another.

In R all arguments are passed as promises, and unused arguments will be
unevaluated.  Our tracer does not collect information on arguments that were
not evaluated as they may not be valid expressions.  Consider the {\tt
  magrittr} package, which implements the pipe function that performs a kind
of currying.

{\small\begin{lstlisting}[style=R]
mul <- function(x,y)  x*y
4 %>% mul(3)
\end{lstlisting}}

The call to the pipe desugars to \code{`\%>\%`(lhs=4, rhs=mul(3))} where the
\code{rhs} is not a valid expression. It is treated as code that will
manipulated to add one argument (the \code{lhs}).

To be as specific as possible, our analysis collects shape information about
arguments and return values. The details of the information we collect follows.

\begin{itemize}
\item If the analysis encounters a primitive vector, say
  \code{typeof(x)=="double"}, it records the length of {\tt x} to determine
  if it can be treated as a scalar.
\item For a list, \code{typeof(x)=="list"}, shape information on all
  elements is used to describe the list's shape.  To avoid undue slowdowns,
  we only collect the content's \code{typeof}, and ascribe \ANY (any) if the
  elements are not of the same kind.k
\item If the analysis encounters a scalar \code{NA}, we ascribe a unique
  NULL type.  In R, \code{NA} inhabits all types, but for a scalar NA,
  \code{typeof(NA)=="logical"}. Scalar NAs are used as uninitialized values.
\item For a matrix (i.e. the value has matrix class and has a dims and
  optionally dimnames attribute), we ascribe the shape \attr{\tt T}{mat}
  where {\tt T} is the primitive element type.
\item For a data frame (i.e., class {\tt data.frame} with appropriate
  attributes), we ascribe the shape \attr{\l}{df}.
\end{itemize}

The infrastructure memoizes emitted signatures to avoid writing multiple
identical signatures for the same function.

\section{Types for R}

For a meaningful notion of polymorphism, it is necessary to settle on a set
of basic types for R. There are several possible choices which lead to
increasingly more expressive type systems. The simplest notion of types
aligns with R's builtin types. Richers notions incorporate attributes and
the various object systems built on top of those. For concreteness we will
pick out three different sets of types.

In the following we say that an argument is {\it polymorphic in type at
  level L} if it has been called with at least two values that belong to
distinct types at level \emph{L}. This definition extends naturally to
classes, data frames, and matrices (as these are simple attributes).
Generalized attribute polymorphism is a bit trickier as values can have a
large and changing set of attributes.

\subsection{L0 Types}

The logical ``baseline'' for the design of a type system for R is to adopt
the language's builtin types.  Types in this system, which we call {\bf L0}
reflect the types of values in the runtime environment, the \code{typeof} an
object is in direct correspondence with the runtime type tag of the value.
Thus we include the following types: vectors of primitive types (integers
\I, double \D, character \C, logical \L, raw \R, complex \X), lists of any
type (\l), and scalar values (environment \sE, closures \sF, symbols \sY, S4
object \sS, and null \sN). We add one type stand in for unevaluated values,
the type any (written \ANY). In {\bf L0} subtyping follows from the
conversion rules of the language.  Logicals are subtypes of integers, which
are subtypes of doubles, \L <: \I <: \D.  All types are subtypes of \ANY,
thus {\tt T}<:\ANY for an type {\tt T}.

\subsection{L1 Types}

{\bf L0} is limited in expressivity as it supports polymoprhism for list in
a trivial way, it does not address some of the popular constructed data
types such as matrices and data frames, and cannot differentiate between
scalar values and vectors.  {\bf L1} types build on and extend the previous
level with with scalar values for primitive types (\sI, \sD, \sC, \sL, \sR,
and \sX), type parametric lists (written \lT{T} for any type {\tt T}), data
frames (written \attr{\l}{df} as they are built up from lists of type any),
and matrices (\attr{\vec{\tt T}}{mat} where {\tt T} is a primitive type).
Subtyping is extended such that {\tt T}<:{$\vec{\tt T}$} for any primitive
type {\tt T}. There is no subtyping relation between lists of different
types.

\subsection{L2 Types}

{\bf L2} extends the previous system with more support for attributes.
While there is a finite number of results from the {\tt typeof} function,
this is not the case for \code{class} and \code{attributes}.  Classes and
attributes are user-defined.  Of course, not all classes are user-defined,
and R has a number of built-in or otherwise standard classes, ranging from
truly primitive classes such as {\tt numeric} to the more complex {\tt
  data.frame}.  We have already used the latter in {\bf L1}. We will ignore
those classes and thus highlight user-defined ones.  As for attributes,
again some patterns arise naturally, for instance all matrices have a {\tt
  dims} attribute (indeed, it's this attribute that makes a matrix a
matrix).  Attributes are name-value pairs, to keep things simple we reduce
these to names. The only attribute that we treat specially is the {\tt
  class} for those we will recurse and capture the set of values defined by
the user (these are the class names for the object).  Thus, we extend {\bf
  L1} with attribute lists and classes: \attrclass{T}{C}{N} where {\tt T}
can be any L1 type other than data frame or matrix, {\tt C} and {\tt N}
stands for a list names.  The subtype relation is extended with
\attrclass{T}{C}{N}<:\attrclass{T}{C'}{D'} if {\tt C} $\subseteq$ {\tt C'}
and {\tt N}$\subseteq${\tt N'}.



\section{Analysis Results}\label{sec:results}

This section presents our results for each of the levels we have considered.

\subsection{L0 polymorphism}

At L0, the type system is mostly aligned with the types of the core language.  We
have observed over 186K monomorphic functions; this accounts for 86\% of our
corpus.  In terms of argument positions, 95\% of arguments are monomorphic.

Table~\ref{tab:L0argcounts} shows the polymorphicity of argument positions
 in R function.  From there, we build up a picture of the polymorphicity of
 functions as a whole, and the data for this function polymorphism can be
 found in Table~\ref{tab:L0funcounts}.

\begin{table}[ht]\label{tab:L0argcounts}\centering\begin{tabular}{lrr}  \hline
Type                   & Count & Percentage \\  \hline
  Monomorphic in Type & 845K & 92.46 \\
  Total Seen & 914K & --- \\
   \hline
\end{tabular}
\caption{Argument position polymorphism in L0.}
\end{table}

\begin{table}[ht]
\label{tab:L0funcounts}
\centering
\begin{tabular}{lrr}
  \hline
Type & Count & Percentage \\
  \hline
%  Full Monomorphic & 145K & 66.77 \\
  Monomorphic in Type & 187K & 86.4\% \\
%  Monomorphic in Class & 163K & 75.21 \\
%  Monomorphic in Attribute Pattern & 161K & 74.37 \\
  Total Seen & 216K & --- \\
   \hline
\end{tabular}
\caption{Function polymorphism in L0.}
\end{table}

Tables~\ref{tab:L0argcounts} and~\ref{tab:L0funcounts} show that the overall amount of polymorphism in R is quite low, but that doesn't quite tell the whole story.
To get an idea of what kinds of polymorphism occur, we will look at the most commonly occurring polymorphic argument signatures.
Table~\ref{tab:L0toppoly} has the top polymorphic signatures for L0.

\begin{table}[ht]\label{tab:L0toppoly}\centering
\begin{tabular}{lrr}  \hline
type & count & perc \\
\hline
  \D, \sN & 8301 & 17.33 \\
  \D, \l & 6291 & 13.13 \\
  \C, \D & 4338 & 9.06 \\
  \C, \sN & 4162 & 8.69 \\
  \l, \sN & 2912 & 6.08 \\
  \C, \l & 1836 & 3.83 \\
  \C, \I & 1719 & 3.59 \\
  \C, \D, \sN & 1057 & 2.21 \\
  \D, error & 906 & 1.89 \\
  \I, \sN & 797 & 1.66 \\   \hline
\end{tabular}
\caption{Top polymorphic signatures for the L0 type system.}
\end{table}

\AT{TODO explain results in table.}

%Some entries in the Table~\ref{tab:L0toppoly} are clear (e.g., double,
%integer polymorphism is unsurprising), others are strange (e.g., character,
%double), but in fact they all have some explanation.

%\begin{enumerate}
%\item \{\D, \I\}: in R, doubles and integers are effectively
%  interchangeable, as casting from one to the other is straightforward and
%  automatically performed when appropriate.  For example, \code{c(1, 2)[1.9]
%    == 1}.  This distinction is mainly just relevant to the implementation,
%  as integers and doubles are stored and dealt with differently in the
%  runtime but programmers only see the distinction when printing.
%
%\item \{\D, \l\} and \{\C, \l\}: at first glance, these type signatures are
%  suspect.  Why would so many programmers want to pass a double or a list to
%  some function?  The key insight here is that doubles and characters are
%  {\it vectorized} in R, meaning that a double is really a vector of
%  doubles.  Thus, these seemingly strange signatures are revealed to be
%  entirely reasonable: these signatures are likely to describe arguments
%  which are either vectors or lists.
%
%\item \{\ANY,\sN\}: in most languages, NULL is an inhabitant of all types,
%  and we see that this is not the case in R.  That said, there is a pattern
%  of many functions having default or optional arguments, and in many of
%  these cases NULL is the default value.
%
%\item \{\D,\L\}: the first values of type logical that come to mind are TRUE
%  and FALSE, which does little to explain this strange signature.  Indeed,
%  the key here is an implementation detail of R's: recall that NAs inhabit
%  every type.  As it happens, the default type of NA is logical (e.g.,
%  \code{typeof(NA) == "logical"}), and this is likely what is being seen
%  here.  We will control for this by distinguishing NAs with their own type,
%  {\tt raw\_NA}, but we will get to that in time.
%
%\item \{\C,\D\}, \{\C,\I\} and \{character, double, integer\}: yet more
%  strange signatures.  These signatures are a little more subtle to explain.
%  For one, indexing lists and vectors can be done with either strings (i.e.,
%  to access a named index) or with numbers.  Another option is dates: R has
%  many ways to present dates, and strings and numbers feature among them.
%  Finally, many \AT{get numbers} functions in R return a string if an error
%  occurs during execution.
%
%\end{enumerate}
%
%How will we deal with this?
%
%\begin{enumerate}
%\item consolidating doubles and integers into a single real-number type
%  would account for all of this polymorphism.
%\item to get an idea of how often vectors and lists co-occur in signatures,
%  we need to expand our analysis of vectorized types.  Notably, when we
%  encounter e.g. a double, we should collect length information to see if
%  the double is a vector of doubles or just a scalar.  This will feature in
%  our next type system, L1, which we will get to momentarily.
%\item if we roll NULLs into every type, we can cleanly deal with these X,
%  NULL signatures.  As we have established that an annotation for an
%  optional argument is useful, this will allow us to reveal other
%  interesting patterns.
%\item similarly with NULLs, we're interested in rolling NAs in to all types.
%  As we mentioned, this is actually what the R runtime does, but a default
%  type needed to be chosen for NAs appearing without context.  Subsuming NAs
%  should help to reveal other interesting patterns.
%\item dealing with this polymorphism will be more tricky.  We might need to
%  pull class information in (to pick up on dates), and possibly make a
%  signature judgment depending on which argument position has the signature
%  (e.g. a \{character, X\} signature on a return).  We will come back to
%  this later.
%
%\end{enumerate}

There are some clear limitations to the L0 type system that we should
address before working through any of these other issues. As we mentioned,
\code{typeof} makes no distinction between vectors and scalars, as indeed
there is no such distinction in R.  But as we saw, we might benefit from
such a distinction, and indeed an all-new runtime environment design for R
might benefit from e.g. not needing to vectorize scalar values.  This is only one
of the modifications we make, all of which are thoroughly explored in
the following section.
%
%
%
%
\subsection{L1}

As we mentioned in \AT{an earlier section}, and as was revealed in the previous section,
L0 is somewhat limited in its expressivity. To address this, we develop L1, which espouses
a finer-grained approach to types. Major modifications from L0 include the differentiation of
scalars and vectors, the parameterization of list types over element types, and the use of
attribute and class information to distinguish types for data frames and matrices.

An example of a signature in this type system can be found in
Figure~\ref{fig:exL1}.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
add_make_list <- function(x, y) {
  l <- as.list(x + y)
  attr(l, "example") <- "this is an attribute"
  l
}
add_make_list(2, c(3, 1)) # => list(5, 3)
\end{lstlisting}}

\begin{tabular}{@{}r|l|l|l@{}}\hline
\bf Argument & \bf Type & \bf Class &\bf Attributes \\
x & \sD & numeric & \{\} \\
y & \D & numeric & \{\} \\
retv & \lT{D} & list & \{\xspace example: \sC\}
\end{tabular}
\caption{Example call and L1 signature.}\label{fig:exL1}\end{figure}

Let us begin as we did in L0 by looking at the amount of polymorphism in L1.
The data can be found in Tables~\ref{tab:argcountsL1}~and~\ref{tab:funcountsL1}.
\AT{TODO: have these tables be generated automatically}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:03:30 2019
\begin{table}[ht]
\label{tab:argcountsL1}
\centering
\begin{tabular}{lrr}
  \hline
 Argument Polymorphism & Count & Percentage \\
  \hline
%  Full Monomorphic & 781K & 85.37 \\
  Monomorphic in Type & 834K & 91.23 \\
%  Monomorphic in Class & 827K & 90.42 \\
%  Monomorphic in Attribute Pattern & 825K & 90.27 \\
  Total Seen & 914K & --- \\
     \hline
\end{tabular}
\caption{Account of {\it argument} polymorphism in L1.}
\end{table}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:10:46 2019
\begin{table}[ht]
\label{tab:funcountsL1}
\centering
\begin{tabular}{lrr}
  \hline
 Function Polymorphism & Count & Percentage \\
  \hline
  Monomorphic in Type & 167K & 77.25 \\
  Total Seen & 216K & --- \\
   \hline
\end{tabular}
\caption{Account of {\it function} polymorphism in L1.}
\end{table}

\AT{We should figure out the pipeline to get the notebook output into here automatically.}

We see that roughly 9\% of all argument positions in L1 are polymorphic in
type, as opposed to 7.5\% in L0.  This small difference is perhaps
surprising, as we did define a number of additional types and might expect
for there to be a lot more polymorphism.  This is explained by the fact that
drawing the distinctions added a lot of polymorphism to already polymorphic
arguments.  \AT{We can get numbers on this.}

As for functions, roughly 23\% are polymorphic in type, or put differently
23\% have at least one polymorphic argument or a polymorphic return.  Again,
the small difference between L0 and L1 in this regard is explained by a
tendency to make already polymorphic functions more polymorphic.

As before, these numbers alone are insufficient, so we ask again: {\it of the polymorphic arguments, what are the most common patterns?}
The answer to that question can be found in Table~\ref{tab:toppolyL1}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 21:27:37 2019
\begin{table}[ht]
\label{tab:toppolyL1}
\centering
\begin{tabular}{lrr}
  \hline
 Polymorphic Type Signature & \# Occurrences & \% of Total Polymorphism \\
  \hline
  \D, \I & 7742 & 9.66 \\
  \sD, \sI & 7201 & 8.99 \\
  matrix, \D & 5384 & 6.72 \\
  NULL, \sC & 2325 & 2.90 \\
  list$<$any$>$, list$<$double$>$ & 2231 & 2.78 \\
  data.frame, matrix & 2116 & 2.64 \\
  NULL, \sD & 2082 & 2.60 \\
  NULL, \D & 2069 & 2.58 \\
  list$<$any$>$, list$<$list$>$ & 1432 & 1.79 \\
  NULL, \C & 1380 & 1.72 \\
     \hline
\end{tabular}
\caption{Top polymorphic argument signatures in L1.}
\end{table}

We would like to make one important note before digesting the data in
Table~\ref{tab:toppolyL1}.  We drew a distinction between scalars and
vectors of the same type, and that distinction would be the single largest
source of polymorphism in L1 had we not dealt with it appropriately.  We
note that an argument which exhibits a signature including e.g. \{\sD, \D\}
really must be an argument designed to take vectors, as recall that unit
length vectors appear as scalars by our definition.  We have already
collapsed this distinction.

In Table~\ref{tab:toppolyL1}, we again see the prominence of {\tt double} and {\tt integer} polymorphism.
As discussed previously, we will address this by creating a new {\it real} type encompassing both.
Refer back to the snippet in Figure~\ref{fig:realex} for details.
Also, another clear pattern in Table~\ref{tab:toppolyL1} is that of NULL, X.
As we discussed, we will roll NULL (and raw NAs) into other types.

%
%
\subsubsection{Scalars}

As we make the distinction between vectors and scalars, one other
interesting question that arises is: {\it how many arguments are strictly
  scalar?}  As it happens, over 40\% of monomorphic arguments with primitive
vectorized types are always scalar (so, 40\% of {\it monomorphic}
occurrences of {\tt scalar/X} and {\tt vector/X} are scalar).  This shows
that there is value in a scalar annotation, which could be used to
communicate to an R runtime that vectorization of a value is not necessary,
which may in turn lead to a performance improvement.

At this point, we have seen a number of reasonable polymorphic patterns that arise in R, both in L0 and L1.
The next type system design we consider will deal with these, and we refer to it as L$1^{+}$.
The number of monomorphic arguments and functions can be found in Tables~\ref{tab:argcountsL1p} and~\ref{tab:funcountsL1p} respectively.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:03:30 2019
\begin{table}[ht]
\label{tab:argcountsL1p}
\centering
\begin{tabular}{lrr}
  \hline
 Argument Polymorphism & Count & Percentage \\
  \hline
  Monomorphic in Type & 869K & 95.01 \\
%  Monomorphic in Class & 831K & 90.31 \\
%  Monomorphic in Attribute Pattern & 830K & 90.25 \\
  Total Seen & 914K & --- \\
     \hline
\end{tabular}
\caption{Account of {\it argument} polymorphism in L$1^{+}$.}
\end{table}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:10:46 2019
\begin{table}[ht]
\label{tab:funcountsL1p}
\centering
\begin{tabular}{lrr}
  \hline
 Function Polymorphism & Count & Percentage \\
  \hline
  Monomorphic in Type & 185K & 85.30 \\
%  Monomorphic in Class & 163K & 75.13 \\
%  Monomorphic in Attribute Pattern & 161K & 74.31 \\
  Total Seen & 216K & --- \\
   \hline
\end{tabular}
\caption{Account of {\it function} polymorphism in L$1^{+}$.}
\end{table}

We had some big wins here, with only ~5\% of argument polymorphism unaccounted for.
To get an idea of what more we can do, let's see what the most frequent remaining polymorphic signatures are.
See Table~\ref{tab:toppolyL1p}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Tue Mar 26 17:46:45 2019
\begin{table}[ht]
\label{tab:toppolyL1p}
\centering
\begin{tabular}{lrr}
  \hline
Polymorphic Type Signature & \# Occurrences & \% of Total Polymorphism \\
  \hline
  matrix, \realv & 6360 & 13.94 \\
  list$<$any$>$, list$<$real$>$ & 2799 & 6.13 \\
  \C, \realv & 2774 & 6.08 \\
  \sC, \reals & 2552 & 5.59 \\
  data.frame, matrix & 2167 & 4.75 \\
  list$<$any$>$, list$<$list$>$ & 1496 & 3.28 \\
  list$<$any$>$, list$<$character$>$ & 1214 & 2.66 \\
  data.frame, \realv & 1046 & 2.29 \\
  \sL, \reals & 920 & 2.02 \\
  \sC, \realv & 868 & 1.90 \\
   \hline
\end{tabular}
\caption{Top 10 argument signatures in L$1^{+}$.}
\end{table}

A few of the top polymorphic signatures stand out.
First, the matrix/\realv signature seems interesting, converting between vectors and matrices is very simple, and indeed numeric vectors are mathematically {\tt 1 x n} matrices.
In fact, in R matrices can be used identically to vectors in many cases, and we refer the reader to Figure~\ref{fig:matasvec} for examples.
\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
v <- c(1, 2, 3, 4)
ma <- matrix(data=v, nrow=4, ncol=1)
v[2] == m[2] == m[2, 1]

mb <- matrix(data=v, nrow=2, ncol=2)
v[3] == m[3] == m[1, 2]
\end{lstlisting}}\caption{Using a matrix like a vector.}\label{fig:matasvec}\end{figure}

This similarity is reminiscent of another pattern which was more clear in Table~\ref{tab:L0toppoly}; that of list/vector polymorphism.
Transforming between vectors and matrices is just as easy as transforming between lists and vectors is simple.
\AT{We should figure out how we want to deal with this polymorphism.}

Another pattern that we can shed some light on the list$<$any$>$/list$<$X$>$ pattern.
These data points are an artifact of R's three object systems: S3, S4, and R5.
An S3 object is any value with a ``class'' attribute, and indeed many of these tend to be lists.
For example, an S3-style point might be a list with an entry for each dimension in the point, with a ``class'' attribute set to ``Point''.
Our next type system, L2, will account for these.

Another significant source of apparently strange polymorphism is the character, real polymorphism.
Recall Figure~\ref{fig:chardbl}, which laid out an example of this polymorphic signature: lists and vectors can be accessed by strings as well as by numerics.
This is a somewhat unusual example of polymorphism, and our argument depends on there being perhaps a list or vector passed alongside the {\tt character}- or {\tt real}-typed argument.
So, just how often is this the case?
Well, as it happens, character/real polymorphism coincides with some list-flavored type in \INDEXCOINCIDENCE instances (\INDEXYPERC of occurrences).

Another possible explanation for character, real polymorphism is the date data type.
In R, dates are often represented as either strings or numbers, and many packages make use of dates.
Dates (sometimes) manifest themselves in the class of a value, for examples refer to Figure~\ref{fig:date}.
\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
class(date()) # => character
class(Sys.Date()) # => Date
class(as.POSIXlt(Sys.Date())) # => POSIXlt, POSIXt
as.integer(as.Date(1, origin = "1900-01-01"))
\end{lstlisting}}\caption{Dates in R.}\label{fig:date}\end{figure}

This figure reveals an unfortunate truth that date data type in R is not standardized.
That said, as before with matrices and vectors, and vectors and lists, translating between the data types is straightforward.
Once again, this polymorphism can be captured with classes, which is the focus of our next discussion.

We have seen that once we have done away with trivial sources of polymorphism, a number of interesting patterns arise.
In many of these cases, we can appeal to other sources of reflection in R to expand our understanding.
For example, to deal with the possibility of dates, we can appeal to the \code{class} of values, and to gain more information on matrices we might be interested in looking at attributes.
The next type system addresses these concerns.

%
%
%
%
\subsection{L2}


To see how often arguments are polymorphic with respect to class and attributes, we refer to Table~\ref{tab:classcountsL2}.
\AT{Showing full monomorphic all the time might be a bit confusing?}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Apr  1 13:02:08 2019
\begin{table}[ht]
\label{tab:classcountsL2}
\centering
\begin{tabular}{lrr}
  \hline
Type & Count & Percentage \\
  \hline
Full Monomorphic & 783K & 85.59 \\
%  Monomorphic in Type & 869.00 & 95.01 \\
  Monomorphic in Class & 850K & 92.94 \\
  Monomorphic in Attribute Pattern & 836K & 91.41 \\
  Total Seen & 914K & --- \\
   \hline
\end{tabular}
\caption{Account of class- and attribute-based polymorphism in L2.}
\end{table}

The numbers here are in line with the type-based polymorphism of L1.
As before, to build up a better sense of what kind of polymorphism we're dealing with, we extract some of the most common signatures.
We begin with classes, and refer to Table~\ref{tab:classpolyL2}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Apr  1 14:48:55 2019
\begin{table}[ht]
\label{tab:classpolyL2}
\centering
\begin{tabular}{lrr}
  \hline
Class & Count & Percentage \\
  \hline
  primitive, tbl, tbl\_df & 1796 & 2.78 \\
  array, primitive & 1379 & 2.14 \\
  data.table, primitive & 777 & 1.20 \\
  POSIXct, POSIXt & 716 & 1.11 \\
  formula, primitive & 639 & 0.99 \\
   \hline
\end{tabular}
\caption{Top 5 class signatures for class-based polymorphism in L2.}
\end{table}

\AT{TODO number of colocated primitive/other classes. Could use to suggest some optional class annotation.
Maybe grab full class poly to see if there's something big? Without collapse to primitives.}

The first thing to note is the low percentages overall: unlike with types in L0, L1, or L1+, there are no major contributors for class-based polymorphism.
For instance, the single highest source of polymorphism here is due to the ``tbl'' and ``tbl\_df'' classes (of the dplyr package).
For what it's worth, these are classes ascribed to dplyr data objects (i.e., spruced-up data frames).
The next largest contributor is the array class, which is a class for N>2-dimensional matrices (part of the core R language).
\AT{Maybe I should also turn arrays into primitive class?}

To see how we might use class information, we should see how often a monomorphic class signature co-occurs with a polymorphic type signature.
We count \NUMPOLYTYPEMONOCLASS argument positions which match this criteria, which certainly doesn't explain the much higher number of list polymorphism in L1+.

In our analysis, we did not encounter any substantial patterns with class-based polymorphism.
Class is nonetheless important in R, as the dynamic dispatcher uses class information to select the correct method to call.
In short, a small optional annotation indicating the class of an argument may well be enough.

Let us now turn our attention to attributes.
The top contributors for attribute-based polymorphism can be found in Table~\ref{tab:topattrpoly}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Apr  1 23:06:59 2019
\begin{table}[ht]
\label{tab:topattrpoly}
\centering
\begin{tabular}{lrl}
  \hline
Attribute Signature & Count & Percentage \\
  \hline
\{\}, \{names: \C\} & 16589 & 21.11 \\
  \{\}, \{dim: \I\} & 5010 & 6.38 \\
  \{dim:\I,dimnames:list$<$any$>$\}, \{dim:\I\} & 2264 & 2.88 \\
  \{class:\C,names:\C,row.names:\C\}, \{class:\C,names:\C,row.names:\I\} & 2102 & 2.68 \\
  \{\}, \{srcref:\I\} & 2037 & 2.59 \\
  \{dim:\I,dimnames:list$<$\C$>$\}, \{dim:\I\} & 1740 & 2.21 \\
  \{\}, \{class:\C\} & 1673 & 2.13 \\
  \{\}, \{class:\C,levels:\C\} & 1437 & 1.83 \\
  \{\}, \{class:\C,names:\C\} & 1324 & 1.69 \\
  \{\}, \{class:\C,names:\C,row.names:\I\} & 1251 & 1.59 \\
   \hline
\end{tabular}
\end{table}

First, the largest source of attribute-based polymorphism is in optional list names.
The names attribute on a vector or list allows indexing with name rather than numeric position.
Another significant source of this polymorphism is matrices, which can be seen in all the ``dim'' and ``dimnames'' attributes.

\AT{TODO control for the common cases?}

%
%
%
%
\subsection{R's Most Polymorphic Functions}

\AT{We might want to discuss some of the most polymorphic functions in the language.}

In this section, our goal is to shed some light on the most polymorphic functions in R.
Table~\ref{tab:bigpolyfuns} shows the 10 functions with the biggest recorded signatures.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Thu Mar 28 16:01:37 2019
\begin{table}[ht]
\label{tab:bigpolyfuns}
\centering
\begin{tabular}{llrl}
  \hline
Package & Function & Size & Description \\
  \hline
magrittr & \%$>$\% & 23.00 & Tidyverse pipe function \\
  magrittr & freduce & 17.67 & Apply a list of functions sequentially \\
  rlang & captureArgInfo & 16.50 & Capture arg info \\
  rlang & enquo & 16.00 & Create quosure \\
  rlang & get\_expr & 15.00 & Extract expression \\
  data.table & replace\_dot\_alias & 13.50 & Deals with '.' in passed expr \\
  tibble & strip\_dim & 13.00 & Removes 'dim' attribute \\
  data.table & copy & 11.00 & Create copy of passed value \\
  htmlwidgets & shouldEval & 11.00 & Eval passed value depending on tag \\
  magrittr & \%$<$$>$\% & 11.00 & Like \%$>$\% but assigns \\
   \hline
\end{tabular}
\caption{10 most polymorphic functions.}
\end{table}

The size metric we use is roughly the size of each individual argument signature, divided by the number of function arguments.
The division is performed to avoid unfairly favouring functions with many arguments.

Which packages are the most polymorphic?
Figure~\ref{tab:packagepolysize} has the answer.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Thu Mar 28 18:15:26 2019
\begin{table}[ht]
\label{tab:packagepolysize}
\centering
\begin{tabular}{lrl}
  \hline
Package & Total Size & Description \\
  \hline
  magrittr & 4.16 & Tidyverse pipe suite \\
  types & 3.25 & Simple type annotations for R \\
  r6extended & 3.00 & Utilities extending barebones R6 package \\
  utils & 2.93 & Utilities for R developers \\
  zeallot & 2.85 & Metaprogramming package for assignment \\
  listviewer & 2.72 & R list visualizer \\
  proto & 2.60 & Prototype object-based programming \\
  apercu & 2.49 & Display a short view of a complex R value (list, vector, etc.) \\
  pipeR & 2.46 & Another pipe suite \\
  rethinker & 2.44 & RethinkDB client for R \\
   \hline
\end{tabular}
\caption{10 most polymorphic packages.}
\end{table}

\AT{Draft}



The most polymorphic function in the corpus of code we analyzed is {\tt magrittr}'s {\tt \%>\%} (pipe) function.
{\tt magrittr} is part of the Tidyverse, and the pipe function is frequently used to chain function calls.
The fact that the pipe is the most polymorphic function is not shocking, as its goal is simply to take the right hand argument and apply it to the left hand argument, and this in a sense irrespective of type.

...

%
%
%
%
\subsection{Metaprogramming}

\AT{Will probably change.}

Tidyverse owes some of its success to the new syntax defined by some of the included packages.
One such example is the pipe operator \%$>$\% that we saw in the last section, and indeed this function was highly exercised by many other packages \AT{get counts}.
We also grappled with the unfortunate truth that arguments to \%$>$\% can't always be evaluated, which causes some annoyances in the analysis.

We would like to know how often packages contain functions designed for metaprogramming.
As it happens, when arguments fail to evaluate during our analysis even as functions execute successfully, we are actually dealing with instances of metaprogramming.
Some functions, such as those in the magrittr package, will take one (or more) of their arguments and use them as an AST, performing surgery on the expression to create some new expression that they evaluate.
Sometimes, these arguments intended for metaprogramming {\it will} successfully evaluate in our tracer, but this is merely chance.
For example, consider the code snippet in Figure~\ref{fig:metaexworks}.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
mul_by_w_defaults <- function(x=1, y=1) x * y

4 %>% mul_by_w_defaults(2) == `%>%`(lhs = 4, rhs = mul_by(3))
\end{lstlisting}}\caption{Dates in R.}\label{fig:metaexworks}\end{figure}

In this example, our analysis will successfully evaluate the {\tt rhs} argument

%
%
%
%
\subsection{Function Polymorphism}

Now, we will turn our attention to whole function polymorphism.

This is different because we can now look at argument polymorphism across several arguments, like we did when thinking about character/real polymorphism.

We're interested to see what kinds of ``meta-annotations'' we can make at the function level.
For instance, should we link argument types with return types?
Consider Figure~\ref{fig:funanno}.
\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
iden <- function(x) x;

# observed calls:
f(2); # x: double -> double
f(list(1, 2, 3)) # x: list<double> -> list<double>

# possible annotations:
# [1] x: {double, list<double>} -> {double, list<double>}
# [2] x: {double -> double, x: list<double> -> list<double>}
\end{lstlisting}}\caption{Possible function annotations.}\label{fig:funanno}\end{figure}
\AT{Say more about this?}

Harkening back to our discussion of character/real polymorphism, we mentioned that we might want to consider such a type signature to be a list or vector index only if a list or vector was present as another argument.
Now, this may be moving well beyond the purview of R programmers, but perhaps there is value in such a scenario, particularly if we include even {\it more} information in a type system.
For example, say we have a function which takes in a 2D point list.
There are a plethora of useful annotations, here, and of varying degrees of granularity.
We might want simply a "list" annotation, or we might want a "list<double>" annotation to be more specific.
In addition, though, we might want to know if the list has a names attribute, yielding "list<double>@names".
Then, perhaps it would be useful to know what the names were, so "list<double>@names\{x, y\}".

Now, depending on the granularity of individual argument annotations, we might want to link certain arguments together.
Consider the code snippet in Figure~\ref{fig:indexof}.
\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
access <- function(l, i) l[[i]]

L <- list(a=1, b=2, c=3)
access(L, 2) # => 2
access(L, "c") # => 3

# possible annotations:
# [1] l: list, i: {character, real} -> real
# [2] l: list<double>, i: {character, real} -> real
# [3] l: list<X>, i: {character, real} -> X
# [4] l: list<double>@names, i: index -> double
# [5] l: list<double>@names{a, b, c}, i: index@indexOf(l) -> double
\end{lstlisting}}\caption{Linking annotations.}\label{fig:indexof}\end{figure}
The annotations could be used to generate code to check for compliance, like a contract.

%
%
%
%
\subsection{Coverage}

\AT{What is the coverage of these new annotations?}

%
%
%
\section{Examples of Polymorphism}
\label{sec:polyex}


First, Figure~\ref{fig:realex} shows an example of how integers and doubles
are very compatible.  \AT{TODO add matrix (dimensions) to the example.}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
> 5L + 1L # => 6L, an integer
> 5L + 1.2 # => 6.2, a double (5L coerced to 5.0)
> c(10, 20, 30)[1.2] # => 10, 1.2 coerced to 1L
\end{lstlisting}}\caption{Example of {\it real} type usage.}\label{fig:realex}\end{figure}

Figure~\ref{fig:optnull} shows an example of a function with an optional
argument.  In the function, {\tt x} is a sorted vector of income values, and
{\tt w} is a vector of weights, optionally NULL.  The function computes
fractional ranks required for the computation of some coefficient
implemented by the package.  We see that if {\tt w} is NULL, then default
unit weights are generated.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
frac.ranks <- function(x, w=NULL) {
  if (is.null(w)) w <- rep(1, length(x)) # if no weights passed, take all weights = 1
  ...
\end{lstlisting}}\caption{Example of optional argument (from {\tt acid} package).}\label{fig:optnull}\end{figure}

Figure~\ref{fig:listvec} shows an example of a function with list and vector
polymorphism.  The function takes a list or vector {\tt point} and a data
frame {\tt polyg} representing a polygon.  In it, we see that {\tt point}
can be either a list or a vector, and the function code casts the argument
to a double vector.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
is.point.inside <- function (point, polyg) {
    p <- as.numeric(point) # as.numeric(list(1, 2)) => c(1, 2)
    ...
\end{lstlisting}}\caption{Example of list/vector argument (from {\tt bivrp} package).}\label{fig:listvec}\end{figure}

Figure~\ref{fig:charclos} shows an example of a function with character and
function polymorphism.  \AT{Bluh.}  At a high level, this function
calculates point estimates for an {\tt angmcmc} object (specific to some
packages).  The caller specifies {\tt fn}, either a function or a function
name which will be evaluated on the object samples to estimate parameters.
The call to lookup the function name is on line 10.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
pointest <- function (object, fn = mean, par.name, comp.label, chain.no,  ...) {
    ...
    if (is.character(fn))
        if (fn == "MODE" | fn == "MAP")   do_MAP <- TRUE
        else {
            do_MAP <- FALSE
            fn <- match.fun(fn) # looks up the function name
        }
\end{lstlisting}}\caption{Example of char/closure argument (from {\tt BAMBI} package).}\label{fig:charclos}\end{figure}

\AT{I'll explain this better, maybe?}  In Figure~\ref{fig:dfdbl}, we see a
function that, among other things, takes in some data ({\tt dat}) and a
character string ({\tt spss}) specifying either {\tt "in"} or {\tt "out"}.
Now, depending on the value of {\tt spss}, {\tt dat} will either be a data
frame or a vector of doubles.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
nret.translator <- function(dat, spss="out", ...) {
  ...
  if(identical(spss, "out")) {
    if(!is.vector(dat))  stop(simpleError("...'dat' must be a vector!"))
    ...
  } else {
    ...
    # here, dat is data.frame
    items.idx <- items.idx[order(names(dat[, items.idx]))]
  }
\end{lstlisting}}\caption{Example of a data.frame/(\D) argument (from {\tt klausR} package).}\label{fig:dfdbl}\end{figure}

In Figure~\ref{fig:chardbl}, we see a function which takes in a list ({\tt network}), a vector of indices of that list ({\tt fixIndices}), and a
vector of values ({\tt values}).  The locations specified by {\tt fixIndicies} in {\tt network} are updated with {\tt values}.  Here, {\tt fixIndicies} has been observed to be either a character or double vector:
In R, list indices are typically doubles, but can be characters (if the list
or vector being indexed has a {\tt names} attribute).

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
fixGenes <- function (network, fixIndices, values) {
  ...
  network$fixed[fixIndices] <- as.integer(values)
  ...
\end{lstlisting}}\caption{Example of character/double argument (from {\tt BoolNet} package).}\label{fig:chardbl}\end{figure}%$

In Figure~\ref{fig:matvec}, we see that the function can take in a vector,
but immediately transforms it (transpose, with {\tt t}) into a matrix.  As
an idea, matrix/vector polymorphism seems sensible, as mathematically
vectors are matrices.  R echoes this by making conversion between the two
``types'' easy: \code{as.vector(m)} flattens a matrix \code{m} into a vector
(e.g., \code{as.vector(matrix(2, 2, 2)) == c(2, 2, 2, 2)}), and
\code{as.matrix(v)} builds a {\tt length(v) x 1} matrix (e.g.,
\code{as.matrix(c(1, 2)) == matrix(data=c(1, 2))}).  \AT{TODO fix line break
  in code at end of last sentence}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
tee <- function (x, theta, D1, D2, phi)  {
    if (is.vector(x)) x <- t(x)
    ...
\end{lstlisting}}\caption{Example of matrix/(\D) argument (from {\tt calibrator} package).}\label{fig:matvec}\end{figure}

%
%
%
%
\section{Related Work}

\AT{Not sure what the best place for this is, but we should discuss these papers somewhere.}

The idea of measuring polymorphism via some program analysis isn't new.
For instance, some work~\cite{aakerblom2015measuring} ... .

More generally, there is an existing literature on analyzing usage patterns
of language features.  For instance, the dynamic features of Smalltalk were
analyzed in some work~\cite{callau2011howdevelopers}, ...

Other analysis work~\cite{milojkovic2017duck} ... .

\section{Conclusions and Future Work}

\bibliographystyle{boilerplate/ACM-Reference-Format}
\bibliography{bib/biblio,bib/jv,bib/r,bib/new}
\end{document}
