\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings,xspace}
\lstset{language=R}
\usepackage{my_style}

\definecolor{LightGray}{rgb}{.95,.95,.95}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray}
}

\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={env, equal, c, runif, trace, args},
  breaklines=true
}

\lstdefinestyle{Rin}{ %
  style=R,
  numberstyle=none,
  basicstyle=\normalsize\ttfamily,
  breaklines=false
}
\newcommand{\code}[1]{\lstinline|#1|\xspace}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A Large-scale Study of Polymorphism in R}

\newcommand{\PACKAGES}{7,500\xspace}
\newcommand{\PROGRAMMERS}{10,000\xspace}

\begin{abstract}
The R programming language is widely used in a variety of scientific domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in over X millions
of lines of R code spread among some \PACKAGES packages, written over a
period of over 20 years by over 10,000 programmers.  We perform a dynamic
analysis, leveraging tests and use-cases, to determine the level of
polymorphism that is present in the code. We do this for several potential
notions of types. Our result suggest that polymorphism is important in some
key parts of the system but that relatively simple type annotations could be
used to capture most of the interesting cases.
\end{abstract}

\maketitle

\section{Introduction}

Our community builds, improves, and reasons about programming languages.  To
make design decisions that benefit most users, we need to understand the
language we are working with as well as the real-world needs it
answers. Often, we, as researchers, can appeal to our intuition as many
languages are intended to be general purpose and appeal to users with some
computer science training. Unfortunately, these intuitions don't always
apply to domain-specific languages, languages designed for and by a specific
group of users to solve very specific needs. This is the case of the data
science language R.

R and it's ancestor S are languages designed, implemented and maintained by
statisticians. Originally they were designed as glue languages, languages
that would allow to read data into vectors and call statistical routines
written in Fortran. Over three decades, the languages became widely used
across many fields of science and in industry for data analysis and data
visualization; with time additional features were added.  Modern R, as a
linguistic object of study, is fascinating. It is a vectorized, dynamically
typed, lazy functional language with limited side-effects, extensive
reflective facilities and retrofitted object-oriented programming support.

Many of the design decisions that gave us R were intended to foster an
interactive, exploratory, programming style. This includes, to name a few,
the lack of type annotations on variables and functions, the ability to use
syntactic shortcut, and the automatic conversion between data types.  While
these choices have led to a language that is surprisingly easy to use by
beginners --many data science programs do not teach the language itself but
simply introduce some of its key libraries-- they have also created a
language where almost all computations yield a numeric result and where
errors can go undetected. 

One way to increase assurance in the results obtained when using R would be
to add type annotations to functions and variable declarations. These
annotations could then be used, either statically or (more likely)
dynamically, to catch mismatches between expected and provided data values.
The nature of R is such that it is unlikely to be ever fully statically
checked, furthermore end users may not be willing to write types when
carrying out exploratory programming tasks. So, we are looking for an
optional type system that would allow us to capture as much of behavior of
library functions as possible while remaining easy to understand for
end-users and library developers alike.

This papers is a data-driven study of what a type system for the R language
could look like. Longer term, our intention is to propose changes to
language, but for any changes to be accepted by the user community, they
must clearly benefit the language without endangering backwards
compatibility. Our goal is thus to find a compromise between simplicity and
usefulness; the proposed type system should cover most common programming
idioms while remaining easy to use. In order to do this, we need to
understand the degree of polymorphism present in R code, that is to say, how
programmers leverage the dynamic nature of R to write code that can accept
arguments of different types.  This understanding will drive our design.

We propose to capture the degree of polymorphism present in R by the means
of a dynamic analysis of widely used libraries. For each function call we
can record the types of its arguments and of its return value. This allows
us to observe how many different combination of types are accepted by any
given function. Unlike many other languages, R has a carefully curated
software repository called CRAN. To be deposited in CRAN, a package must
come with sample dataset, tests and executable use-cases. As part of normal
operations these tests are run regularly and failing packages are removed.
This allowed us to have access to \PACKAGES libraries and about an order of
magnitude more runnable scripts that exercise those libraries.

The contributions of this paper are thus as follows:
\begin{itemize}
\item A large-scale analysis of the polymorphism present in function
  signatures of \PACKAGES widely used and actively maintained R packages.
\item A tracing and analysis pipeline that extends a previously published
  test generation tool named \code{genthat}.
\item Manual analysis of 100 functions to validate the dynamic analysis
  results.
\end{itemize}

One threat to validity of our work is that we rely on dynamic analysis, so
our conclusions are only as good as the coverage of the possibly function
behaviors. Previous work~\cite{issta18}, reported that running all the
scripts that come with CRAN packages gives, on average, 68\% test coverage.
We attempted to mitigate the threat coming from the fact that only part of
the code is being exercised by manual analysis. It would be reasonable to
ask for confirmation of the data by static analysis of the code, but sound
static analysis of R is difficult because of the extensive use of reflective
features such as \code{eval} and of the ability to redefine the meaning of
operators such as \code{+} and \code{if}.  Another threat to validity is
that we only have access to code that has been deposited in the CRAN
repository. While this may bias our findings towards code written to be
reusable and, possibly, better engineered than typical user code. This is
also the code that would most benefit from type annotations.

\section{The R Programming Language}\label{sec:rlang}

Over the last decade, the R Project has become a key tool for implementing
sophisticated data analysis algorithms in fields ranging from Computational
Biology~\cite{R05} to Political Science~\cite{R:Keele:2008}. At the heart of
the R project is a \emph{vectorized, dynamic, lazy, functional,
  object-oriented} programming language with a rather unusual combination of
features~\cite{ecoop12} designed to ease learning by non-programmer and
enable rapid development of new statistical methods.  The language, commonly
referred to as R was designed in 1993 by Ross Ihaka and Robert
Gentleman~\cite{R96} as a successor to S~\cite{S88}.  First released in
1995, under a GNU license, R rapidly became the lingua franca for
statistical data analysis. Today, there are over 13,000 R packages available
from repositories such as CRAN and Bioconductor.  With 55 R user groups
world-wide, Smith~\cite{eco11} estimates that there are over 2 million
end-users.


As an introduction to R, consider the code snippet in Fig.~\ref{sample} from
a top-level interaction where the user defines a function \code{normSum}
that accepts vectors of integers, logicals, doubles and complex values and
normalizes the vector with respect to its sum and rounds the results. The
function definition does not require type annotations, and all operations
transparently work on vectors of any length and different types.

\begin{figure}{\small
\begin{lstlisting}[style=R]
> normSum <- function( m )  round( m / sum(m), 2)
> normSum(c(1L,3L,6L))
[1] 0.1 0.3 0.6
> normSum(c(1.1,3.3,6.6))
[1] 0.1 0.3 0.6
> normSum(c(1.6,3.3,6.1))
[1] 0.15 0.30 0.55
> normSum(complex(r=rnorm(3),i=rnorm(3)))
[1] 0.49+0.21i 0.30-0.18i 0.22-0.03i
\end{lstlisting}}
\caption{Sample R code}\label{sample}
\end{figure}

\subsection{Types of Data}

Before attempting to define a type system for R, we should understand the
different kinds of values that programs operate on.  As we will see
different notions of type may emerge depending on how granular we want to
be.

The first distinction that can be made is based on the value of the
\code{typeof} function.  This function, given a value, will return one of
the following types: \code{logical} (vector of booleans), \code{integer}
(vector of integers), \code{double} (vector of floating points),
\code{complex} (vector of complex numbers), \code{character} (vector of
character strings), \code{raw} (vector of bytes) and \code{list}
(polymorphic vector), \code{NULL} (vector of NULL),
\code{closure} (function), \code{environment} (naming environment),
\code{S4} (S4 object) and others that are used within the implementation of
R (\code{special}, \code{builtin}, \code{symbol}, \code{pairlist},
\code{promise}, \code{language}, \code{char}, \code{...}, \code{any},
\code{expression}, \code{externalptr}, \code{bytecode} and \code{weakref}).

%% TODO : make the types all bold.
\AT{Some examples of use of \code{typeof}:}
To build a sense of how these types arise in R code, consider the following:
\AT{We could think of another way to do this also.}

\begin{itemize}

	\item \code{typeof(5) == typeof(c(5)) == typeof(c(5, 5)) == "double"}.
	As we mentioned, R is vectorized, so even apparent scalars are vectors.
	\AT{Just for fun,} note that \code{5[1] == 5} in R.
	
	\item to actually get an integer, you need \code{typeof(5L) == "integer"}.
	Doubles are by far the most common numeric type in R \AT{I can get data for this}, and you can even use them to index lists and vectors: \code{list(1, 2)\[\[1.9\]\] == 1}.
	Note the double brackets for list indexing.
	
	\item NAs are common in data-centric languages (e.g. MATLAB, \AT{others?}), and R is no exception.
	As for their type, we see that \code{typeof(NA) == "logical"}, but that isn't to say that NA only has type logical.
	Consider \code{typeof(c(1, NA, 3)\[2\])}: NA actually inhabits every type in R, \AT{details}.

	\item \AT{other examples?}

\end{itemize}

Another form of type information relates to metadata on values, and R allows programmers to add said metadata in the form of {\it attributes}.
The functionality to do so in R is through the \code{attributes(x)} and \code{attr(x, "name")}  functions.
\code{attributes} 

  - dim

class:


\section{The \texttt{genthat} Package}

Unfortunately, getting our hands on ``front-line'' R code is tricky: R is
not like many other languages, where large systems are built up and stored
in a repository such as Github.  Instead, R programmers are liable to
leverage a number of packages and work them into their data analysis
pipelines.

Interestingly, we are somewhat able to replicate this using only package
code.  CRAN, has a policy that packages made available on the CRAN platform
should be accompanied by a series of examples, tests, and/or vignettes
showing off how the code is intended to be used.  Nominally unrelated, the
{\tt genthat} project~\cite{issta18} traces this swath of example code to
generate new examples which package designers may not have thought of.  As
it happens, we can leverage {\tt genthat} to extract dynamic type
information from running package test code, which presumably indicates how
package code is intended to be used.

\AT{Draft}

We modify the {\tt genthat} tracer to capture the type information of
arguments.  {\tt genthat} generates traces for each unique function
invocation with the express purpose of re-invoking the function at a later
time.  Instead of generating traces for these invocations, we can instead
capture type information of the arguments and return values of functions.
This is done by evaluating the arguments and return values of functions at
the end of the function's scope in the relevant environment.  The sort of
types and granularity of the type information that are generated can be
specified in our tracer: Essentially, our tracer is parameterized over a
user-specified type system.  For instance, if desired our tracer can dig
into lists and data frames to get the types of list elements and fields.
Our tracer produces trace results for each unique type signature
encountered, which may then be used for data analysis.

An unfortunate consequence of R's most common usage pattern is the lack of
"deployed" R code.  R is most often used for exploratory data analysis, and
very little of this analysis code is made publicly available on e.g. GitHub.
The {\tt genthat} angle is thus desirable: With the scarcity of non-library
code, examples of package usage is about as representative as one can hope
for of the sort of code people will write in their analysis pipelines.

We can do a little bit better than merely package usage examples, though.
Available through CRAN is a list of {\it reverse dependencies} of packages:
for some package {\tt p}, a list of all packages which require {\tt p} in
some way.  Our tracer is able to leverage and trace this code as well.  When
running a package, we also trace how all inherited functions are used, thus
painting an even clearer picture of function usage.  This additional data is
considered alongside similar data collected for all other packages, as well
as the data obtained by tracing the package's examples.

% Some prose we might want to pull from.
%First, we leverage {\tt genthat}'s tracer to trace function executions.  The
%idea here is that we don't really have access to non-library code written in
%R, as general use patterns are (possibly?) to write small scripts which
%analyze some bit of data and possibly visualize results.  The goal is likely
%not to build big working systems, instead to explore data with by writing
%and rewriting small scripts, ad infinitum.  In looking at package tests,
%vignettes, and examples, we are painting a picture of how the package
%designers intended their packages to be used, which we believe is a close
%approximation of what R users would do.


\section{The Method}

In this section, we will detail our methodology for collecting data.

%
\subsection{Data Collection Pipeline}

As we mentioned in the previous section, we modify the {\tt genthat} tracer
to collect and output the type information for all function invocations.

\AT{somewhere} Our analysis can be parameterized by a user-specified ``type
system'', simply a map between existing dynamically observable type
information and user-defined types.  If no type system is specified, the
analysis gains as much information as it can.  For instance, if the analysis
encounters {\tt typeof(x) == "double"}, it will look at the length of {\tt
  x} to determine if {\tt x} is a scalar or a vector, generating the
annotation {\tt scalar/double} or {\tt vector/double} accordingly.  Note
that this is imperfect, as a vector of unit length would (perhaps
incorrectly) show up as a {\tt scalar/X}, but any other invocation where the
vector does not have unit length will generate the correct {\tt vector/X}
type, and the distinction between scalars and vectors can be later collapsed
if desired.  Note also that the type system described here is the
\AT{SOMETHING} Type System, described in detail in Section~\AT{TODO}.

The process for tracing one package {\tt p} is as follows:

\begin{enumerate}
\item first, all functions in {\tt p} are instrumented with on-exit hooks.
  These hooks call a number of R's available reflection functions on each of
  the arguments, as well as the return type: The called functions are {\tt
    typeof}, {\tt class}, and {\tt attributes}.  See Section~\ref{sec:rlang}
  for details on these functions.
	
\item then, depending on the specified type 
\end{enumerate}

Then, we aggregate the information in all traces for a particular function.
If there was only one function trace, we discard the result as the function
is trivially monomorphic.  We can see which arguments had which types over
how many traces, and from here we can build a \textit{signature} for each
function argument, and consider them together to create a function
signature.  A \textbf{polymorphic argument} is one which has been inhabited
by values of at least two different types, and a \textbf{polymorphic
  function} is a function with at least one polymorphic argument or a
polymorphic return.

We repeat the above for each package on the Comprehensive R Archive Network
(CRAN), the premier source for R libraries (called packages), and the
Bioconductor package repository.  Now, depending on the data point we're
after, our methods from here differ slightly.

\begin{itemize}
    \item for \textbf{counting signatures}: for each signature, we count how
      many functions or arguments have the requisite signature;
    \item for \textbf{finding the most common signatures}: we enumerate as
      before, and sort the signatures based on how often they appear;
\end{itemize}

%
%
\subsection{Attributes}\label{sec:method:attributes}

Types (in the sense of the result of a call to {\tt typeof}) are easy to
analyze as values can only have one type.  Attributes, however, are a
different story, as values can have an arbitrary number of attributes.
Essentially, we're not interesting by function arguments which have had
multiple attributes, we're interested in function arguments which have had
multiple attribute \textit{patterns}. \AT{is this defined earlier?}
Arguments are said to be polymorphic in attribute if they have been
inhabited with values which have had different attribute patterns.  \AT{We
  have not investigated whether functions which are polymorphic in attribute
  use attributes extensively inside the function code.}

\AT{Paragraph justifying our construction of an attribute pattern.
Either just name, or (name, type).}

\AT{Paragraph about so-called ``naturally-occurring'' attribute patterns.}


%
%
%
%
\subsection{{\tt genthat} Tracer Quirks}

First and foremost, {\tt genthat} captures arguments only on function exit,
so as to not force promises during function execution.  \AT{Uh oh:} One
clear limitation here is that if an argument \textit{gains} attributes over
the functione execution, then they will appear in our signatures.  As R is
side-effecting, this is an entirely valid practice (i.e., passing objects to
functions to modify them), though \AT{it's unclear if people actually do
  this}.

\AT{Maybe talk about the quirks of genthat in here?
For example, how it deals with default arguments, and stuff like that.}

%
%
%
%
\subsection{Goals}

Ultimately, our goal is to collect data and report on patterns which emerge
organically.  That said, the comprehensive picture of type usage we are
developing can also be used to inform the design of a set of \textit{type
  annotations} for the language, and this goal informs our analysis.  As we
collect our data, we keep in mind the idea that the data we produce should
clearly suggest what sorts of type annotations would reflect language usage
patterns.

%
%
%
%
\subsection{The Corpus}

\AT{I'm hesitant to go into too much detail here, as I'm not sure if we're quite done running our analysis.
I can imagine a scenario where we will try to rerun with a longer timeout just to get a little bit more data (because why not).}

We ran our analysis over a subset of CRAN, the Comprehensive R Archive
Network.  We analyzed \AT{over 7500} packages, over half of all packages
available on CRAN.  Our analysis had an explicit timeout of \AT{30 minutes}
per package, as a number of R packages feature functions which have a
nigh-eternal running time (e.g., machine learning packages and complex
simulations) \AT{cite those packages/examples}.

There are a number of interesting packages in our corpus.  \AT{References
  for all of these.}  For example, we have analyzed some packages for
plotting data: among others, the {\tt ggplot2} package is an immensely
popular package for generating highly-customizable plots, and the {\tt
  plotmo} package is an interesting package aimed at displaying models.  We
also analyzed {\tt dplyr}, an integral package to the {\tt tidyverse} set of
R packages \AT{talk about tidyverse earlier, or here}.  {\tt dplyr} is a
popular package that makes full use of R's features to create what is
tantamount to a small DSL for data manipulation within R.  Another
interesting package is {\tt rquery}, a package for generating SQL-like
queries.

\AT{Should we talk about some of the packages?
What else do we have to say about the corpus?
We can restate the numbers from the abstract.}

%
%
%
%
%
%
\section{Results}

\AT{PSA:} There are a few ways we could do this section.
One way would be to go through our data in a sort of linear fashion, akin to how the main notebook is set up.
We can explore each of our type systems one at a time, and present them (thus breaking the section up by type system).
Another option is to just break down by-result, so have a section for the counts, for the top polymorphism...
I like the first option better.

\AT{Draft}
In this section, we will present the results of our analysis. 
We approach the data with the purpose of informing the design of a static type system for R.
We propose to begin with a very basic type system, discussed next.

\subsection{Base R Type System}

The {\it Base R Type System} (BRTS) ...

%
%
%
%
\subsection{Types in R}

\AT{may not be the correct place for this. i think i accidentally deleted it.}

R is a unityped language, and as such is trivially monomorphic, and this notion of monomorphism is far from useful.
We are interested in expanding the notion of ``type'' in R, and thankfully the language has some tools on offer.
\AT{this is discussed later on}

%
%
\subsubsection{What are possible type systems?}

Great question.
Some things:

\begin{itemize}

\item {\it The Obvious Type System (OTS)}: this type system corresponds directly with the type information that one can fetch using the {\tt typeof}, {\tt attributes}, and {\tt class} functions.
In this system, there is no distinction between vectors and scalars, and lists of differently-typed elements (e.g., a list of integers vs. a list of strings), and there is a distinction between integers and doubles, and the type of NULL and all other types.
This type system isn't particularly informative to a programmer, though it does reflect the treatment of values in the R runtime, for the {\tt typeof} a value is the type that the value has according to the R internals.

\item {\it The Fine-Grained Type System (FGTS)}: this type systems includes a number of relevant additions.
Chiefly, it distinguishes between vectors and scalars, as it is conceivable that boxing scalars as single-element vectors might be overkill.
In addition, list types are parameterized over the types of their elements (e.g., list<integer> vs. list<character>).
Finally, NAs ``in the wild'' are given the type {\tt raw\_NA}, as {\tt typeof(NA) = logical} in R.

\item {\it The Null-and-NA-Free Type System (NNTS)}: most languages consider NULLs to inhabit all types, and R does so for NAs \AT{elaborate}.
This gives us ... .

\end{itemize}

Of course, one could conceive of a more fine-grained type system than FGTS, but it's unclear that we would gain much in doing so.
Indeed, there is a space to explore in slightly less fine-grained type systems, which make modifications including:

\begin{itemize}

\item {\bf collapsing vectors and scalars}: this yields a type system slightly closer to OTS.

\item {\bf rolling NULL into other types}: in most languages, NULL is an inhabitant of all types, and this adjustment seems interesting.
Note that this goes beyond even OTS, as in R {\tt typeof(NULL) = NULL}.

\item {\bf rolling NA into other types}: \AT{i should try this}
similarly, NA is an inhabitant of all types, though NA standing alone has type {\tt logical}.
FGTS distinguishes such NAs with the {\tt raw\_NA} type, and if instead we consider NA to inhabit all types we may \AT{win}.

\item {\bf combining integers and doubles}: as in most languages, doubles and ints can stand-in for each other in nearly every scenario.
Even if not, it is trivial to convert from one to the other, particularly converting ints to doubles.
Aside, in R a double can be used to index a list, and R will floor it when performing the lookup.

\end{itemize}

%
%
%
%
\subsection{Usage Patterns}

In this section, we will discuss function usage patterns which arose in our analysis.
We will start by looking at the morphicity of functions.

\subsubsection{Function Argument and Return Morphicity}

First and foremost, we would like to know how often R programmers create polymorphic functions.
Recall that we define a polymorphic function to be a function with at least one polymorphic argument, or a polymorphic return.
We will turn our attention now to the data in \todo{Figure}, and go through each entry in the table.

Here, we see that the vast majority of function arguments are indeed monomorphic.
Monomorphic arguments are easy to annotate, as the {\tt typeof} the arguments is exactly the most precise annotation we could give, at least in terms of \textit{type}.
That said, types alone don't always paint the whole picture:
recall that \textit{attributes} are an R language feature which allows programmers to stick metadata onto values.
So in which circumstances \textit{do} types paint the whole picture?

The second data point in \textbf{TODO FIGURE} indicates that a
\isit{majority} of function arguments are monomorphic in type \textit{and}
have no attributes.  These represent arguments which are truly trivial to
annotate, as the {\tt typeof} an argument perfectly describes the usage of
that argument.  That said, some attributes arise naturally in R: For
instance, names in a named list (e.g. {\tt x} and {\tt y} in {\tt list(x=0,
  y=0)}) appear as an attribute on the value.  In
Section~\ref{sec:method:attributes}, we outlined these naturally-occurring
attribute patterns, and the third data point in \textbf{TODO FIGURE} shows
that a nontrivial amount of functions are monomorphic in type with said
natural attribute patterns.

Another facet of type information in R is in the {\tt class} attribute.
Recall that values have a \textit{class} in addition to a type: For example,
a {\tt data.frame} has type {\tt list} and class {\tt data.frame}.
\todo{Plug classes earlier} R has a number of built-in classes, such as
\todo{X, Y, and Z}, but users are free to redefine the class of any value at
runtime, and easily define new classes.  The next data point in
\todo{Figure} shows that user-defined classes don't appear altogether often,
though they do indeed feature.  \AT{In a following section, we will discuss
  how the usage of these classes manifests itself.}

\AT{Separate section for functions? Right now, this is at argument granularity.}

\subsubsection{Type Signatures}

In the last section, we presented a high-level overview of the morphicity ... .

\subsubsection{Attribute Signatures}

Recall that attributes are a way for programmers to store metadata on values in R.
What are the common attribute patterns?
And how often is the attribute pattern polymorphic?

\subsubsection{Takeaways}

\begin{itemize}
    \item the vast majority of arguments are monomorphic in type;
    \item of those, over 60\% have no attribute information;
    \item of the 40\% with attribute information, roughly 1/2 have fairly
      simple attributes corresponding to base R constructs (named lists and
      vectors, matrices, and data frames);
    \item now, of those arguments which are polymorphic, a sizable chunk (well over half) have defensible signatures (e.g., double and character for named list indexing, double and integer for obvious reasons, etc.).
\end{itemize}

In short, it looks like R (package) programmers are reasonable.  I'd
conjecture that a lot of the polymorphism (e.g., double and list) is coming
from how easy it is to use either type in a given situation (e.g.,
converting from vector of doubles to list or vice versa is simple).

%
%
%
%
%
%
\section{Synthesis}

In this section, we will discuss the conclusions that we draw from our data.


%
%
\subsection{Suggested Annotations}

\AT{How can we capture these patterns with annotations?}

Some possible annotations:

\begin{itemize}
    \item \textit{real}: for \textit{double} and \textit{integer} values
    \item \textit{function}: for \textit{closure}, \textit{special}, and \textit{builtin} values
    \item \textit{vector}: to indicate that something should be vectorized
    \item \textit{scalar}: to indicate that something should \textbf{not} be vectorized
    \item \textit{index}: for \textit{real} and \textit{character} values
\end{itemize}

%
%
\subsubsection{Struct-Like Attribute Declarations}

\AT{What's a convenient way to annotate attributes?  Should investigate how
  often attributes are consistent.}

%
\subsubsection{Coverage}

\AT{What is the coverage of these new annotations?}

%
%
%
%
\section{Related Work}

\AT{Not sure what the best place for this is, but we should discuss these papers somewhere.}

The idea of measuring polymorphism via some program analysis isn't new.
For instance, some work~\cite{aakerblom2015measuring} ... .

More generally, there is an existing literature on analyzing usage patterns
of language features.  For instance, the dynamic features of Smalltalk were
analyzed in some work~\cite{callau2011howdevelopers}, ...

Other analysis work~\cite{milojkovic2017duck} ... .

\section{Conclusions and Future Work}

\bibliographystyle{boilerplate/ACM-Reference-Format}
\bibliography{bib/biblio,bib/jv,bib/r,bib/new}

\end{document}
