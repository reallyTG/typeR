\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings,xspace,wrapfig}
\lstset{language=R}
\usepackage{my_style}

\definecolor{LightGray}{rgb}{.95,.95,.95}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\graphicspath{ {./plots/} }

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray},
  numbers=left
}

\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={env, equal, c, runif, trace, args},
  breaklines=true
}

\lstdefinestyle{Rin}{ %
  style=R,
  numberstyle=none,
  basicstyle=\normalsize\ttfamily,
  breaklines=false
}
\newcommand{\code}[1]{\lstinline|#1|\xspace}

\newcommand{\genthat}{{\sc Genthat}\xspace}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A Large-scale Study of Polymorphism in R}

\newcommand{\PACKAGES}{11,463\xspace}
\newcommand{\PROGRAMMERS}{?\xspace}
\newcommand{\PERCENTCRAN}{83\%\xspace}
\newcommand{\CRANTOTAL}{13,841\xspace}
\newcommand{\RLOC}{15,050,267\xspace}
\newcommand{\CLOC}{9,373,542\xspace}
\newcommand{\YEARS}{?\xspace}

\begin{abstract}
The R programming language is widely used in a variety of scientific domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in \RLOC lines of R 
code spread among some \PACKAGES packages, written over a
period of over \YEARS years by \PROGRAMMERS programmers.  We perform a dynamic
analysis, leveraging tests and use-cases, to determine the level of
polymorphism that is present in the code. We do this for several potential
notions of types. Our result suggest that polymorphism is important in some
key parts of the system but that relatively simple type annotations could be
used to capture most of the interesting cases.
\end{abstract}

\maketitle

\section{Introduction}

Our community builds, improves, and reasons about programming languages.  To
make design decisions that benefit most users, we need to understand the
language we are working with as well as the real-world needs it
answers. Often, we, as researchers, can appeal to our intuition as many
languages are intended to be general purpose and appeal to users with some
computer science training. Unfortunately, these intuitions don't always
apply to domain-specific languages, languages designed for and by a specific
group of users to solve very specific needs. This is the case of the data
science language R.

R and its ancestor S are languages designed, implemented and maintained by
statisticians. Originally they were designed as glue languages, languages
that would allow to read data into vectors and call statistical routines
written in Fortran. Over three decades, the languages became widely used
across many fields of science and in industry for data analysis and data
visualization; with time additional features were added.  Modern R, as a
linguistic object of study, is fascinating. It is a vectorized, dynamically
typed, lazy functional language with limited side-effects, extensive
reflective facilities and retrofitted object-oriented programming support.

Many of the design decisions that gave us R were intended to foster an
interactive, exploratory, programming style. This includes, to name a few,
the lack of type annotations on variables and functions, the ability to use
syntactic shortcut, and the automatic conversion between data types.  While
these choices have led to a language that is surprisingly easy to use by
beginners --many data science programs do not teach the language itself but
simply introduce some of its key libraries-- they have also created a
language where almost all computations yield a numeric result and where
errors can go undetected. 

One way to increase assurance in the results obtained when using R would be
to add type annotations to functions and variable declarations. These
annotations could then be used, either statically or (more likely)
dynamically, to catch mismatches between expected and provided data values.
The nature of R is such that it is unlikely to be ever fully statically
checked, furthermore end users may not be willing to write types when
carrying out exploratory programming tasks. So, we are looking for an
optional type system that would allow us to capture as much of behavior of
library functions as possible while remaining easy to understand for
end-users and library developers alike.

This papers is a data-driven study of what a type system for the R language
could look like. Longer term, our intention is to propose changes to
language, but for any changes to be accepted by the user community, they
must clearly benefit the language without endangering backwards
compatibility. Our goal is thus to find a compromise between simplicity and
usefulness; the proposed type system should cover most common programming
idioms while remaining easy to use. In order to do this, we need to
understand the degree of polymorphism present in R code, that is to say, how
programmers leverage the dynamic nature of R to write code that can accept
arguments of different types.  This understanding will drive our design.

We propose to capture the degree of polymorphism present in R by the means
of a dynamic analysis of widely used libraries. For each function call we
can record the types of its arguments and of its return value. This allows
us to observe how many different combination of types are accepted by any
given function. Unlike many other languages, R has a carefully curated
software repository called CRAN. To be deposited in CRAN, a package must
come with sample dataset, tests and executable use-cases. As part of normal
operations these tests are run regularly and failing packages are removed.
This allowed us to have access to \PACKAGES libraries and about an order of
magnitude more runnable scripts that exercise those libraries.

The contributions of this paper are thus as follows:
\begin{itemize}
\item A large-scale analysis of the polymorphism present in function
  signatures of \PACKAGES widely used and actively maintained R packages.
\item A tracing and analysis pipeline that extends a previously published
  test generation tool named \genthat.
\item Manual analysis of 100 functions to validate the dynamic analysis
  results.
\end{itemize}

One threat to validity of our work is that we rely on dynamic analysis, so
our conclusions are only as good as the coverage of the possibly function
behaviors. Previous work~\cite{issta18}, reported that running all the
scripts that come with CRAN packages gives, on average, 68\% test coverage.
We attempted to mitigate the threat coming from the fact that only part of
the code is being exercised by manual analysis. It would be reasonable to
ask for confirmation of the data by static analysis of the code, but sound
static analysis of R is difficult because of the extensive use of reflective
features such as \code{eval} and of the ability to redefine the meaning of
operators such as \code{+} and \code{if}.  Another threat to validity is
that we only have access to code that has been deposited in the CRAN
repository. While this may bias our findings towards code written to be
reusable and, possibly, better engineered than typical user code. This is
also the code that would most benefit from type annotations.

\newpage  %%Leave here

\section{The R Programming Language}\label{sec:rlang}

Over the last decade, the R Project has become a key tool for implementing
sophisticated data analysis algorithms in fields ranging from Computational
Biology~\cite{R05} to Political Science~\cite{R:Keele:2008}. At the heart of
the R project is a \emph{vectorized, dynamic, lazy, functional,
  object-oriented} programming language with a rather unusual combination of
features~\cite{ecoop12} designed to ease learning by non-programmer and
enable rapid development of new statistical methods.  The language, commonly
referred to as R was designed in 1993 by Ross Ihaka and Robert
Gentleman~\cite{R96} as a successor to S~\cite{S88}.  First released in
1995, under a GNU license, R rapidly became the lingua franca for
statistical data analysis. Today, there are over 13,000 R packages available
from repositories such as CRAN and Bioconductor.  With 55 R user groups
world-wide, Smith~\cite{eco11} estimates that there are over 2 million
end-users.

As an introduction to R, consider the code snippet in Fig.~\ref{sample} from
a top-level interaction where the user defines a function \code{normSum}
that accepts vectors of integers, logicals, doubles and complex values and
normalizes the vector with respect to its sum and rounds the results. The
function definition does not require type annotations, and all operations
transparently work on vectors of any length and different types.

\begin{figure}[!hb]{\small
\begin{lstlisting}[style=R]
> normSum <- function( m )  round( m / sum(m), 2)
> normSum(c(1L,3L,6L))
[1] 0.1 0.3 0.6
> normSum(c(1.1,3.3,6.6))
[1] 0.1 0.3 0.6
> normSum(c(1.6,3.3,6.1))
[1] 0.15 0.30 0.55
> normSum(complex(r=rnorm(3),i=rnorm(3)))
[1] 0.49+0.21i 0.30-0.18i 0.22-0.03i
\end{lstlisting}}
\caption{Sample R code}\label{sample}
\end{figure}

In R, function can be called with named parameters, R support variable
argument lists, and arguments can have default values. Putting all of these
together consider the following declaration:

\begin{lstlisting}[style=R]
f <- function(x, ..., y=3) x + y
\end{lstlisting}

\noindent
Function \k{f} can be called with a single argument \code{f(3)}, with named
argument \code{f(y=4,x=2)} and with a variable number of arguments,
\code{f(1,2,3,4,y=5)}, all of these calls will return \code{6}.

R has a number of features that are not crucial to the present
discussion. We will mention some of them here for completeness.  In R, data
structures are reference counted and have copy-on-write semantics, thus the
assignment \code{x[12]<-3} results in an update to a copy of \code{x} unless
the reference count on that object is 1.  This semantics gives R a
functional flavor while allowing updating in place within loops (the first
update copies, subsequent updates are performed on the copy). Arguments to
functions are evaluated only when needed, they are bundled in so-called
promises which package the original expression (as an AST), its environment
as well as the result of evaluating the expression. Promises can be
leveraged for meta-programming as it is possible to retrieve the text of a
promise and evaluate that in a different environment.

\subsection{Types of Data}

Before attempting to define a type system for R, we should understand the
different kinds of values that programs operate on.  As we will see
different notions of type may emerge depending on how granular we want to
be.

\renewcommand{\k}[1]{{\tt #1}\xspace}

R has one builtin notion of type that can be queried by the \k{typeof}
function. Over the years, programmers have found the need for a richer type
structure and have added attributes. The best way to think of attributes is
as an optional map from name to values that can be attached to any object.
Attributes are used to encode various type structures. They can be queried
with functions such as \k{attributes} and \k{class}.

\begin{wrapfigure}{r}{6.1cm}
\footnotesize\begin{tabular}{l|l@{}}\hline
\multicolumn{2}{l}{\bf Vectorized data types:}  \\\hline
\k{logical}  & vector of boolean values\\
\k{integer}   & vector of 32 bit integer values\\
\k{double} & vector of 64 bit floating points\\
\k{complex} & vector of complex values\\
\k{character} & vector of strings values\\
\k{raw} & vector of bytes\\
\k{list} & vector of values of any type\\\hline
\multicolumn{2}{l}{\bf Scalar data types:}\\\hline
\k{NULL}  &  singleton null value\\
\k{S4}    &  instance of a S4 class \\
\k{closure} & a function with its environment\\
\k{environment} & a mapping from symbol to value \\\hline
\multicolumn{2}{l}{\bf Implementation data types:}\\\hline
\multicolumn{2}{l}{\k{special},
\k{builtin},
\k{symbol},
\k{pairlist},
\k{promise}}\\
\multicolumn{2}{l}{
\k{language},
\k{char},
\k{...}, 
\k{any},
\k{expression},
}\\
\multicolumn{2}{l}{
\k{externalprt},
\k{bytecode},
\k{weakref}}\\\hline
\end{tabular}\caption{Builtin Types}\label{types}\end{wrapfigure}

Figure~\ref{types} lists all of the builtin types that are provided by the
language. They are the possible return values of \k{typeof}. There is no
intrinsic notion of subtyping in R. But, in many context a \k{logical} will
convert to \k{integer}, and an \k{integer} will convert to \k{double}.  Some
off conversion can occur in corner cases, such as \k{1<"2"} holds and
\k{c(1,2)[1.6]} returns the first element of the vector, as the double is
converted to an integer. R does not distinguish between scalars and vectors
(they are all vectors), so \code{typeof(5) ==} \code{typeof(c(5)) ==
  typeof(c(5,5))} \code{ == "double"}. Finally all vectorized data types have a
distinguished missing value denoted by \code{NA}. The default type of
\code{NA} is \k{logical}. We can see that \code{typeof(NA)=="logical"}, but
NA inhabits every type: \code{typeof(c(1,NA)[2])=="double"}.

With one exception all vectorized data types are monomorphic, the exception
is the \k{list} type which can hold values of any other type including
\k{list}. For all monomorphic data types, attempting to store a value of a
different type will cause a conversion. Either the value is converted to the
type of vector, or the vector is converted to the type of the value.

Scalar data types include the distinguished \k{NULL} value, which is also of
type \k{NULL}, instance of classes written using the S4 object system,
closures and environments.  The implementation of R has a number of other
types that are mostly not used by user code, they are listed in
Figure~\ref{types} for reference.

The addition of attributes lets programmers extend the set of types by
tagging data with user-defined attributes. For example, one could define a
vector of four values, \code{x<-c(1,2,3,4)} and then attach the attribute
\k{dim} with a pair of numbers as value: \code{attr(x,"dim")<-c(2,2)}.  From
that point, arithmetic functions will treat \k{x} as a 2x2 matrix. Another
attribute that can be set is the \k{class}.  This attribute can be bound to
a list of class names. For instance, \code{class(x)<-"human"}, set the class
of \k{x} to be \k{human}.  Attributes are thus used for object-oriented
programming. The S3 object system support single dispatch on the class of
the first argument of a function, whereas the S4 object system allows
multiple dispatch (on all arguments). Some of the most widely used data
type, such as data frames, leverage attributes. A data frame, for instance,
is a list of vectors with a class and a column name attribute.

\paragraph{Summary.} The most common values in R computations are vectorized
types. R programs do not have a way to constrain values to be scalar.
\k{NULL} is sometimes used to represent the case when no value is
available. \k{NA} is used within vector to represent missing observations.
Attributes can decorate values and are used as building blocks for
object-oriented programming. A potential type system for R could focus only
on the builtin types, if one wanted to strive for simplicity, or it could
try to capture attributes at the risk of increased complexity.

\newpage
\section{Corpus}\label{sec:corpus}

In this section, we present our dataset. R aims to accommodate data
analysts, their workflows start with data import, followed by cleaning, and
then by steps of modeling, transformation and visualization. Often, the code
of these analysis pipelines resides, together with the data and results, in
notebooks. Few notebooks are publicly shared, and when they are, the data
isn't. For this reason our analysis focuses on packages which bundle
reusable units of R code with documentation, sample data and use-cases.

We focus on packages hosted on the \emph{Comprehensive R Archive Network} or
CRAN.  With over 13,000 packages, CRAN is the largest repository of software
written in R. It is experiencing sustained growth with an average of size
new packages a day~\cite{LIgges2017}.  Unlike sites like GitHub, CRAN is a
curated collection of packages: A package is only accepted to CRAN if it
abides by a number of well-formedness rules.  Most relevant for our
purposes, packages must have data, examples, vignettes and tests, all of
which must successfully run. From our perspective this means that each
package in CRAN comes with several executable scripts that exercise some of
its functionality.  Notable exceptions to this rule are packages only
containing data, which have no runnable code but are referenced by other
packages \AT{I should collect stats on this}.

Our corpus is a subset of CRAN, consisting of \PACKAGES packages, accounting
for some \PERCENTCRAN of all packages.  These packages have a total of \RLOC
lines of R code and \CLOC lines of C code. For each package, we extracted
all executable code snippets from documentation, vignettes and tests and ran
them independently recording all calls to R functions.  It is noteworthy that
in order to run the scripts in one package, it is often necessary to load a
number of other packages.  In~\cite{issta18}, the authors estimated code
coverage to be around 68\% when including reverse dependencies.  As our infrastructure
adds overhead to script execution, coupled with the fact that some scripts take 
inordinate amounts of time to run, we limit our analysis of any given package to one hour.
We capture and record function signatures on-the-fly, though, so we do collect (partial) data even 
for these long-running packages. \AT{We expect to extend our coverage of CRAN for the next version of this paper.}

Figure~\ref{most} shows the ten most downloaded CRAN packages.  For each
one, we list how many lines of R and C/C++ the packages contains.
Typically, C/C++ code is used to implement performance critical portions of
the code. We show the number of scripts that could be extracted from the
package. Each script corresponds to either one use-case or a set of unit
tests.  Figure~\ref{allcloc} shows the size of the code across all the
packages we have used.  Lastly, Figure~\ref{recorded} gives the number of
function signatures that we recorded while executing the scripts of each
package.

\AT{TODO Change "recorded" to "observed" in recordsbypkg}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{linesofrandccode}
\caption{Lines of code in the analyzed corpus. \# of lines of R code are above 0, \# of lines of C code are below 0.}
\label{allcloc}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{recordsbypkg}
\caption{Numbers of Recorded Function Invocations in the Analyzed Corpus}
\label{recorded}
\end{center}
\end{figure}

% https://www.r-pkg.org/downloaded
\begin{figure}[!th]{\footnotesize\begin{tabular}{@{}r||l|r|r|r|r|r@{}}\hline
\bf Package & \bf Description & \bf R LOC &\bf C LOC &\bf Scripts & \bf Calls Observed & \bf Calls Recorded \\
\hline
\tt Rcpp  & Seamless C++ integration & 2.1K & 4K & 25 & 55K & 282 \\
\tt rlang & Functions for 'Tidyverse' & 4.4K & 3.8K & 122 & 3,924K & 5,171 \\
\tt glue  & Interpreted string literals & 0.2K & 0.2K & 8 & 4K & 120 \\
\tt tibble & Simple data frames & 1.3K & 0.3K & 16 & 1,332K & 4,148 \\
\tt stringi &  String processing & 1.5K & 515K & 64 & 923K & 666 \\
\tt ggplot2 & Data visualisations & 10.8K & 0 & 130 & 153K & 2,636\\
\tt dplyr  &  Data manipulation & 3.8K & 4.7K & 78 & 233K & 1,967 \\
\tt pillar & Formatting for columns & 1.3K & 0 & 13 & 803K & 1,273 \\
\tt R6 & Classes w. ref. semantics & 0.6K & 0 & 2 & 1K & 226 \\
\tt stringr & String operations & 0.5K & 0 & 32 & 1,764K & 371 \\
\end{tabular}}\caption{10 Most Downloaded Packages.}\label{most}
\end{figure}

\newpage
\section{Methods}

In this section, we will detail our methodology for collecting data.  The
basic idea we are exploring is to observe arguments and return values of
function calls, and from these generalize possible type signatures for the
called functions. To this end, we build on an open source tool called
\genthat whose purpose is to generate unit tests for R
libraries~\cite{issta18}.  To generate test cases, one must record and
accurately recreate arguments and return values and avoid generating
redundant tests (i.e. tests that do not increase code coverage). For our
purposes, we change the existing tool in two main ways: We record argument
\emph{shapes} rather than their values, and given the limited number of shapes
this allowed us to avoid having to run the code coverage minimization phase of
the tool. Both changes were beneficial for scalability---allowing us to
trace more calls than was reported in~\cite{issta18}.

\subsection{Example}

To illustrate our approach, consider the following script which adds a double
to an integer, and then creates a matrix and a vector, and adds them together to
get a new matrix.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
> 1 + 2L
[1] 3
> x <- c(1,2,3,4)
> attr(x,"dim") <- c(2,2)
> x + c(1,2)
     [,1] [,2]
[1,]    2    4
[2,]    4    6
\end{lstlisting}}\caption{Example script}\label{example}\end{figure}

The functions being called here are \k{`+`}, \k{`<-`}, \k{c} and
\k{`attr<-`}.  Some functions syntactic support \AT{?}, addition is infix, and the
assignment to an attribute is written mixfix. The shapes we would expect to
record are as follows:

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
`+`: D[1], I[1] -> D[1] # (args: double size 1, integer size 1, return: double size 1)
c: D[1], D[1], D[1], D[1] -> D[4]
`<-`: S, D[3] -> D[3]
c: D[1], D[1] -> D[2]
`attr<-1`:  S, C, D[2] -> 
c: D[1], D[1] -> D[2]
`+`: D[4]{dim=D[2]}, D[2] -> D[4]{dim=D[2]}
\end{lstlisting}}\caption{Recorded shapes}\label{shapes}\end{figure}

In the above we abreviate types, \k{double}, \k{integer}, \k{symbol} and
\k{character}. For vectorized types we record their length and for all types
we record attributes with some of their values. Looking at the signatures
observed for addition, it is clear that the function is polymorphic as it
starts with the addition of two scalar numbers of different types, and then
adds a matrix to a vector returning a matrix.

\subsection{Implementation}

A high-level description of the workflow of our tool for one package
retrieved from CRAN is as follows:

\begin{enumerate}
\item {\bf Exec generation:} All runnable code in the package is extracted
  from its tests, examples and vignettes. The code snippets are combined
  into a single file.
\item {\bf Installation:} All packages that are required for execution of
  the current package are downloaded and installed.
\item {\bf Instrumentation:} As code is loaded into the R, every function
  definition is instrumented with an \code{on-exit} hook which is invoked
  when the function returns either normally or through an exception.
\item {\bf Recording:} When a hook is called, arguments and return value are
  inspected. We record \k{typeof}, \k{class} and \k{attributes} recursively.
  For \code{list} values, an extra bit of analysis is performed to record
  the element type.
\item {\bf Writing:} Unique signatures are recorded to file with information
  about which package triggered the recording.
\end{enumerate}

Our recording mechanism does not remember the order in which arguments were
passed, nor does it record which arguments were not passed (and for which
default values were used). If an argument was not passed, and no default
value was specified, we record it as \emph{missing}. In R, trying to use
such a missing value results in an error. For practical reasons, we do not
record the contents of environments. These can be used as hash tables and
may be big and are quite likely different from one another.

In R all arguments are passed as promises, unused arguments will be
unevaluated, for these we can not report a type. THUS WE DO WHAT? SAY MORE.

For each function of each package, we consolidate all recorded observations
into a function signature. The details of how consolidation is parameterized
by the definition of a potential type system for R and will be discussed in
the next section.


\section{Stuff}

We will need a notion of polymorphism when undertaking this analysis. R
is unityped, and the only notion of polymorphism in the language is its use
of dynamic dispatch on the class of an argument.  For our purposes, we will
define 3 simple types of polymorphism, closely connected with the reflection
functions we use.  We start by defining what it means for an {\it argument}
to be polymorphic.  \AT{Could be valuable to have an example here.}

\begin{itemize}
\item an argument is {\it polymorphic in type} if it has been called with at
  least two distinct ``types'' according to the \code{typeof} function and
  our extra analysis.
\item similarly, an argument is {\it polymorphic in class} if it has been
  called with at least two distinct classes according to the \code{class}
  function.
	
\item attributes are a bit more tricky.  We can't simply count the number of
  attributes that have inhabited an argument, since values can contain
  arbitrarily many attributes (and having several attributes isn't
  polymorphism---think records).  Instead, we define the {\it attribute
    pattern} of a value to be the set of all attribute name and type pairs
  in the attribute list of a value (obtained via \code{attributes}).  And so
  an argument is {\it polymorphic in attribute} if it has been called with
  at least two distinct attribute patterns.

\end{itemize}

There is an important distinction to be made between the result of
\code{typeof} and that of \code{class} and \code{attributes}: In R, the
\code{typeof} a value cannot change, and this is not the case with class and
attributes.  As mentioned in \AT{an earlier section}, the class of values
can be easily redefined by assigning to the value's class attribute.
Similarly, attributes can be dynamically added, removed, or changed.

This begs the question: what sort of classes and attribute patterns occur
naturally, before possible user tampering?  In terms of classes, the
discussion can be found in Section~\ref{sec:rlang}.  As for attributes:

\begin{itemize}

\item data frames have three attributes: {\tt names} for column names, {\tt
  row.names} for row names, and {\tt class} (which is instantiated to {\tt
  "data.frame"} upon creation).
	
\item matrices have at least a {\tt dim} attribute with matrix dimensions,
  and optionally a {\tt dimnames} attribute for dimension names.

\item lists and vectors may have a {\tt names} attribute, which assigns
  names to locations.
	
\item to ease dealing with large sorted sets of non-numeric values, R offers
  the \code{factor} and \code{levels} functions.  \code{factor(x,
    levels=someOrder)} adds a {\tt "levels"} attribute to \code{x}, which
  specifies an ordering for \code{x} according to the \code{sortedOrder}
  parameter.  \code{factor} also changes the class of the parameter being
  factored to \texttt{"factor"}.

\end{itemize}

These naturally-occurring attribute patterns and classes can be separately
accounted for to highlight user-defined patterns and behaviors.  This will
be made clear in Section~\ref{sec:results}.

%
%

%
\subsection{Granularity of Type Information}

As we mentioned in Section~\ref{sec:rlang}, R has several notions of
``type'', ranging from the results of \code{typeof} to metadata on values
obtainable via the \code{attributes} function.  Individually, these paint an
incomplete picture of the type of a value: For example, recall that
\code{typeof(5) == typeof(c(5, 5)) == "double"}.

To be as specific as possible, our analysis collects additional information
to create a more specific type than the result of any of R's reflection
functions.  For instance, if the analysis encounters {\tt typeof(x) ==
  "double"}, it will look at the length of {\tt x} to determine if {\tt x}
is a scalar or a vector, generating the annotation {\tt scalar/double} or
{\tt vector/double} accordingly.  One notable imperfection in this example
is that unit-length vectors will appear as scalars, but if some function was
called with unit-length vectors {\it as well as} larger ones, both scalar
and vector types will appear, and this distinction can be later collapsed.
Another example is that our analysis builds parametric list types by
capturing the types of list elements.  We also distinguish NAs ``in the
wild'' with a {\tt raw\_NA} type.

%
%
%
%
%
%
\section{Examples of Polymorphism}
\label{sec:polyex}

\AT{Putting examples of polymorphism in this section, we can talk about them and where they should go.}

First, Figure~\ref{fig:realex} shows an example of how integers and doubles are very compatible.
\AT{TODO add matrix (dimensions) to the example.}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
> 5L + 1L # => 6L, an integer
> 5L + 1.2 # => 6.2, a double (5L coerced to 5.0)
> c(10, 20, 30)[1.2] # => 10, 1.2 coerced to 1L
\end{lstlisting}}\caption{Example of {\it real} type usage.}\label{fig:realex}\end{figure}

Figure~\ref{fig:optnull} shows an example of a function with an optional argument.
In the function, {\tt x} is a sorted vector of income values, and {\tt w} is a vector of weights, optionally NULL.
The function computes fractional ranks required for the computation of some coefficient implemented by the package.
We see that if {\tt w} is NULL, then default unit weights are generated.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
frac.ranks <- function(x, w=NULL) {
  if (is.null(w)) w <- rep(1, length(x)) # if no weights passed, take all weights = 1
  ...
\end{lstlisting}}\caption{Example of optional argument (from {\tt acid} package).}\label{fig:optnull}\end{figure}

Figure~\ref{fig:listvec} shows an example of a function with list and vector polymorphism.
The function takes a list or vector {\tt point} and a data frame {\tt polyg} representing a polygon.
In it, we see that {\tt point} can be either a list or a vector, and the function code casts the argument to a double vector.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
is.point.inside <- function (point, polyg) {
    p <- as.numeric(point) # as.numeric(list(1, 2)) => c(1, 2)
    ...
\end{lstlisting}}\caption{Example of list/vector argument (from {\tt bivrp} package).}\label{fig:listvec}\end{figure}

Figure~\ref{fig:charclos} shows an example of a function with character and function polymorphism.
\AT{Bluh.}
At a high level, this function calculates point estimates for an {\tt angmcmc} object (specific to some packages).
The caller specifies {\tt fn}, either a function or a function name which will be evaluated on the object samples to estimate parameters.
The call to lookup the function name is on line 10.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
pointest <- function (object, fn = mean, par.name, comp.label, chain.no,  ...) {
    ...
    if (is.character(fn)) {
        if (fn == "MODE" | fn == "MAP") {
            do_MAP <- TRUE
        }
        else {
            do_MAP <- FALSE
            fn <- match.fun(fn) # looks up the function name
        }
    }
    ...
\end{lstlisting}}\caption{Example of char/closure argument (from {\tt BAMBI} package).}\label{fig:charclos}\end{figure}

\AT{I'll explain this better, maybe?}
In Figure~\ref{fig:dfdbl}, we see a function that, among other things, takes in some data ({\tt dat}) and a character string ({\tt spss}) specifying either {\tt "in"} or {\tt "out"}.
Now, depending on the value of {\tt spss}, {\tt dat} will either be a data frame or a vector of doubles.
\AT{This ties in to the (arg, retv) coupling, which we will have data on soon.}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
nret.translator <- function(dat, spss="out", ...) {
  ...
  if(identical(spss, "out")) {
    if(!is.vector(dat)) {
        stop(simpleError("...'dat' must be a vector!"))
      }
    ...
  } else {
    ...
    # here, dat is data.frame
    items.idx <- items.idx[order(names(dat[, items.idx]))]
  }
\end{lstlisting}}\caption{Example of a data.frame/(vector/double) argument (from {\tt klausR} package).}\label{fig:dfdbl}\end{figure}

In Figure~\ref{fig:chardbl}, we see a function which takes in a list ({\tt network}), a vector of indices of that list ({\tt fixIndices}), and a vector of values ({\tt values}).
The locations specified by {\tt fixIndicies} in {\tt network} are updated with {\tt values}.
Here, {\tt fixIndicies} has been observed to be either a character or double vector:
In R, list indices are typically doubles, but can be characters (if the list or vector being indexed has a {\tt names} attribute).

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
fixGenes <- function (network, fixIndices, values) {
  ...
  network$fixed[fixIndices] <- as.integer(values)
  ...
}
\end{lstlisting}}\caption{Example of character/double argument (from {\tt BoolNet} package).}\label{fig:chardbl}\end{figure}

In Figure~\ref{fig:matvec}, we see that the function can take in a vector, but immediately transforms it (transpose, with {\tt t}) into a matrix.
As an idea, matrix/vector polymorphism seems sensible, as mathematically vectors are matrices.
R echoes this by making conversion between the two ``types'' easy:
\code{as.vector(m)} flattens a matrix \code{m} into a vector (e.g., \code{as.vector(matrix(2, 2, 2)) == c(2, 2, 2, 2)}), and \code{as.matrix(v)} builds a {\tt length(v) x 1} matrix (e.g., \code{as.matrix(c(1, 2)) == matrix(data=c(1, 2))}).
\AT{TODO fix line break in code at end of last sentence}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
tee <- function (x, theta, D1, D2, phi)  {
    if (is.vector(x)) {
        x <- t(x)
    }
    ...
\end{lstlisting}}\caption{Example of matrix/(vector/double) argument (from {\tt calibrator} package).}\label{fig:matvec}\end{figure}

%
%
%
%
%
%
\section{Results}\label{sec:results}

\AT{Meta-Comment:} My plan for this section is to linearly go through the
data, and show logical transitions between various type systems.  I'll start
with the finest-grained one.

Having built an understanding of our data collection and analysis
pipeline, we will turn our attention to the results of our analysis.  We
approach the data with the purpose of informing the design of a static type
system for R. 

The logical ``baseline'' for the design of a static type system for R is
promotion of the language's dynamic types (in the sense of \code{typeof}) to
static types, but there are some clear limitations to this approach.  For
example, \code{typeof} makes no distinction between vectors and scalars, as
indeed there is no such distinction in R.  But that doesn't mean that there
{\it couldn't} be a distinction, as an all-new runtime environment design
for R might benefit from e.g. not needing to vectorize scalar values.

To address these limitations, we augment the logical naive type system with
more information which was alluded to in the discussion of our data
collection methodology.  We will consider unit-length vectors to be scalars,
collect list member type information to build an accurate list type, and
promote NAs to their own type.  In addition, we collect attribute and class
information to distinguish a type for data frames and matrices, as the
\code{typeof} these is simply {\tt list} in R.  This type system is referred
to as the {\it Fine-Grained Type System}, or FGTS.  An example of a signature in
this type system can be found in Figure~\ref{fig:exFGTS}.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
> add_make_list <- function(x, y) as.list(x + y)
> add_make_list(2, c(3, 1)) # => list(5, 3)
\end{lstlisting}}
\begin{tabular}{@{}r|l|l|l@{}}\hline
\bf Argument & \bf Type & \bf Class &\bf Attributes \\
x & \tt scalar/double & \tt numeric & \tt \{\} \\
y & \tt vector/double & \tt numeric & \tt \{\} \\
retv & \tt list<double> & \tt list & \tt \{\}  
\end{tabular}
\caption{Example call and FGTS signature}\label{fig:exFGTS}\end{figure}

The first question we will address is: {\it how polymorphic is R?}  To
answer that, we will first look at the number of monomorphic arguments
across all of the packages we analyzed.  The data can be found in
Tables~\ref{tab:argcountsFGTS}~and~\ref{tab:funcountsFGTS}.
\AT{TODO: have these tables be generated automatically}
\AT{TODO: put all counts in one table?}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:03:30 2019
\begin{table}[ht]
\label{tab:argcountsFGTS}
\centering
\begin{tabular}{lrr}
  \hline
 Argument Polymorphism & Count & Percentage \\ 
  \hline
  Full Monomorphic & 784K & 85.28 \\ 
  Monomorphic in Type & 836K & 90.91 \\ 
  Monomorphic in Class & 831K & 90.31 \\ 
  Monomorphic in Attribute Pattern & 830K & 90.25 \\ 
  Total Seen & 920K & --- \\ 
     \hline
\end{tabular}
\caption{Account of {\it argument} polymorphism in the FGTS.}
\end{table}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:10:46 2019
\begin{table}[ht]
\label{tab:funcountsFGTS}
\centering
\begin{tabular}{lrr}
  \hline
 Function Polymorphism & Count & Percentage \\ 
  \hline
  Full Monomorphic & 143K & 65.80 \\ 
  Monomorphic in Type & 167K & 76.84 \\ 
  Monomorphic in Class & 163K & 75.13 \\ 
  Monomorphic in Attribute Pattern & 161K & 74.31 \\ 
  Total Seen & 217K & --- \\ 
   \hline
\end{tabular}
\caption{Account of {\it function} polymorphism in the FGTS.}
\end{table}

\AT{We should figure out the pipeline to get the notebook output into here.}

In the tables, ``Full Monomorphic'' denotes monomorphism in type, class, and attribute.
We see that roughly 9\% of all argument positions in R are polymorphic in type.
The numbers for attribute and class polymorphism are included for
completeness, but the important number is that of type polymorphism (as our
method of constructing types sometimes factors in class and attribute).
Specifically for classes, often the class and \code{typeof} a value will
line up (refer to Section~\ref{sec:rlang}).

As for functions, roughly 22\% are polymorphic in type, or put differently 22\% have at least one polymorphic argument or a polymorphic return.
The discrepancy between this and the polymorphicity of arguments is explained by the tendency of some functions to have {\it many} arguments.

These numbers alone are insufficient, so we ask: {\it of the polymorphic arguments, what are the most common patterns?}
The answer to that question can be found in Table~\ref{tab:toppolyFDTS}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 21:27:37 2019
\begin{table}[ht]
\label{tab:toppolyFDTS}
\centering
\begin{tabular}{lrr}
  \hline
 Polymorphic Type Signature & \# Occurrences & \% of Total Polymorphism \\ 
  \hline
  vector/double, vector/integer & 7677 & 9.18 \\ 
  scalar/double, scalar/integer & 7194 & 8.61 \\ 
  matrix, vector/double & 5354 & 6.40 \\ 
  NULL, scalar/character & 2333 & 2.79 \\ 
  list$<$any$>$, list$<$double$>$ & 2224 & 2.66 \\ 
  data.frame, matrix & 2115 & 2.53 \\ 
  NULL, scalar/double & 2066 & 2.47 \\ 
  NULL, vector/double & 2042 & 2.44 \\ 
  list$<$any$>$, list$<$list$>$ & 1426 & 1.71 \\ 
  NULL, vector/character & 1385 & 1.66 \\ 
     \hline
\end{tabular}
\caption{Top polymorphic argument signatures in the FDTS.}
\end{table}

\AT{This should probably be discussed earlier.}
Not pictured here, the most common polymorphic pattern in the FGTS is the {\tt scalar/X}, {\tt vector/X} pattern, but this is dealt with separately:
If a function argument is observed to be both a unit-length and non-unit-length vector, then it's reasonable to assume that the argument is intended to be a vector, so we collapse the distinction in such cases.

In Table~\ref{tab:toppolyFDTS}, we see the prominence of {\tt double} and {\tt integer} polymorphism.
In many dynamic languages, primitive double and integer types can easily stand-in for each other as coercion between them is easy.
In R, the distinction is mainly just relevant to the implementation, as integers and doubles are stored and dealt with differently in the runtime but programmers only see the distinction when printing.
We address this by creating a new type, {\it real}, encompassing both.
The expected coercion is evident throughout R.
Refer to the snippet in Figure~\ref{fig:realex} for details.

As we make the distinction between vectors and scalars, one other interesting question that arises is: {\it how many arguments are strictly scalar?}
As it happens, over 40\% of monomorphic arguments with primitive vectorized types are always scalar (so, 40\% of {\it monomorphic} occurrences of {\tt scalar/X} or {\tt vector/X} are scalar).
This is important as it shows that there is value in a scalar annotation, which could be used to communicate to an R runtime that vectorization of a value is not necessary, which may in turn lead to a performance improvement.

Another clear pattern in Table~\ref{tab:toppolyFDTS} is that of NULL, X.
In many programming languages, NULLs inhabit every type, and this is not the case in our FGTS.
The FGTS reflects many of R's implementation details, and this distinction is no exception:
NULLs are represented as \AT{fill in} in the R runtime.

In terms of programmer behavior, we observe a pattern of optional arguments throughout several R packages, wherein programmers specify a default argument value which is sometimes NULL.
Consider the snippet in Figure~\ref{fig:optnull} for an example.
\AT{TODO put that figure here}
The distinction between NULL and other types is not particularly useful to the programmer, so we propose to also collapse this distinction.

Not pictured in Table~\ref{tab:toppolyFDTS} are NAs, which are ascribed the type {\tt raw\_NA} in FGTS; NAs in fact inhabit every type, though the default NA type is {\tt logical}.
Just as we fold NULL into all types, we will do the same with NAs.

The next type system design we will consider is ... .
The number of monomorphic arguments and functions can be found in Tables~\ref{tab:argcountsWHATTS} and~\ref{tab:funcountsWHATTS} respectively.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:03:30 2019
\begin{table}[ht]
\label{tab:argcountsWHATTS}
\centering
\begin{tabular}{lrr}
  \hline
 Argument Polymorphism & Count & Percentage \\ 
  \hline
  Full Monomorphic & 785K & 85.38 \\ 
  Monomorphic in Type & 871K & 94.65 \\ 
%  Monomorphic in Class & 831K & 90.31 \\ 
%  Monomorphic in Attribute Pattern & 830K & 90.25 \\ 
  Total Seen & 920K & --- \\ 
     \hline
\end{tabular}
\caption{Account of {\it argument} polymorphism in the NEW TYPE SYSTEM.}
\end{table}

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Fri Mar 22 18:10:46 2019
\begin{table}[ht]
\label{tab:funcountsWHATTS}
\centering
\begin{tabular}{lrr}
  \hline
 Function Polymorphism & Count & Percentage \\ 
  \hline
  Full Monomorphic & 143K & 65.95 \\ 
  Monomorphic in Type & 184K & 84.778 \\
%  Monomorphic in Class & 163K & 75.13 \\ 
%  Monomorphic in Attribute Pattern & 161K & 74.31 \\ 
  Total Seen & 217K & --- \\ 
   \hline
\end{tabular}
\caption{Account of {\it function} polymorphism in the NEW TYPE SYSTEM.}
\end{table}

We had some big wins here, with only ~5\% of argument polymorphism unaccounted for.
To get an idea of what more we can do, let's see what the most frequent remaining polymorphic signatures are.
See Table~\ref{tab:toppolyWHATTS}.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Tue Mar 26 17:46:45 2019
\begin{table}[ht]
\label{tab:toppolyWHATTS}
\centering
\begin{tabular}{lrr}
  \hline
Polymorphic Type Signature & \# Occurrences & \% of Total Polymorphism \\ 
  \hline
  matrix, vector/real & 6307 & 12.82 \\ 
  list$<$any$>$, list$<$real$>$ & 2792 & 5.68 \\ 
  vector/character, vector/real & 2719 & 5.53 \\ 
  scalar/character, scalar/real & 2536 & 5.16 \\ 
  data.frame, matrix & 2163 & 4.40 \\ 
  list$<$any$>$, list$<$list$>$ & 1492 & 3.03 \\ 
  list$<$any$>$, list$<$character$>$ & 1217 & 2.47 \\ 
  data.frame, vector/real & 1025 & 2.08 \\ 
  scalar/logical, scalar/real & 917 & 1.86 \\ 
  scalar/character, vector/real & 844 & 1.72 \\ 
   \hline
\end{tabular}
\caption{Top 10 argument signatures in the NEW TYPE SYSTEM.}
\end{table}

What's up with matrix, vector/real?

%
%
%
%
\subsection{Types in R}

\AT{I'll pull from this section when appropriate, as the discussion of the type systems becomes relevant.}

%
%
\subsubsection{What are possible type systems?}

Great question.
Some things:

\begin{itemize}

\item {\it The Obvious Type System (OTS)}: this type system corresponds directly with the type information that one can fetch using the {\tt typeof}, {\tt attributes}, and {\tt class} functions.
In this system, there is no distinction between vectors and scalars, and lists of differently-typed elements (e.g., a list of integers vs. a list of strings), and there is a distinction between integers and doubles, and the type of NULL and all other types.
This type system isn't particularly informative to a programmer, though it does reflect the treatment of values in the R runtime, for the {\tt typeof} a value is the type that the value has according to the R internals.

\item {\it The Fine-Grained Type System (FGTS)}: this type systems includes a number of relevant additions.
Chiefly, it distinguishes between vectors and scalars, as it is conceivable that boxing scalars as single-element vectors might be overkill.
In addition, list types are parameterized over the types of their elements (e.g., list<integer> vs. list<character>).
Finally, NAs ``in the wild'' are given the type {\tt raw\_NA}, as {\tt typeof(NA) = logical} in R.

\item {\it The Null-and-NA-Free Type System (NNTS)}: most languages consider NULLs to inhabit all types, and R does so for NAs \AT{elaborate}.
This gives us ... .

\end{itemize}

Of course, one could conceive of a more fine-grained type system than FGTS, but it's unclear that we would gain much in doing so.
Indeed, there is a space to explore in slightly less fine-grained type systems, which make modifications including:

\begin{itemize}

\item {\bf collapsing vectors and scalars}: this yields a type system slightly closer to OTS.

\item {\bf rolling NULL into other types}: in most languages, NULL is an inhabitant of all types, and this adjustment seems interesting.
Note that this goes beyond even OTS, as in R {\tt typeof(NULL) = NULL}.

\item {\bf rolling NA into other types}: \AT{i should try this}
similarly, NA is an inhabitant of all types, though NA standing alone has type {\tt logical}.
FGTS distinguishes such NAs with the {\tt raw\_NA} type, and if instead we consider NA to inhabit all types we may \AT{win}.

\item {\bf combining integers and doubles}: as in most languages, doubles and ints can stand-in for each other in nearly every scenario.
Even if not, it is trivial to convert from one to the other, particularly converting ints to doubles.
Aside, in R a double can be used to index a list, and R will floor it when performing the lookup.

\end{itemize}

%
%
%
%
\subsection{Usage Patterns}

In this section, we will discuss function usage patterns which arose in our analysis.
We will start by looking at the morphicity of functions.

\subsubsection{Function Argument and Return Morphicity}

First and foremost, we would like to know how often R programmers create polymorphic functions.
Recall that we define a polymorphic function to be a function with at least one polymorphic argument, or a polymorphic return.
We will turn our attention now to the data in \todo{Figure}, and go through each entry in the table.

Here, we see that the vast majority of function arguments are indeed monomorphic.
Monomorphic arguments are easy to annotate, as the {\tt typeof} the arguments is exactly the most precise annotation we could give, at least in terms of \textit{type}.
That said, types alone don't always paint the whole picture:
recall that \textit{attributes} are an R language feature which allows programmers to stick metadata onto values.
So in which circumstances \textit{do} types paint the whole picture?

The second data point in \textbf{TODO FIGURE} indicates that a
\isit{majority} of function arguments are monomorphic in type \textit{and}
have no attributes.  These represent arguments which are truly trivial to
annotate, as the {\tt typeof} an argument perfectly describes the usage of
that argument.  That said, some attributes arise naturally in R: For
instance, names in a named list (e.g. {\tt x} and {\tt y} in {\tt list(x=0,
  y=0)}) appear as an attribute on the value.  In
Section~\ref{sec:method:attributes}, we outlined these naturally-occurring
attribute patterns, and the third data point in \textbf{TODO FIGURE} shows
that a nontrivial amount of functions are monomorphic in type with said
natural attribute patterns.

Another facet of type information in R is in the {\tt class} attribute.
Recall that values have a \textit{class} in addition to a type: For example,
a {\tt data.frame} has type {\tt list} and class {\tt data.frame}.
\todo{Plug classes earlier} R has a number of built-in classes, such as
\todo{X, Y, and Z}, but users are free to redefine the class of any value at
runtime, and easily define new classes.  The next data point in
\todo{Figure} shows that user-defined classes don't appear altogether often,
though they do indeed feature.  \AT{In a following section, we will discuss
  how the usage of these classes manifests itself.}

\AT{Separate section for functions? Right now, this is at argument granularity.}

\subsubsection{Type Signatures}

In the last section, we presented a high-level overview of the morphicity ... .

\subsubsection{Attribute Signatures}

Recall that attributes are a way for programmers to store metadata on values in R.
What are the common attribute patterns?
And how often is the attribute pattern polymorphic?

\subsubsection{Takeaways}

\begin{itemize}
    \item the vast majority of arguments are monomorphic in type;
    \item of those, over 60\% have no attribute information;
    \item of the 40\% with attribute information, roughly 1/2 have fairly
      simple attributes corresponding to base R constructs (named lists and
      vectors, matrices, and data frames);
    \item now, of those arguments which are polymorphic, a sizable chunk (well over half) have defensible signatures (e.g., double and character for named list indexing, double and integer for obvious reasons, etc.).
\end{itemize}

In short, it looks like R (package) programmers are reasonable.  I'd
conjecture that a lot of the polymorphism (e.g., double and list) is coming
from how easy it is to use either type in a given situation (e.g.,
converting from vector of doubles to list or vice versa is simple).

%
%
%
%
%
%
\section{Synthesis}

In this section, we will discuss the conclusions that we draw from our data.


%
%
\subsection{Suggested Annotations}

\AT{How can we capture these patterns with annotations?}

Some possible annotations:

\begin{itemize}
    \item \textit{real}: for \textit{double} and \textit{integer} values
    \item \textit{function}: for \textit{closure}, \textit{special}, and \textit{builtin} values
    \item \textit{vector}: to indicate that something should be vectorized
    \item \textit{scalar}: to indicate that something should \textbf{not} be vectorized
    \item \textit{index}: for \textit{real} and \textit{character} values
\end{itemize}

%
%
\subsubsection{Struct-Like Attribute Declarations}

\AT{What's a convenient way to annotate attributes?  Should investigate how
  often attributes are consistent.}

%
\subsubsection{Coverage}

\AT{What is the coverage of these new annotations?}

%
%
%
%
\section{Related Work}

\AT{Not sure what the best place for this is, but we should discuss these papers somewhere.}

The idea of measuring polymorphism via some program analysis isn't new.
For instance, some work~\cite{aakerblom2015measuring} ... .

More generally, there is an existing literature on analyzing usage patterns
of language features.  For instance, the dynamic features of Smalltalk were
analyzed in some work~\cite{callau2011howdevelopers}, ...

Other analysis work~\cite{milojkovic2017duck} ... .

\section{Conclusions and Future Work}

\bibliographystyle{boilerplate/ACM-Reference-Format}
\bibliography{bib/biblio,bib/jv,bib/r,bib/new}

\end{document}
