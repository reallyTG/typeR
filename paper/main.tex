\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings}
\lstset{language=R}
\usepackage{my_style}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A Large-scale Study of Polymorphism in R}

\begin{abstract}
The R programming language is widely used in a variety of scientific domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in over X millions
of lines of R code spread among some 7,500 packages, written over a period of
over 20 years by over 10,000 programmers.  We perform a dynamic analysis,
leveraging tests and use-cases, to determine the level of polymorphism that
is present in the code. We do this for several potential notions of
types. Our result suggest that polymorphism is important in some key parts
of the system but that relatively simple type annotations could be used to
capture most of the interesting cases.
\end{abstract}

\maketitle

\section{Introduction}

Our community builds, improves, and reasons about programming languages.  To
make design decisions that benefit most users, we need to understand the
language we are working with as well as the real-world needs it
answers. Often, we, as researchers, can appeal to our intuition as many
languages are intended to be general purpose and appeal to users with some
computer science training. Unfortunately, these intuitions don't always
apply to domain-specific languages, languages designed for and by a specific
group of users to solve very specific needs. This is the case of the data
science language R.

R and it's ancestor S are languages designed, implemented and maintained by
statisticians. Originally they were designed as glue languages, languages
that would allow to read data into vectors and call statistical routines
written in Fortran. Over three decades, the languages became widely used
across many fields of science and in industry for data analysis and data
visualization; with time additional features were added.  Modern R, as a
linguistic object of study, is fascinating. It is a vectorized, dynamically
typed, lazy functional language with limited side-effects, extensive
reflective facilities and retrofitted object-oriented programming support.

Many of the design decisions that gave us R were intended to foster an
interactive, exploratory, programming style. This includes, to name a few,
the lack of type annotations on variables and functions, the ability to use
syntactic shortcut, and the automatic conversion between data types.  While
these choices have led to a language that is surprisingly easy to use by
beginners --many data science programs do not teach the language itself but
simply introduce some of its key libraries-- they have also created a
language where almost all computations yield a numeric result and where
errors can go undetected. 

One way to increase assurance in the results obtained when using R would be
to add type annotations to functions and variable declarations. These
annotations could then be used, either statically or (more likely)
dynamically, to catch mismatches between expected and provided data values.
The nature of R is such that it is unlikely to be ever fully statically
checked, furthermore end users may not be willing to write types when
carrying out exploratory programming tasks. So, we are looking for an
optional type system that would allow us to capture as much of behavior of
library functions as possible while remaining easy to understand for
end-users and library developers alike.

This papers is a data-driven study of what a type system for the R language
could look like. Longer term, our intention is to propose changes to
language, but for any changes to be accepted by the user community, they
must clearly benefit the language without endangering backwards
compatibility. Our goal is thus to find a compromise between simplicity and
usefulness; the proposed type system should cover most common programming
idioms while remaining easy to use. In order to do this, we need to
understand the degree of polymorphism present in R code, that is to say, how
programmers leverage the dynamic nature of R to write code that can accept
arguments of different types.  This understanding will drive our design.



One way or another, R is here to stay, so we may as well figure out how to
deal with it.  One aspect of this is understanding R programmers and how
they interact with the language, and a step towards this goal is to
understand \AT{how types arise in R programs.}  We will shed some light on
this by analyzing the usage of R functions, and build a picture of how
polymorphic R code is.

\begin{itemize}
    \item Understanding R can help us improve the language, as well as
      better equip us to build languages which have an appeal outside of the
      realm of computer science.
    \item We're gonna tackle this by looking at how R programmers use types
      and type information in their programs.
    \item We will be mindful of the runtime representations of values to
      give more context to the information we glean from programmer-visible
      type information.
\end{itemize}

The main contributions of this work are:

\begin{itemize}
\item \AT{maybe} a tool to analyze the polymorphicity of R code;
\item a study of polymorphism in the R language \AT{using said tool};
\item a number of proposed static type systems for R which cover most of the
  type usage patterns which arise in R code.
\end{itemize}

%
\section{The R Programming Language}
\label{sec:rlang}

R is a lazy, side-effecting, dynamically-typed, ... .

Typical usage of the language is ... . \AT{Do we have something to cite for
  this?}  \AT{Should we motivate why we only look at R packages?  Some
  people might wonder why we don't look at non-library code.}

Most R libraries (called \textit{packages}) are stored in the Comprehensive
R Archive Network (CRAN) and Bioconductor package repositories.  \AT{Talk
  about important packages? tidyverse?}

\AT{Kind of ... want to combine the above two paragraphs somehow? Together,
  they lead in nicely to the genthat talk.}


\subsection{The \texttt{genthat} Package}

Unfortunately, getting our hands on ``front-line'' R code is tricky: R is
not like many other languages, where large systems are built up and stored
in a repository such as Github.  Instead, R programmers are liable to
leverage a number of packages and work them into their data analysis
pipelines.

Interestingly, we are somewhat able to replicate this using only package
code.  CRAN, has a policy that packages made available on the
CRAN platform should be accompanied by a series of examples, tests, and/or
vignettes showing off how the code is intended to be used.  Nominally
unrelated, the {\tt genthat} project~\cite{kvrikava2018tests} traces this swath of example
code to generate new examples which package designers may not have thought
of.  As it happens, we can leverage {\tt genthat} to extract dynamic type
information from running package test code, which presumably indicates
how package code is intended to be used.

\AT{Draft}

We modify the {\tt genthat} tracer to capture the type information of arguments.
{\tt genthat} generates traces for each unique function invocation with the express purpose of re-invoking the function at a later time.
Instead of generating traces for these invocations, we can instead capture type information of the arguments and return values of functions.
This is done by evaluating the arguments and return values of functions at the end of the function's scope in the relevant environment.
The sort of types and granularity of the type information that are generated can be specified in our tracer:
Essentially, our tracer is parameterized over a user-specified type system.
For instance, if desired our tracer can dig into lists and data frames to get the types of list elements and fields.
Our tracer produces trace results for each unique type signature encountered, which may then be used for data analysis.

An unfortunate consequence of R's most common usage pattern is the lack of "deployed" R code.
R is most often used for exploratory data analysis, and very little of this analysis code is made publicly available on e.g. GitHub.
The {\tt genthat} angle is thus desirable:
With the scarcity of non-library code, examples of package usage is about as representative as one can hope for of the sort of code people will write in their analysis pipelines.

We can do a little bit better than merely package usage examples, though.
Available through CRAN is a list of {\it reverse dependencies} of packages:
for some package {\tt p}, a list of all packages which require {\tt p} in some way.
Our tracer is able to leverage and trace this code as well.
When running a package, we also trace how all inherited functions are used, thus painting an even clearer picture of function usage.
This additional data is considered alongside similar data collected for all other packages, as well as the data obtained by tracing the package's examples.

% Some prose we might want to pull from.
%First, we leverage {\tt genthat}'s tracer to trace function executions.  The
%idea here is that we don't really have access to non-library code written in
%R, as general use patterns are (possibly?) to write small scripts which
%analyze some bit of data and possibly visualize results.  The goal is likely
%not to build big working systems, instead to explore data with by writing
%and rewriting small scripts, ad infinitum.  In looking at package tests,
%vignettes, and examples, we are painting a picture of how the package
%designers intended their packages to be used, which we believe is a close
%approximation of what R users would do.


\section{The Method}

In this section, we will detail our methodology for collecting data.

%
%
%
%
\subsection{Data Collection Pipeline}

As we mentioned in the previous section, we modify the {\tt genthat} tracer to collect and output the type information for all function invocations.

\AT{somewhere}
Our analysis can be parameterized by a user-specified ``type system'', simply a map between existing dynamically observable type information and user-defined types.
If no type system is specified, the analysis gains as much information as it can.
For instance, if the analysis encounters {\tt typeof(x) == "double"}, it will look at the length of {\tt x} to determine if {\tt x} is a scalar or a vector, generating the annotation {\tt scalar/double} or {\tt vector/double} accordingly.
Note that this is imperfect, as a vector of unit length would (perhaps incorrectly) show up as a {\tt scalar/X}, but any other invocation where the vector does not have unit length will generate the correct {\tt vector/X} type, and the distinction between scalars and vectors can be later collapsed if desired.
Note also that the type system described here is the \AT{SOMETHING} Type System, described in detail in Section~\AT{TODO}.

The process for tracing one package {\tt p} is as follows:

\begin{enumerate}

	\item first, all functions in {\tt p} are instrumented with on-exit hooks.
	These hooks call a number of R's available reflection functions on each of the arguments, as well as the return type:
	The called functions are {\tt typeof}, {\tt class}, and {\tt attributes}.
	See Section~\ref{sec:rlang} for details on these functions.
	
	\item then, depending on the specified type 

\end{enumerate}

Then, we aggregate the information in all traces for a particular function.
If there was only one function trace, we discard the result as the function
is trivially monomorphic.  We can see which arguments had which types over
how many traces, and from here we can build a \textit{signature} for each
function argument, and consider them together to create a function
signature.  A \textbf{polymorphic argument} is one which has been inhabited
by values of at least two different types, and a \textbf{polymorphic
  function} is a function with at least one polymorphic argument or a
polymorphic return.

We repeat the above for each package on the Comprehensive R Archive Network
(CRAN), the premier source for R libraries (called packages), and the
Bioconductor package repository.  Now, depending on the data point we're
after, our methods from here differ slightly.

\begin{itemize}
    \item for \textbf{counting signatures}: for each signature, we count how
      many functions or arguments have the requisite signature;
    \item for \textbf{finding the most common signatures}: we enumerate as
      before, and sort the signatures based on how often they appear;
\end{itemize}

%
%
%
%
\subsection{Attributes}
\label{sec:method:attributes}

Types (in the sense of the result of a call to {\tt typeof}) are easy to
analyze as values can only have one type.  Attributes, however, are a
different story, as values can have an arbitrary number of attributes.
Essentially, we're not interesting by function arguments which have had
multiple attributes, we're interested in function arguments which have had
multiple attribute \textit{patterns}. \AT{is this defined earlier?}
Arguments are said to be polymorphic in attribute if they have been
inhabited with values which have had different attribute patterns.  \AT{We
  have not investigated whether functions which are polymorphic in attribute
  use attributes extensively inside the function code.}

\AT{Paragraph justifying our construction of an attribute pattern.
Either just name, or (name, type).}

\AT{Paragraph about so-called ``naturally-occurring'' attribute patterns.}


%
%
%
%
\subsection{{\tt genthat} Tracer Quirks}

First and foremost, {\tt genthat} captures arguments only on function exit,
so as to not force promises during function execution.  \AT{Uh oh:} One
clear limitation here is that if an argument \textit{gains} attributes over
the functione execution, then they will appear in our signatures.  As R is
side-effecting, this is an entirely valid practice (i.e., passing objects to
functions to modify them), though \AT{it's unclear if people actually do
  this}.

\AT{Maybe talk about the quirks of genthat in here?
For example, how it deals with default arguments, and stuff like that.}

%
%
%
%
\subsection{Goals}

Ultimately, our goal is to collect data and report on patterns which emerge
organically.  That said, the comprehensive picture of type usage we are
developing can also be used to inform the design of a set of \textit{type
  annotations} for the language, and this goal informs our analysis.  As we
collect our data, we keep in mind the idea that the data we produce should
clearly suggest what sorts of type annotations would reflect language usage
patterns.

%
%
%
%
\subsection{The Corpus}

\AT{I'm hesitant to go into too much detail here, as I'm not sure if we're quite done running our analysis.
I can imagine a scenario where we will try to rerun with a longer timeout just to get a little bit more data (because why not).}

We ran our analysis over a subset of CRAN, the Comprehensive R Archive Network.
We analyzed \AT{over 7500} packages, over half of all packages available on CRAN.
Our analysis had an explicit timeout of \AT{30 minutes} per package, as a number of R packages feature functions which have a nigh-eternal running time (e.g., machine learning packages and complex simulations) \AT{cite those packages/examples}.

There are a number of interesting packages in our corpus.
\AT{References for all of these.}
For example, we have analyzed some packages for plotting data: among others, the {\tt ggplot2} package is an immensely popular package for generating highly-customizable plots, and the {\tt plotmo} package is an interesting package aimed at displaying models.
We also analyzed {\tt dplyr}, an integral package to the {\tt tidyverse} set of R packages \AT{talk about tidyverse earlier, or here}.
{\tt dplyr} is a popular package that makes full use of R's features to create what is tantamount to a small DSL for data manipulation within R.
Another interesting package is {\tt rquery}, a package for generating SQL-like queries. 

\AT{Should we talk about some of the packages?
What else do we have to say about the corpus?
We can restate the numbers from the abstract.}

%
%
%
%
%
%
\section{Results}

\AT{PSA:} There are a few ways we could do this section.
One way would be to go through our data in a sort of linear fashion, akin to how the main notebook is set up.
We can explore each of our type systems one at a time, and present them (thus breaking the section up by type system).
Another option is to just break down by-result, so have a section for the counts, for the top polymorphism...
I like the first option better.

\AT{Draft}
In this section, we will present the results of our analysis. 
We approach the data with the purpose of informing the design of a static type system for R.
We propose to begin with a very basic type system, discussed next.

\subsection{Base R Type System}

The {\it Base R Type System} (BRTS) ...

%
%
%
%
\subsection{Types in R}

\AT{may not be the correct place for this. i think i accidentally deleted it.}

R is a unityped language, and as such is trivially monomorphic, and this notion of monomorphism is far from useful.
We are interested in expanding the notion of ``type'' in R, and thankfully the language has some tools on offer.
\AT{this is discussed later on}

%
%
\subsubsection{What are possible type systems?}

Great question.
Some things:

\begin{itemize}

\item {\it The Obvious Type System (OTS)}: this type system corresponds directly with the type information that one can fetch using the {\tt typeof}, {\tt attributes}, and {\tt class} functions.
In this system, there is no distinction between vectors and scalars, and lists of differently-typed elements (e.g., a list of integers vs. a list of strings), and there is a distinction between integers and doubles, and the type of NULL and all other types.
This type system isn't particularly informative to a programmer, though it does reflect the treatment of values in the R runtime, for the {\tt typeof} a value is the type that the value has according to the R internals.

\item {\it The Fine-Grained Type System (FGTS)}: this type systems includes a number of relevant additions.
Chiefly, it distinguishes between vectors and scalars, as it is conceivable that boxing scalars as single-element vectors might be overkill.
In addition, list types are parameterized over the types of their elements (e.g., list<integer> vs. list<character>).
Finally, NAs ``in the wild'' are given the type {\tt raw\_NA}, as {\tt typeof(NA) = logical} in R.

\item {\it The Null-and-NA-Free Type System (NNTS)}: most languages consider NULLs to inhabit all types, and R does so for NAs \AT{elaborate}.
This gives us ... .

\end{itemize}

Of course, one could conceive of a more fine-grained type system than FGTS, but it's unclear that we would gain much in doing so.
Indeed, there is a space to explore in slightly less fine-grained type systems, which make modifications including:

\begin{itemize}

\item {\bf collapsing vectors and scalars}: this yields a type system slightly closer to OTS.

\item {\bf rolling NULL into other types}: in most languages, NULL is an inhabitant of all types, and this adjustment seems interesting.
Note that this goes beyond even OTS, as in R {\tt typeof(NULL) = NULL}.

\item {\bf rolling NA into other types}: \AT{i should try this}
similarly, NA is an inhabitant of all types, though NA standing alone has type {\tt logical}.
FGTS distinguishes such NAs with the {\tt raw\_NA} type, and if instead we consider NA to inhabit all types we may \AT{win}.

\item {\bf combining integers and doubles}: as in most languages, doubles and ints can stand-in for each other in nearly every scenario.
Even if not, it is trivial to convert from one to the other, particularly converting ints to doubles.
Aside, in R a double can be used to index a list, and R will floor it when performing the lookup.

\end{itemize}

%
%
%
%
\subsection{Usage Patterns}

In this section, we will discuss function usage patterns which arose in our analysis.
We will start by looking at the morphicity of functions.

\subsubsection{Function Argument and Return Morphicity}

First and foremost, we would like to know how often R programmers create polymorphic functions.
Recall that we define a polymorphic function to be a function with at least one polymorphic argument, or a polymorphic return.
We will turn our attention now to the data in \todo{Figure}, and go through each entry in the table.

Here, we see that the vast majority of function arguments are indeed monomorphic.
Monomorphic arguments are easy to annotate, as the {\tt typeof} the arguments is exactly the most precise annotation we could give, at least in terms of \textit{type}.
That said, types alone don't always paint the whole picture:
recall that \textit{attributes} are an R language feature which allows programmers to stick metadata onto values.
So in which circumstances \textit{do} types paint the whole picture?

The second data point in \textbf{TODO FIGURE} indicates that a
\isit{majority} of function arguments are monomorphic in type \textit{and}
have no attributes.  These represent arguments which are truly trivial to
annotate, as the {\tt typeof} an argument perfectly describes the usage of
that argument.  That said, some attributes arise naturally in R: For
instance, names in a named list (e.g. {\tt x} and {\tt y} in {\tt list(x=0,
  y=0)}) appear as an attribute on the value.  In
Section~\ref{sec:method:attributes}, we outlined these naturally-occurring
attribute patterns, and the third data point in \textbf{TODO FIGURE} shows
that a nontrivial amount of functions are monomorphic in type with said
natural attribute patterns.

Another facet of type information in R is in the {\tt class} attribute.
Recall that values have a \textit{class} in addition to a type: For example,
a {\tt data.frame} has type {\tt list} and class {\tt data.frame}.
\todo{Plug classes earlier} R has a number of built-in classes, such as
\todo{X, Y, and Z}, but users are free to redefine the class of any value at
runtime, and easily define new classes.  The next data point in
\todo{Figure} shows that user-defined classes don't appear altogether often,
though they do indeed feature.  \AT{In a following section, we will discuss
  how the usage of these classes manifests itself.}

\AT{Separate section for functions? Right now, this is at argument granularity.}

\subsubsection{Type Signatures}

In the last section, we presented a high-level overview of the morphicity ... .

\subsubsection{Attribute Signatures}

Recall that attributes are a way for programmers to store metadata on values in R.
What are the common attribute patterns?
And how often is the attribute pattern polymorphic?

\subsubsection{Takeaways}

\begin{itemize}
    \item the vast majority of arguments are monomorphic in type;
    \item of those, over 60\% have no attribute information;
    \item of the 40\% with attribute information, roughly 1/2 have fairly
      simple attributes corresponding to base R constructs (named lists and
      vectors, matrices, and data frames);
    \item now, of those arguments which are polymorphic, a sizable chunk (well over half) have defensible signatures (e.g., double and character for named list indexing, double and integer for obvious reasons, etc.).
\end{itemize}

In short, it looks like R (package) programmers are reasonable.  I'd
conjecture that a lot of the polymorphism (e.g., double and list) is coming
from how easy it is to use either type in a given situation (e.g.,
converting from vector of doubles to list or vice versa is simple).

%
%
%
%
%
%
\section{Synthesis}

In this section, we will discuss the conclusions that we draw from our data.


%
%
\subsection{Suggested Annotations}

\AT{How can we capture these patterns with annotations?}

Some possible annotations:

\begin{itemize}
    \item \textit{real}: for \textit{double} and \textit{integer} values
    \item \textit{function}: for \textit{closure}, \textit{special}, and \textit{builtin} values
    \item \textit{vector}: to indicate that something should be vectorized
    \item \textit{scalar}: to indicate that something should \textbf{not} be vectorized
    \item \textit{index}: for \textit{real} and \textit{character} values
\end{itemize}

%
%
\subsubsection{Struct-Like Attribute Declarations}

\AT{What's a convenient way to annotate attributes?  Should investigate how
  often attributes are consistent.}

%
\subsubsection{Coverage}

\AT{What is the coverage of these new annotations?}

%
%
%
%
\section{Related Work}

\AT{Not sure what the best place for this is, but we should discuss these papers somewhere.}

The idea of measuring polymorphism via some program analysis isn't new.
For instance, some work~\cite{aakerblom2015measuring} ... .

More generally, there is an existing literature on analyzing usage patterns of language features.
For instance, the dynamic features of Smalltalk were analyzed in some work~\cite{callau2011howdevelopers}, ...

Other analysis work~\cite{milojkovic2017duck} ... .

\section{Conclusions and Future Work}

\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

\end{document}
