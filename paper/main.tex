\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings,xspace,wrapfig}
\lstset{language=R}
\usepackage{my_style}

\definecolor{LightGray}{rgb}{.95,.95,.95}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray}
}

\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={env, equal, c, runif, trace, args},
  breaklines=true
}

\lstdefinestyle{Rin}{ %
  style=R,
  numberstyle=none,
  basicstyle=\normalsize\ttfamily,
  breaklines=false
}
\newcommand{\code}[1]{\lstinline|#1|\xspace}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A Large-scale Study of Polymorphism in R}

\newcommand{\PACKAGES}{7,763\xspace}
\newcommand{\PROGRAMMERS}{10,000\xspace}
\newcommand{\PERCENTCRAN}{56\%\xspace}
\newcommand{\CRANTOTAL}{13,841\xspace}
\newcommand{\RLOC}{8,996,572\xspace}
\newcommand{\CLOC}{5,100,719\xspace}
\newcommand{\YEARS}{20\xspace}

\begin{abstract}
The R programming language is widely used in a variety of scientific domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in \RLOC lines of R 
code spread among some \PACKAGES packages, written over a
period of over \YEARS years by \PROGRAMMERS programmers.  We perform a dynamic
analysis, leveraging tests and use-cases, to determine the level of
polymorphism that is present in the code. We do this for several potential
notions of types. Our result suggest that polymorphism is important in some
key parts of the system but that relatively simple type annotations could be
used to capture most of the interesting cases.
\end{abstract}

\maketitle

\section{Introduction}

Our community builds, improves, and reasons about programming languages.  To
make design decisions that benefit most users, we need to understand the
language we are working with as well as the real-world needs it
answers. Often, we, as researchers, can appeal to our intuition as many
languages are intended to be general purpose and appeal to users with some
computer science training. Unfortunately, these intuitions don't always
apply to domain-specific languages, languages designed for and by a specific
group of users to solve very specific needs. This is the case of the data
science language R.

R and it's ancestor S are languages designed, implemented and maintained by
statisticians. Originally they were designed as glue languages, languages
that would allow to read data into vectors and call statistical routines
written in Fortran. Over three decades, the languages became widely used
across many fields of science and in industry for data analysis and data
visualization; with time additional features were added.  Modern R, as a
linguistic object of study, is fascinating. It is a vectorized, dynamically
typed, lazy functional language with limited side-effects, extensive
reflective facilities and retrofitted object-oriented programming support.

Many of the design decisions that gave us R were intended to foster an
interactive, exploratory, programming style. This includes, to name a few,
the lack of type annotations on variables and functions, the ability to use
syntactic shortcut, and the automatic conversion between data types.  While
these choices have led to a language that is surprisingly easy to use by
beginners --many data science programs do not teach the language itself but
simply introduce some of its key libraries-- they have also created a
language where almost all computations yield a numeric result and where
errors can go undetected. 

One way to increase assurance in the results obtained when using R would be
to add type annotations to functions and variable declarations. These
annotations could then be used, either statically or (more likely)
dynamically, to catch mismatches between expected and provided data values.
The nature of R is such that it is unlikely to be ever fully statically
checked, furthermore end users may not be willing to write types when
carrying out exploratory programming tasks. So, we are looking for an
optional type system that would allow us to capture as much of behavior of
library functions as possible while remaining easy to understand for
end-users and library developers alike.

This papers is a data-driven study of what a type system for the R language
could look like. Longer term, our intention is to propose changes to
language, but for any changes to be accepted by the user community, they
must clearly benefit the language without endangering backwards
compatibility. Our goal is thus to find a compromise between simplicity and
usefulness; the proposed type system should cover most common programming
idioms while remaining easy to use. In order to do this, we need to
understand the degree of polymorphism present in R code, that is to say, how
programmers leverage the dynamic nature of R to write code that can accept
arguments of different types.  This understanding will drive our design.

We propose to capture the degree of polymorphism present in R by the means
of a dynamic analysis of widely used libraries. For each function call we
can record the types of its arguments and of its return value. This allows
us to observe how many different combination of types are accepted by any
given function. Unlike many other languages, R has a carefully curated
software repository called CRAN. To be deposited in CRAN, a package must
come with sample dataset, tests and executable use-cases. As part of normal
operations these tests are run regularly and failing packages are removed.
This allowed us to have access to \PACKAGES libraries and about an order of
magnitude more runnable scripts that exercise those libraries.

The contributions of this paper are thus as follows:
\begin{itemize}
\item A large-scale analysis of the polymorphism present in function
  signatures of \PACKAGES widely used and actively maintained R packages.
\item A tracing and analysis pipeline that extends a previously published
  test generation tool named \code{genthat}.
\item Manual analysis of 100 functions to validate the dynamic analysis
  results.
\end{itemize}

One threat to validity of our work is that we rely on dynamic analysis, so
our conclusions are only as good as the coverage of the possibly function
behaviors. Previous work~\cite{issta18}, reported that running all the
scripts that come with CRAN packages gives, on average, 68\% test coverage.
We attempted to mitigate the threat coming from the fact that only part of
the code is being exercised by manual analysis. It would be reasonable to
ask for confirmation of the data by static analysis of the code, but sound
static analysis of R is difficult because of the extensive use of reflective
features such as \code{eval} and of the ability to redefine the meaning of
operators such as \code{+} and \code{if}.  Another threat to validity is
that we only have access to code that has been deposited in the CRAN
repository. While this may bias our findings towards code written to be
reusable and, possibly, better engineered than typical user code. This is
also the code that would most benefit from type annotations.

\newpage  %%Leave here

\section{The R Programming Language}\label{sec:rlang}

Over the last decade, the R Project has become a key tool for implementing
sophisticated data analysis algorithms in fields ranging from Computational
Biology~\cite{R05} to Political Science~\cite{R:Keele:2008}. At the heart of
the R project is a \emph{vectorized, dynamic, lazy, functional,
  object-oriented} programming language with a rather unusual combination of
features~\cite{ecoop12} designed to ease learning by non-programmer and
enable rapid development of new statistical methods.  The language, commonly
referred to as R was designed in 1993 by Ross Ihaka and Robert
Gentleman~\cite{R96} as a successor to S~\cite{S88}.  First released in
1995, under a GNU license, R rapidly became the lingua franca for
statistical data analysis. Today, there are over 13,000 R packages available
from repositories such as CRAN and Bioconductor.  With 55 R user groups
world-wide, Smith~\cite{eco11} estimates that there are over 2 million
end-users.

As an introduction to R, consider the code snippet in Fig.~\ref{sample} from
a top-level interaction where the user defines a function \code{normSum}
that accepts vectors of integers, logicals, doubles and complex values and
normalizes the vector with respect to its sum and rounds the results. The
function definition does not require type annotations, and all operations
transparently work on vectors of any length and different types.

\begin{figure}[!hb]{\small
\begin{lstlisting}[style=R]
> normSum <- function( m )  round( m / sum(m), 2)
> normSum(c(1L,3L,6L))
[1] 0.1 0.3 0.6
> normSum(c(1.1,3.3,6.6))
[1] 0.1 0.3 0.6
> normSum(c(1.6,3.3,6.1))
[1] 0.15 0.30 0.55
> normSum(complex(r=rnorm(3),i=rnorm(3)))
[1] 0.49+0.21i 0.30-0.18i 0.22-0.03i
\end{lstlisting}}
\caption{Sample R code}\label{sample}
\end{figure}

R has a number of features that are not crucial to the present
discussion. We will mention some of them here for completeness.  In R, data
structures are reference counted and have copy-on-write semantics, thus the
assignment \code{x[12]<-3} results in an update to a copy of \code{x} unless
the reference count on that object is 1.  This semantics gives R a
functional flavor while allowing updating in place within loops (the first
update copies, subsequent updates are performed on the copy). Arguments to
functions are evaluated only when needed, they are bundled in so-called
promises which package the original expression (as an AST), its environment
as well as the result of evaluating the expression. Promises can be
leveraged for meta-programming as it is possible to retrieve the text of a
promise and evaluate that in a different environment.

\subsection{Types of Data}

Before attempting to define a type system for R, we should understand the
different kinds of values that programs operate on.  As we will see
different notions of type may emerge depending on how granular we want to
be.

\renewcommand{\k}[1]{{\tt #1}\xspace}

R has one builtin notion of type that can be queried by the \k{typeof}
function. Over the years, programmers have found the need for a richer type
structure and have added attributes. The best way to think of attributes is
as an optional map from name to values that can be attached to any object.
Attributes are used to encode various type structures. They can be queried
with functions such as \k{attributes} and \k{class}.

\begin{wrapfigure}{r}{6.1cm}
\footnotesize\begin{tabular}{l|l@{}}\hline
\multicolumn{2}{l}{\bf Vectorized data types:}  \\\hline
\k{logical}  & vector of boolean values\\
\k{integer}   & vector of 32 bit integer values\\
\k{double} & vector of 64 bit floating points\\
\k{complex} & vector of complex values\\
\k{character} & vector of strings values\\
\k{raw} & vector of bytes\\
\k{list} & vector of values of any type\\\hline
\multicolumn{2}{l}{\bf Scalar data types:}\\\hline
\k{NULL}  &  singleton null value\\
\k{S4}    &  instance of a S4 class \\
\k{closure} & a function with its environment\\
\k{environment} & a mapping from symbol to value \\\hline
\multicolumn{2}{l}{\bf Implementation data types:}\\\hline
\multicolumn{2}{l}{\k{special},
\k{builtin},
\k{symbol},
\k{pairlist},
\k{promise}}\\
\multicolumn{2}{l}{
\k{language},
\k{char},
\k{...}, 
\k{any},
\k{expression},
}\\
\multicolumn{2}{l}{
\k{externalprt},
\k{bytecode},
\k{weakref}}\\\hline
\end{tabular}\caption{Builtin Types}\label{types}\end{wrapfigure}

Figure~\ref{types} lists all of the builtin types that are provided by the
language. They are the possible return values of \k{typeof}. There is no
intrinsic notion of subtyping in R. But, in many context a \k{logical} will
convert to \k{integer}, and an \k{integer} will convert to \k{double}.  Some
off conversion can occur in corner cases, such as \k{1<"2"} holds and
\k{c(1,2)[1.6]} returns the first element of the vector, as the double is
converted to an integer. R does not distinguish between scalars and vectors
(they are all vectors), so \code{typeof(5) ==} \code{typeof(c(5)) ==
  typeof(c(5,5))} \code{ == "double"}. Finally all vectorized data types have a
distinguished missing value denoted by \code{NA}. The default type of
\code{NA} is \k{logical}. We can see that \code{typeof(NA)=="logical"}, but
NA inhabits every type: \code{typeof(c(1,NA)[2])=="double"}.

With one exception all vectorized data types are monomorphic, the exception
is the \k{list} type which can hold values of any other type including
\k{list}. For all monomorphic data types, attempting to store a value of a
different type will cause a conversion. Either the value is converted to the
type of vector, or the vector is converted to the type of the value.

Scalar data types include the distinguished \k{NULL} value, which is also of
type \k{NULL}, instance of classes written using the S4 object system,
closures and environments.  The implementation of R has a number of other
types that are mostly not used by user code, they are listed in
Figure~\ref{types} for reference.


Another form of type information relates to metadata on values, and R allows
programmers to add said metadata in the form of {\it attributes}.  The
functionality to do so in R is through the \code{attributes(x)} and
\code{attr(x, "name")} functions.  \code{attributes}


Besides these primitive data types, R has an out-of-the-box data type called
the data frame for storing data, and this data type is essential to a large
part of R's functionality.  A data frame is a two-dimensional array where
each row represents an observation, with columns indicating the values of
some observation.  For example, our own analysis is written in R, and one of
our data frames has a row for each package analyzed, with columns for values
ranging from number of polymorphic functions, number of monomorphic
arguments, unique attribute patterns, etc.  \AT{Put a figure?}  Much of R's
base functionality, and indeed much of the functionality provided by
packages, operate on data frames.


\section{The Corpus}\label{sec:corpus}

In this section, we present our dataset. R is designed to accommodate data
analysts.  Typical workflows go from data import, to data cleaning, followed
by steps of modeling, transformation and visualization. Often the code that
implement these analysis pipelines together with their results resides in
notebooks. Unfortunately notebooks are typically not shared in public
repositories such as GitHub, and if they were, the data required to run them
would surely not be shared. For this reason our analysis focuses on
packages; these bundle reusable units of R code with documentation, sample
data, use-cases and tests.

We focus on packages hosted on the \emph{Comprehensive R Archive Network} or
CRAN.  With over 13,000 packages, CRAN is the largest repository of software
written in R. It is experiencing sustained growth with an average of size
new packages a day~\cite{LIgges2017}.  Unlike sites like GitHub, CRAN is a
curated collection of packages. A package is only accepted to CRAN if it
abides by a number of well-formedness rules.  Most relevant for our
purposes, packages must have data, examples, vignettes and tests. All of
which must successfully run. From our perspective this means that each of
the package in CRAN comes with up to a dozen executable scripts that
exercise some of its functionality.


Our corpus is a subset of CRAN, consisting of \PACKAGES packages, accounting
for some \PERCENTCRAN of all packages.  These packages have a total of \RLOC
lines of R code and \CLOC lines of C code. For each package, we extracted
all executable code snippets from documentation, vignettes and tests and ran
them independently recording all call to R functions.  It is noteworthy that
in order to run the scripts in one package, it is often necessary to load a
number of other packages.  In~\cite{issta18}, the authors estimated code
coverage to be around 68\% when including reverse dependencies.  The
rationale for our choice of packages was pragmatic, our infrastructure adds
overhead to the execution of scripts, we eliminated any package that would
take more than one hour to run. We expect to extend our coverage of CRAN for
the next version of this paper.

Figure~\ref{most} shows the ten most downloaded CRAN packages.  For each
one, we list how many lines of R and C/C++ the packages contains.
Typically, C/C++ code is used to implement performance critical portions of
the code. We show the number of scripts that could be extracted from the
package. Each script corresponds to either one use-case or a set of unit
tests.  Figure~\ref{allcloc} shows the size of the code across all the
packages we have used.  Lastly, Figure~\ref{recorded} gives the number of
function signatures that we recorded while executing the scripts of each
package.

% https://www.r-pkg.org/downloaded
\begin{figure}[!th]{\footnotesize\begin{tabular}{@{}r||l|r|r|r@{}}\hline
\bf Package & \bf Description & \bf R LOC &\bf C LOC &\bf Scripts\\\hline
\tt Rcpp  & Seamless C++ integration & 10K & 3K & 12 \\
\tt rlang & Functions for 'Tidyverse'&&& \\
\tt glue  & Interpreted string literals&&& \\
\tt tibble & Simple data frames&& &\\
\tt stringi &  String processing &&& \\
\tt ggplot2 & Data visualisations&& &\\
\tt dplyr  &  Data manipulation&&& \\
\tt pillar & Formatting for columns&&& \\
\tt R6 & Classes w. ref. semantics&&& \\
\tt stringr & String operations&&& \\
\end{tabular}}\caption{10 Most Downloaded Packages.}\label{most}
\end{figure}

%
%
%
%
\section{The \texttt{genthat} Package}
\label{sec:genthat}

As we mentioned, getting our hands on ``front-line'' non-library R code is tricky: 
R is not like many other languages, where large systems are built up and stored in a repository such as GitHub.  
Instead, R programmers are liable to leverage a number of packages and work them into their data analysis pipelines.

Thankfully, we are somewhat able to make up for this lack of code.
CRAN has a policy that published packages are to be accompanied by at least one vignette, and typically by tests and examples showing off how the code is intended to be used.  
Nominally unrelated, the {\tt genthat} project~\cite{issta18} traces this example code to generate new unit tests which package designers may not have thought of.
As it happens, we can leverage {\tt genthat} to extract dynamic type information from running package test code, which presumably indicates how package code is intended to be used.

\AT{Draft}

Our tracer is built on of the {\tt genthat}~\cite{issta18} tracer, which we modified to capture the type information of arguments.  
{\tt genthat} generates traces for each unique function invocation with the express purpose of re-invoking the function at a later time, and instead of generating traces for these invocations we capture type information of the arguments and return values of functions.
This is done by evaluating the arguments and return values of functions at the end of the function's scope in the relevant environment.  
The sort of types and granularity of the type information that are generated can be specified in our tracer: 
Essentially, our tracer is parameterized over a user-specified type system.  
For instance, if desired our tracer can dig into lists and data frames to get the types of list elements and fields.
Our tracer produces trace results for each unique type signature encountered, which may then be used for data analysis.

An unfortunate consequence of R's most common usage pattern is the lack of "deployed" R code. 
R is most often used for exploratory data analysis, and very little of this analysis code is made publicly available on e.g. GitHub.
The {\tt genthat} angle is thus desirable: 
With the scarcity of non-library code, examples of package usage is about as representative as one can hope for of the sort of code people will write in their analysis pipelines.

We can do a little bit better than merely package usage examples, though.
Available through CRAN is a list of {\it reverse dependencies} of packages: 
for some package {\tt p}, a list of all packages which require {\tt p} in some way.  
Our tracer is able to leverage and trace this code as well. 
When running a package, we also trace how all inherited functions are used, thus painting an even clearer picture of function usage.  
This additional data is considered alongside similar data collected for all other packages, as well as the data obtained by tracing the package's examples.

% Some prose we might want to pull from.
%First, we leverage {\tt genthat}'s tracer to trace function executions.  The
%idea here is that we don't really have access to non-library code written in
%R, as general use patterns are (possibly?) to write small scripts which
%analyze some bit of data and possibly visualize results.  The goal is likely
%not to build big working systems, instead to explore data with by writing
%and rewriting small scripts, ad infinitum.  In looking at package tests,
%vignettes, and examples, we are painting a picture of how the package
%designers intended their packages to be used, which we believe is a close
%approximation of what R users would do.


\section{The Method}

In this section, we will detail our methodology for collecting data.

%
%
%
%
\subsection{Granularity of Type Information}

As we mentioned in Section~\ref{sec:rlang}, R has several notions of ``type'', ranging from the results of \code{typeof} to metadata on values obtainable via the \code{attributes} function.
Individually, these paint an incomplete picture of the type of a value:
For example, recall that \code{typeof(5) == typeof(c(5, 5)) == "double"}.

To be as specific as possible, our analysis collects additional information to create a more specific type than the result of any of R's reflection functions.
For instance, if the analysis encounters {\tt typeof(x) == "double"}, it will look at the length of {\tt x} to determine if {\tt x} is a scalar or a vector, generating the annotation {\tt scalar/double} or {\tt vector/double} accordingly. 
One notable imperfection in this example is that unit-length vectors will appear as scalars, but if some function was called with unit-length vectors {\it as well as} larger ones, both scalar and vector types will appear, and this distinction can be later collapsed.
Another example is that our analysis builds parametric list types by capturing the types of list elements.
We also distinguish NAs ``in the wild'' with a {\tt raw\_NA} type.

%
%
%
%
\subsection{Data Collection Pipeline}

As we mentioned in Section~\ref{sec:genthat}, we modify the {\tt genthat} tracer to collect and output the type information for all function invocations.

The high-level process for tracing one package is as follows:

\begin{enumerate}
	\item first, all functions in the package are instrumented with on-exit hooks.
  	These hooks call a number of R's available reflection functions on each of the arguments, as well as the return type: 
  	The called functions are {\tt typeof}, {\tt class}, and {\tt attributes}.  
  	Recording is done on function exit to avoid forcing promises before they are needed (as these promises may cause side effects).
  
  	\item next, runnable code is extracted and run from package tests, examples, and vignettes, and \AT{what about rev deps}.
  
	\item each function invocation triggers the on-exit hook, and it is this on-exit code which collects the information.
	Depending on the results of {\tt typeof}, {\tt class}, and {\tt attributes}, more analysis may be performed.
	For example, if an argument is a list, type information is collected on list elements until a more accurate type (e.g., {\tt list<double>}) can be constructed.
	Indeed, in a later section we do propose some possible type systems for R, and our analysis can be parameterized with these type systems to generate type information according to the types specified, and this will be discussed further in Section \AT{yap}.
	
	\item all of the recorded type information makes up a function {\it trace}.
	This trace is passed to a C++ function (for performance reasons) which hashes it and checks it against already seen trace hashes.
	If the trace is determined to be new, the hash is saved and the trace is written to a file.
	
	\item finally, after all test code has finished executing, we are left with a directory full of all of the unique type signatures that were observed while running the package code.
	
\end{enumerate}

For each package function, we then consolidate all observed signatures: this gives us a {\it function signature}.
\AT{Example?}
In the function signature, each function argument is accompanied by a list of types, attributes, and classes that have inhabited it. 

We will need a notion of polymorphism when undertaking this analysis.
As R is unityped, and the only notion of polymorphism in the language is its use of dynamic dispatch on the class of an argument.
For our purposes, we will define 3 simple types of polymorphism, closely connected with the reflection functions we use.
We start by defining what it means for an {\it argument} to be polymorphic.
\AT{Could be valuable to have an example here.}

\begin{itemize}

	\item an argument is {\it polymorphic in type} if it has been called with at least two distinct ``types'' according to the \code{typeof} function and our extra analysis.
	
	\item similarly, an argument is {\it polymorphic in class} if it has been called with at least two distinct classes according to the \code{class} function.
	
	\item attributes are a bit more tricky.
	We can't simply count the number of attributes that have inhabited an argument, since values can contain arbitrarily many values (and having several attributes isn't polymorphism---think records).
	Instead, we define the {\it attribute pattern} of a value to be the set of all attribute name and type pairs in the attribute list of a value (obtained via \code{attributes}).
	And so an argument is {\it polymorphic in attribute} if it has been called with at least two distinct attribute patterns.

\end{itemize}

There is an important distinction to be made between the result of \code{typeof} and that of \code{class} and \code{attributes}:
In R, the \code{typeof} a value cannot change, and this is not the case with class and attributes.
As mentioned in \AT{an earlier section}, the class of values can be easily redefined by assigning to the value's class attribute.
Similarly, attributes can be dynamically added, removed, or changed.

This begs the question: what sort of classes and attribute patterns occur naturally, before possible user tampering?
In terms of classes, the discussion can be found in Section~\ref{sec:rlang}.
As for attributes:

\begin{itemize}

	\item data frames have three attributes: {\tt names} for column names, {\tt row.names} for row names, and {\tt class} (which is instantiated to {\tt "data.frame"} upon creation).
	
	\item matrices have at least a {\tt dim} attribute with matrix dimensions, and optionally a {\tt dimnames} attribute for dimension names.

	\item lists and vectors may have a {\tt names} attribute, which assigns names to locations.
	
	\item to ease dealing with large sorted sets of non-numeric values, R offers the \code{factor} and \code{levels} functions.
	\code{factor(x, levels=someOrder)} adds a {\tt "levels"} attribute to \code{x}, which specifies an ordering for \code{x} according to the \code{sortedOrder} parameter.
	\code{factor} also changes the class of the parameter being factored to \code{"factor"}.

\end{itemize}

These naturally-occurring attribute patterns and classes can be separately accounted for to highlight user-defined patterns and behaviours.
This will be made clear in Section~\ref{sec:results}.

%
%
%
%
%
%
\section{Results}
\label{sec:results}

\AT{Meta-Comment:} 
My plan for this section is to linearly go through the data, and show logical transitions between various type systems.
I'll start with the finest-grained one.

\AT{Draft}
Having built an understanding of our data collection and analysis pipeline, we will turn our attention to the results of our analysis.
We approach the data with the purpose of informing the design of a static type system for R.
To that end, we propose to begin with a very basic type system, the {\it Fine-Grained Type System} (FGTS).

The logical ``baseline'' for any design of a static type system for R is promotion of the language's dynamic types (in the sense of \code{typeof}) to static types, but there are some clear limitations to this approach.
For example, \code{typeof} makes no distinction between vectors and scalars, as indeed there is no such distinction in R.
But that doesn't mean that there {\it shouldn't} be a distinction, as an all-new runtime environment design for R might benefit from such a distinction.

To address these limitations, we augment the logical naive type system with more information which was alluded to in the discussion of our data collection methodology.
We will consider unit-length vectors to be scalars, collect list member type information to build an accurate list type, and promote NAs to their own type.
In addition, we collect attribute and class information to distinguish a type for data frames and matrices, as the \code{typeof} these is simply {\tt list} in R.
This type system is referred to as the Fine-Grained Type System, or FGTS.
An example of a signature in this type system can be found in Figure~\AT{figggg}.

The first question we will address is: {\it how polymorphic is R?}
To answer that, we will first look at the number of monomorphic arguments across all of the packages we analyzed.
The data can be found in Figure~\AT{fiGgGgGgGGg}.

\AT{We should figure out the pipeline to get the notebook output into here.}

\AT{I'm fairly sure the numbers aren't going to be insanely different, so I'll just manually write them for now.}

We see that roughly 10\% of all argument positions in R are polymorphic.
The numbers for attribute and class polymorphism are included for completeness, but the important number is that of type polymorphism (as our method of constructing types sometimes factors in class and attribute).
Specifically for classes, often the class and \code{typeof} a value will line up (refer to Section~\ref{sec:rlang}).

As for functions, roughly 25\% are polymorphic, or put differently 25\% have at least one polymorphic argument or a polymorphic return.
The discrepancy between this and the polymorphicity of arguments is explained by the tendency of some functions, particularly those dealing with models, to have {\it many} arguments.

These numbers alone are insufficient, so we ask: {\it of the polymorphic arguments, what are the most common patterns?}
The answer to that question can be found in Figure~\AT{eyyy}.

By and large, the most common pattern is {\tt scalar/X} and {\tt vector/X}.
One way to account for this would be to collapse scalar and vector types into a vector type:
If a function argument is observed to be both a unit-length and non-unit-length vector, then it's reasonable to assume that the argument is intended to be a vector.

One other interesting question is: {\it how many arguments are strictly scalar?}
As it happens, over 40\% of monomorphic arguments with primitive vectorized types are always scalar (so, 40\% of {\it monomorphic} occurrences of {\tt scalar/X} and {\tt vector/X} are scalar).
This is important as it shows that there is value in a scalar annotation, which could be used to communicate to an R runtime that vectorization of a value is not necessary, which may in turn ead to a performance improvement.

We will collapse the distinction between vectors and scalars in a moment, but imagine we did so and refer back to Figure~\AT{eyyy}.
Another major source of polymorphism is the {\tt double} and {\tt integer} distinction.
This distinction is mainly just relevant to the implementation, as integers and doubles are stored and dealt with differently in the runtime.
For a programmer, this distinction is meaningless, as both types can be used interchangeably in every situation.
We propose to also collapse this distinction, instead considering doubles and integers as part of a real number type, {\tt real}.

\AT{TODO: do vector/scalar collapse and real collapse and get numbers.}

%
%
%
%
\subsection{Types in R}

\AT{I'll pull from this section when appropriate, as the discussion of the type systems becomes relevant.}

%
%
\subsubsection{What are possible type systems?}

Great question.
Some things:

\begin{itemize}

\item {\it The Obvious Type System (OTS)}: this type system corresponds directly with the type information that one can fetch using the {\tt typeof}, {\tt attributes}, and {\tt class} functions.
In this system, there is no distinction between vectors and scalars, and lists of differently-typed elements (e.g., a list of integers vs. a list of strings), and there is a distinction between integers and doubles, and the type of NULL and all other types.
This type system isn't particularly informative to a programmer, though it does reflect the treatment of values in the R runtime, for the {\tt typeof} a value is the type that the value has according to the R internals.

\item {\it The Fine-Grained Type System (FGTS)}: this type systems includes a number of relevant additions.
Chiefly, it distinguishes between vectors and scalars, as it is conceivable that boxing scalars as single-element vectors might be overkill.
In addition, list types are parameterized over the types of their elements (e.g., list<integer> vs. list<character>).
Finally, NAs ``in the wild'' are given the type {\tt raw\_NA}, as {\tt typeof(NA) = logical} in R.

\item {\it The Null-and-NA-Free Type System (NNTS)}: most languages consider NULLs to inhabit all types, and R does so for NAs \AT{elaborate}.
This gives us ... .

\end{itemize}

Of course, one could conceive of a more fine-grained type system than FGTS, but it's unclear that we would gain much in doing so.
Indeed, there is a space to explore in slightly less fine-grained type systems, which make modifications including:

\begin{itemize}

\item {\bf collapsing vectors and scalars}: this yields a type system slightly closer to OTS.

\item {\bf rolling NULL into other types}: in most languages, NULL is an inhabitant of all types, and this adjustment seems interesting.
Note that this goes beyond even OTS, as in R {\tt typeof(NULL) = NULL}.

\item {\bf rolling NA into other types}: \AT{i should try this}
similarly, NA is an inhabitant of all types, though NA standing alone has type {\tt logical}.
FGTS distinguishes such NAs with the {\tt raw\_NA} type, and if instead we consider NA to inhabit all types we may \AT{win}.

\item {\bf combining integers and doubles}: as in most languages, doubles and ints can stand-in for each other in nearly every scenario.
Even if not, it is trivial to convert from one to the other, particularly converting ints to doubles.
Aside, in R a double can be used to index a list, and R will floor it when performing the lookup.

\end{itemize}

%
%
%
%
\subsection{Usage Patterns}

In this section, we will discuss function usage patterns which arose in our analysis.
We will start by looking at the morphicity of functions.

\subsubsection{Function Argument and Return Morphicity}

First and foremost, we would like to know how often R programmers create polymorphic functions.
Recall that we define a polymorphic function to be a function with at least one polymorphic argument, or a polymorphic return.
We will turn our attention now to the data in \todo{Figure}, and go through each entry in the table.

Here, we see that the vast majority of function arguments are indeed monomorphic.
Monomorphic arguments are easy to annotate, as the {\tt typeof} the arguments is exactly the most precise annotation we could give, at least in terms of \textit{type}.
That said, types alone don't always paint the whole picture:
recall that \textit{attributes} are an R language feature which allows programmers to stick metadata onto values.
So in which circumstances \textit{do} types paint the whole picture?

The second data point in \textbf{TODO FIGURE} indicates that a
\isit{majority} of function arguments are monomorphic in type \textit{and}
have no attributes.  These represent arguments which are truly trivial to
annotate, as the {\tt typeof} an argument perfectly describes the usage of
that argument.  That said, some attributes arise naturally in R: For
instance, names in a named list (e.g. {\tt x} and {\tt y} in {\tt list(x=0,
  y=0)}) appear as an attribute on the value.  In
Section~\ref{sec:method:attributes}, we outlined these naturally-occurring
attribute patterns, and the third data point in \textbf{TODO FIGURE} shows
that a nontrivial amount of functions are monomorphic in type with said
natural attribute patterns.

Another facet of type information in R is in the {\tt class} attribute.
Recall that values have a \textit{class} in addition to a type: For example,
a {\tt data.frame} has type {\tt list} and class {\tt data.frame}.
\todo{Plug classes earlier} R has a number of built-in classes, such as
\todo{X, Y, and Z}, but users are free to redefine the class of any value at
runtime, and easily define new classes.  The next data point in
\todo{Figure} shows that user-defined classes don't appear altogether often,
though they do indeed feature.  \AT{In a following section, we will discuss
  how the usage of these classes manifests itself.}

\AT{Separate section for functions? Right now, this is at argument granularity.}

\subsubsection{Type Signatures}

In the last section, we presented a high-level overview of the morphicity ... .

\subsubsection{Attribute Signatures}

Recall that attributes are a way for programmers to store metadata on values in R.
What are the common attribute patterns?
And how often is the attribute pattern polymorphic?

\subsubsection{Takeaways}

\begin{itemize}
    \item the vast majority of arguments are monomorphic in type;
    \item of those, over 60\% have no attribute information;
    \item of the 40\% with attribute information, roughly 1/2 have fairly
      simple attributes corresponding to base R constructs (named lists and
      vectors, matrices, and data frames);
    \item now, of those arguments which are polymorphic, a sizable chunk (well over half) have defensible signatures (e.g., double and character for named list indexing, double and integer for obvious reasons, etc.).
\end{itemize}

In short, it looks like R (package) programmers are reasonable.  I'd
conjecture that a lot of the polymorphism (e.g., double and list) is coming
from how easy it is to use either type in a given situation (e.g.,
converting from vector of doubles to list or vice versa is simple).

%
%
%
%
%
%
\section{Synthesis}

In this section, we will discuss the conclusions that we draw from our data.


%
%
\subsection{Suggested Annotations}

\AT{How can we capture these patterns with annotations?}

Some possible annotations:

\begin{itemize}
    \item \textit{real}: for \textit{double} and \textit{integer} values
    \item \textit{function}: for \textit{closure}, \textit{special}, and \textit{builtin} values
    \item \textit{vector}: to indicate that something should be vectorized
    \item \textit{scalar}: to indicate that something should \textbf{not} be vectorized
    \item \textit{index}: for \textit{real} and \textit{character} values
\end{itemize}

%
%
\subsubsection{Struct-Like Attribute Declarations}

\AT{What's a convenient way to annotate attributes?  Should investigate how
  often attributes are consistent.}

%
\subsubsection{Coverage}

\AT{What is the coverage of these new annotations?}

%
%
%
%
\section{Related Work}

\AT{Not sure what the best place for this is, but we should discuss these papers somewhere.}

The idea of measuring polymorphism via some program analysis isn't new.
For instance, some work~\cite{aakerblom2015measuring} ... .

More generally, there is an existing literature on analyzing usage patterns
of language features.  For instance, the dynamic features of Smalltalk were
analyzed in some work~\cite{callau2011howdevelopers}, ...

Other analysis work~\cite{milojkovic2017duck} ... .

\section{Conclusions and Future Work}

\bibliographystyle{boilerplate/ACM-Reference-Format}
\bibliography{bib/biblio,bib/jv,bib/r,bib/new}

\end{document}
