
\documentclass[sigplan,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings,xspace,wrapfig}
\lstset{language=R}
\usepackage{my_style}
\definecolor{LightGray}{rgb}{.95,.95,.95}
\definecolor{Gray}{rgb}{.3,.3,.3}
\definecolor{DarkGray}{rgb}{.5,.5,.5}

\graphicspath{ {./plots/} }

\lstset{ %
  columns=flexible,
  captionpos=b,
  frame=single,
  framerule=0pt,
  framexleftmargin=-1mm,
  framexrightmargin=-1mm,
  tabsize=2,
  belowskip=0pt,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{LightGray},
  emphstyle=\sffamily,
  keywordstyle=\bfseries,
  commentstyle=\color{Gray}\em,
  stringstyle=\color{Gray},
 % numbers=left
}

\lstdefinestyle{R}{ %
  language=R,
  deletekeywords={env, equal, c, runif, trace, args},
  breaklines=true
}

\lstdefinestyle{Rin}{ %
  style=R,
  numberstyle=none,
  basicstyle=\normalsize\ttfamily,
  breaklines=false
}

\newcommand{\code}[1]{\lstinline|#1|\xspace}
\newcommand{\genthat}{{\sc Genthat}\xspace}

\setcopyright{none}
\begin{document}

\title{A Type System for R: Empirically Designed and Evaluated}

\begin{abstract}

\AT{PLDI TODO:}

\begin{itemize}

\item complete propagatr to generate constraints

\item feed existing data through resulting constraints

\item finish the annotation and contract generation code

\item write the paper

\end{itemize}

\end{abstract}

\maketitle

%
%
%
%
%
%
\section{Introduction}

\begin{itemize}
  \item Adding types to dynamic languages is increasingly common.
  \item Types help write better code.
  \item We want to design a type system for R that is based on the patterns that we observe in current use of the language. 
  \item We also present a tool which takes a given package and synthesizes type signatures for package functions.
  \item This tool is tested by synthesizing annotations from package tests, examples, and vignettes, and running the code of the package's reverse dependencies (i.e. clients) to validate the type signatures.
  \item We find X, Y, Z.
\end{itemize}

The contributions of this paper are:

\begin{itemize}

\item our analysis data;
\item a type system based on this data;
\item a tool to infer types from package code, and;
\item an evaluation of the tool on \AT{all of} CRAN.

\end{itemize}

%
%
%
%
%
%
\section{The R Programming Language}
\label{sec:rlang}

\AT{This can probably be copied from the old version.
Is this somewhere we want to talk about R-dyntrace?}

%
%
%
%
%
%
\section{The Corpus}
\label{sec:corpus}

\AT{Probably talk about CRAN, and give some stats on the corpus that we worked with. 
This section probably doesn't have to be very long.}

\AT{Fit this in here:}
Unfortunately, there is not a whole lot of end-user R code available for analysis; typical R programs are effectively data pipelines, and many R users elect not to share their data, possibly because the data is proprietary or somehow confidential (e.g., census data), and these data pipeline programs are not very useful without the underlying data.
To alleviate this, we focused on the analysis of package test, example, and vignette code.

\AT{How do we want to argue that our data is valid and representative?}
End-user R scripts are hard to come by.
R is a data science language first-and-foremost, and so R programs are typically data analyses.
The nature of data is that, in many scenarios, it is confidential; imagine a government sharing sensitive information publicly, or a company divulging customer data.
For this reason, it is rare for R programmers to share their code, and even if they did, the code would not be particularly useful without the data underlying it.

Separately, idiomatic usage of R is to appeal to libraries to implement the main functionality of an analysis.
\AT{Probably want some good evidence of this.}
R packages are mainly sourced from the Comprehensive R Archive Network (CRAN), a large curated repository of R packages (libraries).
Of particular relevance to this work is a requirement that all packages should ship with tests, examples, and vignette programs to showcase how packages are intended to be used---indeed, these serve as a starting point for potential package clients.
We can leverage these programs to build up a picture of how programmers interact with packages, and thus of how programmers write R code for their own analyses. 

\AT{I think that talking avout how ``this is as good as it gets guys'' might be a bit of a defeatist way of looking at it.
But then again, I'm not sure what a good balance would be here.}

%
%
%
%
%
%
\section{Overview}

\AT{Somewhere, we need to overview the contribution, talking about the type system, about the tool, and about the data.}

Broadly, the contributions of this work can be divided into two.

First, we present a {\it large-scale analysis} of the type-use programming practices in R.
As mentioned in Section~\ref{sec:corpus}, we focused our efforts on package test, example, and vignette code.
Our tracing framework collects type information of function arguments and return values, and the precise details are elaborated in Section~\ref{sec:methodology}.

Unfortunately, in spite of the requirements imposed on CRAN packages, some package functions are not thoroughly tested and are poorly exercised.
Previous work in a similar vein (\AT{Genthat}) reported that their tool achieved roughly 70\% coverage of package code, which is good but far from perfect (and our tool \AT{is liable to achieve the same coverage}).
In order to further improve our data, we perform an additional phase of type propagation, where we use known type signatures of inner functions to improve the signatures of outer functions.
Essentially, we build a graph of the flows of argument values throughout a function, and infer constraints from that graph and feed our data into it; the precise details will be discussed further in Section~\ref{sec:methodology}.
The data itself will be presented in Section~\ref{sec:results}.

Besides helping us understand the R language, we also used the data to inform the {\it design of a type system} for the language.
Based on the common patterns observed in the analysis, we devised a type system for the language that \AT{maximizes the ``monomorphicity'' in the data} while capturing many of the idioms we observed.
For example, we often ran across function arguments which were exclusively inhabited by unit-length vectors of some primitive type; this led us to add a scalar version of primitive types to the type system, and include a rule akin to scalar\_primitive <: vector\_primitive to ensure that unit-length vectors are treated as such when there is evidence that the function is intended to accept vectors.
This will be elaborated further in Section~\ref{sec:results}. 

Now, in order to {\it validate} this type system, we implemented a framework for annotating R programs, as well as a tool which infers types from package example code, inserts lightweight runtime contracts into the code, and runs the example code in the package's reverse dependencies to try to unearth type errors.
The annotations are lightweight as they are only run {\it the first time} an argument is used, if it is used at all.
Details on and the results of the validation are available in Section~\ref{sec:results}.
Details on the process itself will be given in the next section.

%
%
%
%
\subsection{Annotations}
\label{subsec:annotations}

We extended the R language to support annotations on function arguments and returns.
This involved reserving syntax for the annotations, and modifying the R parser to parse and accept them.
\AT{What other stuff do we want to add to this? Other than presenting the syntax and stuff.}
The syntax allows programmers to specify the basic type, class, attributes, length, and parameterized types of arguments.
Also, we support type variables so as to allow programmers to specify large but common type signatures one time only.

To illustrate the annotation syntax, consider the code in Figure~\ref{fig:annotation-syntax}.
\AT{TODO: This figure.}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
my_fun <- function(@{int} x) {
  ...
}
\end{lstlisting}}\caption{Type annotation syntax.}\label{fig:annotation-syntax}\end{figure}

%
%
%
%
%
%
\section{Methodology}
\label{sec:methodology}

\AT{I think there are two distinct methodologies.}

\AT{First, we had to come up with the type system, and it partly based on things that we saw from our analysis.}

\AT{Second, we should talk about the evaluation of the tool.}

%
%
%
%
\subsection{Typr}

\AT{Are we going to change this? To make it run off of R-dyntrace?}

The first tool in our pipeline is Typr.
The goal of Typr is to gather type information on function invocations, and it is a modification of the Genthat~\cite{issta18} tool.
Genthat records all function invocations and builds a standalone trace to allow for independent reproduction of the call. 
We modified Genthat to instead record a wealth of information on argument and return values.
Typr records:

\begin{itemize}
\item the results of calling {\tt typeof}, {\tt class}, and {\tt attributes} on the argument;
\item the length (if the argument has a length);
\item if the argument is a list or data.frame we additionally reflect on the types of argument members;
\item if the argument is an environment, we reflect on the bindings in the environment.
\end{itemize}

The precise details of the approach are as follows:

\begin{enumerate}
\item for every package that is to be run, all functions (including those inherited from dependencies) are instrumented with on.exit hooks, code which runs when the function is finished executing.
The purpose of waiting for function exit is to ensure that argument promises are not forced, and argument values which are unused are recorded as such, as this indicates that they are likely used for some metaprogramming within the function.
\item package tests, examples, and vignettes are run, and decorated function invocations trigger the on.exit hook, which records argument and return value type information.
\item each new function trace (i.e., each call with a new call signature) is saved.
\end{enumerate}

\AT{At what point do we introduce the type system? I was gonna mention subtyping rules in a hypothetical item 4 where we consolidate trace info. Maybe we should just talk about how we looked at this data, and then add a new phase after?}

%
%
%
%
\subsection{Propagatr}

\AT{Explaining this succinctly will be important.}
After collecting and consolidating type information for function calls, we use our Propagatr tool to propagate known type information between functions that are used in similar circumstances.
In essence, our tool runs over the tests, examples, and vignettes, much like our Typer tool, but instead of collecting type information it tracks values as they flow through code and builds a graph connecting use-sites of values to the argument positions that they are drawn from.
\AT{Man, this sucks.}

For example, consider the code snippet in Figure~\ref{fig:propagatr-example}.

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
f <- function(x, y) {
  z <- x
  g(z, x)
}

f(0, 42)
\end{lstlisting}}\caption{Propagatr example.}\label{fig:propagatr-example}\end{figure}

Propagatr tracks the following data flow:

\begin{itemize}
\item \AT{This might be better achieved as a graph, or something.}
\item the assignment of x to z on line 2 is actually a function call in R; think of it as {\tt assign(x, z)}.
The flow of data here is from x to z, and thus in the remainder of the function both variables are known to share the same value.
\item The call to g(z, x) allows us to deduce that g takes two arguments {\it of the same type}. 
\end{itemize}

An important purpose of the Propagatr tool is to allow us to infer type signatures for poorly-exercised functions.
As a (somewhat degenerate) example, consider the code in Figure~\ref{fig:propagatr-degenerate}.
\AT{maybe shove this example somewhere to save space}

\begin{figure}[!hb]{\small\begin{lstlisting}[style=R]
add_wrapper <- function(a, b) { a + b }
\end{lstlisting}}\caption{Degenerate Propagatr example.}\label{fig:propagatr-degenerate}\end{figure}

Even if {\tt add\_wrapper} is rarely exercised, Propagatr deduces that it should have the same signature as R's builtin addition operator.

The precise details of the tool follow.
\AT{Pseudocode? Something like that? A formal description of the algorithm.}

%
%
%
%
\subsection{Infr}

Our Infr tool leverages our type system to test the effectiveness of package test, example, and vignette code at covering the possible use-cases of package functions.

The approach is as follows:

\begin{itemize}
\item Typr and Propagatr are run on the test, example, and vignette code of a particular package.
This serves to infer an annotation for all package functions.
\item Infr turns these annotations into dynamically-checked contracts on functions:
The contracts check that function argument and return values adhere to the type of their annotation.
These contracts are inserted directly into the promise code of the values, as will be elaborated in Section~\ref{subsubsec:promise}.
\item Then, the test, example, and vignette code for all reverse package dependencies (i.e., for all other packages which have the current package as a dependency) is run, and any type errors that are caught are recorded.
\end{itemize}

\AT{Do we want to do something special about data frames? We sort of already infer their type from the dynamic analysis.}

%
%
\subsubsection{I Promise I'll Do the Check!}
\label{subsubsec:promise}

As mentioned in Section~\ref{sec:rlang}, arguments to R functions are wrapped in promises whose code is only executed if the argument is used.
To insert checks that do not disrupt these semantics, we insert our checks {\it directly into the code of the promise}, which is done in C++ leveraging the Rcpp framework.

Essentially, the value of the e.g. argument promise is replaced by code which checks for compliance with the argument's annotation.
If the check fails, a call to R's stop function is triggered, yielding an informative error message (which includes the value itself, the function, package, and type that the value failed to adhere to).
If successful, the checking code is removed (by reassigning to the argument within the function), so subsequent accesses to the argument are fast.
\AT{Can return values be wrapped in the same way?}



%
%
%
%
%
%
\section{Results}
\label{sec:results}

\AT{What should go in here?}

\AT{There's the data itself, and our breakdown of it.
There's also the results of validating the types and the tool against the reverse dependencies.}

%
%
%
%
%
%
\section{Related Work}

\AT{Look at the stuff we put in the old version of the paper.}

%
%
%
%
\subsection{Studies of Dynamic Languages}

The idea of measuring polymorphism via some program analysis is not new.
For instance, some work~\cite{aakerblom2015measuring} measures the polymorphism of Python programs, but with a slightly different goal than ours: authors seek to assess the need for expressiveness in type systems that are retrofit onto dynamic languages.
Interestingly, they find that 4\% of function call sites are polymorphic, which is considerably less than what we have found with our analysis of R.
In essence, it would appear that R is far more polymorphic in Python.

More generally, there is an existing literature on analyzing usage patterns of language features.  
For instance, the dynamic features of Smalltalk were analyzed in some work~\cite{callau2011howdevelopers}, which aimed to see how often the highly dynamic capabilities of Smalltalk were used and in which kinds of projects.
Other work~\cite{pldi10a} explores how the dynamic features of JavaScript are used.
An interesting finding of theirs is that only 81\% of calls are monomorphic, which is very similar to what we observed in R.
This 80/20 split between monomorphism and polymorphism is echoed in other analysis work on the usage patterns of duck typing in Smalltalk~\cite{milojkovic2017duck}, which finds that ~20\% of functions are ``duck-typed'' (\AT{read: polymorphic}) in Smalltalk. 
\AT{It's notable that this 20\% shows up very often.}

%
%
%
%
\subsection{Synthesizing Types from Dynamic Analysis}

In this work, we have retrofit a type system onto the R programming language.
We based many of the decisions on which types and which rules to include on the results of our analysis.
This shares some similarity with work on so-called {\it trace typing}~\cite{andreasen2016trace}.

%
%
%
%
\subsection{Type Systems for Dynamic Languages}

The type system introduced in this work is a {\it gradual type system}.

%
%.
%
%
%
%
\section{Conclusion}



%
%
%
\bibliographystyle{boilerplate/ACM-Reference-Format}
\bibliography{bib/biblio,bib/jv,bib/r,bib/new,bib/gradual,bib/types4R}
\end{document}


