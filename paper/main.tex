\documentclass[acmsmall,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
\usepackage{booktabs,listings}
\lstset{language=R}
\usepackage{my_style}

\setcopyright{none}
%\setcopyright{acmcopyright}%\setcopyright{acmlicensed}
%\acmDOI{10.475/123_4}
%\acmConference[OOPSLAs]{Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}%\copyrightyear{2016}%\acmPrice{15.00}
\begin{document}

\title{A large-scale study of polymorphism in R}

\begin{abstract}
The R programming language is widely used in a variety of scientfic domains
for tasks related to data science. The language was designed to favor an
interactive style of programming with minimal syntactic and conceptual
overhead. This design is well suited to support interactive data analysis,
but is not well suited to generating performant code or catching programming
errors.  In particular, R has no type annotations and all operations are
dynamically checked at runtime. The starting point for our work is the
question: \emph{what could a static type system for R look like?}  To answer
that question we study the polymorphism that is present in over X millions
of lines of R code, XX packages, written over a period of Y years by Z
programmers.  We perform a dynamic analysis, leveraging tests and use-cases,
to determine the level of polymorphism that is present in the code. We do
this for several potential notions of types. Our result suggest that
polymorphism is important in some key parts of the system but that
relatively simple type annotations could be used to capture most of the
interesting cases.
\end{abstract}
\maketitle

\section{Introduction}

A number of programming languages have gained popularity for data science
tasks, they include MATLAB, Python, Julia and R.

These languages are all dynamically typed.

This is because they emphasize ease of programming, interactive data
exploration, and light-weight syntax.

To faciliate programming these language offer all kinds of short cuts and
facilities that increase the likelyhood that code will ``just work''.

For example abbreviation of name arguments and automatic conversions.

But invariably some applications written in those languages survive past the
prototype stage.

The results obtained are published and even sometimes drive policy decisions.

Given that code almost never stops, and that any computation is likely to
deliver a bunch of numbers, how can we trust those numbers?

Years of work in programming languages suggest that one cost efficient way
to catch some errors is to add types to a language.

In the last decade much attention has been given to the incremental addition
of types. Successes like Hack, TypeScript and Racket suggest that this may
be feasible for wide spectrum languages.


Our community is centred on building, improving, and reasoning about
programming languages.  A key component in all this is to truly
\textit{understand} the languages we're working on, so that we may make the
best decisions for the benefit of all users of a programming language.  In
many cases, we as researchers can appeal to our own intuition to at least
build a sense for how a language is used, as language users typically have
some training from an experienced programmer or software engineer (e.g.,
computer science studies, online tutorials written by language experts,
etc.).  \AT{We claim that this is emphatically not the case in R.}

R is a languages written by and for \textit{statisticians}.  It traces its
roots to the S language, developed by ... .  R programmers don't write big
servers or software systems; instead, they write and modify small data
processing pipelines as a means to facilitate interaction with digital data.
This leads to ``front-line'' R code (i.e., code written for users, not
programmers) not being made available on typical platforms like Github.
\AT{Maybe front-line code isn't a good word, is there a good way to describe
  non-library code?}  Put differently, R is essentially a really big
domain-specific language.

The design of R is also rife with questionable decisions.  For example, the
language is lazy \textit{and} side-effecting, which one could describe as an
antagonistic pair of features.  In addition, data structures must be
entirely copied when being modified, which appears to be far from ideal in a
language meant to process (thus, modify) big data.

In spite of these flaws, the language is incredibly popular.
\AT{We'll want to cite something here.}

One way or another, R is here to stay, so we may as well figure out how to
deal with it.  One aspect of this is understanding R programmers and how
they interact with the language, and a step towards this goal is to
understand \AT{how types arise in R programs.}  We will shed some light on
this by analyzing the usage of R functions, and build a picture of how
polymorphic R code is.

\begin{itemize}
    \item Understanding R can help us improve the language, as well as
      better equip us to build languages which have an appeal outside of the
      realm of computer science.
    \item We're gonna tackle this by looking at how R programmers use types
      and type information in their programs.
    \item We will be mindful of the runtime representations of values to
      give more context to the information we glean from programmer-visible
      type information.
\end{itemize}



%
\section{The R Programming Language}

R is a lazy, side-effecting, dynamically-typed, ... .

Typical usage of the language is ... . \AT{Do we have something to cite for
  this?}  \AT{Should we motivate why we only look at R packages?  Some
  people might wonder why we don't look at non-library code.}

Most R libraries (called \textit{packages}) are stored in the Comprehensive
R Archive Network (CRAN) and Bioconductor package repositories.  \AT{Talk
  about important packages? tidyverse?}

\AT{Kind of ... want to combine the above two paragraphs somehow? Together,
  they lead in nicely to the genthat talk.}


\subsection{The \texttt{genthat} Package}

Unfortunately, getting our hands on ``front-line'' R code is tricky: R is
not like many other languages, where large systems are built up and stored
in a repository such as Github.  Instead, R programmers are liable to
leverage a number of packages and work them into their data analysis
pipelines.

Interestingly, we are somewhat able to replicate this using only package
code.  CRAN, for instance, has a policy that packages made available on the
CRAN platform should be accompanied by a series of examples, tests, and/or
vignettes showing off how the code is intended to be used.  Nominally
unrelated, the {\tt genthat} project aims to trace this swath of example
code to generate new examples which package designers may not have thought
of.  Crucially, {\tt genthat} outputs complete trace information for each of
the functions that it runs so that they can be run again in a fresh R
instance (e.g., by capturing the random seed, environment variables, etc.).
We can leverage this trace information to glean type and attribute patterns
of functions, and since these traces correspond to examples of the packages
being used (rather than just arbitrarily running package code), we have a
reasonable approximation of how R programmers would use the packages.

\AT{We use/modify the {\tt genthat} tracer, probably worth going in to more
  detail here.}
%


\section{The Method}

In this section, we will detail our methodology for collecting data.

First, we leverage {\tt genthat}'s tracer to trace function executions.  The
idea here is that we don't really have access to non-library code written in
R, as general use patterns are (possibly?) to write small scripts which
analyze some bit of data and possibly visualize results.  The goal is likely
not to build big working systems, instead to explore data with by writing
and rewriting small scripts, ad infinitum.  In looking at package tests,
vignettes, and examples, we are painting a picture of how the package
designers intended their packages to be used, which we believe is a close
approximation of what R users would do.

{\tt genthat} generates a trace file which can be loaded into R and contains
all of the necessary information to rerun the tested function.  This
includes the random seed and all relevant environment data.  We utilize this
trace data to collect type information: we run each argument in the
specified environment, and collect type information through R's {\tt typeof}
function, and attribute information through R's {\tt attributes} function.

Then, we aggregate the information in all traces for a particular function.
If there was only one function trace, we discard the result as the function
is trivially monomorphic.  We can see which arguments had which types over
how many traces, and from here we can build a \textit{signature} for each
function argument, and consider them together to create a function
signature.  A \textbf{polymorphic argument} is one which has been inhabited
by values of at least two different types, and a \textbf{polymorphic
  function} is a function with at least one polymorphic argument or a
polymorphic return.

We repeat the above for each package on the Comprehensive R Archive Network
(CRAN), the premier source for R libraries (called packages), and the
Bioconductor package repository.  Now, depending on the data point we're
after, our methods from here differ slightly.

\begin{itemize}
    \item for \textbf{counting signatures}: for each signature, we count how
      many functions or arguments have the requisite signature;
    \item for \textbf{finding the most common signatures}: we enumerate as
      before, and sort the signatures based on how often they appear;
\end{itemize}

%
%
%
%
\subsection{Attributes}
\label{sec:method:attributes}

Types (in the sense of the result of a call to {\tt typeof}) are easy to
analyze as values can only have one type.  Attributes, however, are a
different story, as values can have an arbitrary number of attributes.
Essentially, we're not interesting by function arguments which have had
multiple attributes, we're interested in function arguments which have had
multiple attribute \textit{patterns}. \AT{is this defined earlier?}
Arguments are said to be polymorphic in attribute if they have been
inhabited with values which have had different attribute patterns.  \AT{We
  have not investigated whether functions which are polymorphic in attribute
  use attributes extensively inside the function code.}

\AT{Paragraph justifying our construction of an attribute pattern.
Either just name, or (name, type).}

\AT{Paragraph about so-called ``naturally-occurring'' attribute patterns.}


%
%
%
%
\subsection{{\tt genthat} Tracer Quirks}

First and foremost, {\tt genthat} captures arguments only on function exit,
so as to not force promises during function execution.  \AT{Uh oh:} One
clear limitation here is that if an argument \textit{gains} attributes over
the functione execution, then they will appear in our signatures.  As R is
side-effecting, this is an entirely valid practice (i.e., passing objects to
functions to modify them), though \AT{it's unclear if people actually do
  this}.

\AT{Maybe talk about the quirks of genthat in here?
For example, how it deals with default arguments, and stuff like that.}

%
%
%
%
\subsection{Goals}

Ultimately, our goal is to collect data and report on patterns which emerge
organically.  That said, the comprehensive picture of type usage we are
developing can also be used to inform the design of a set of \textit{type
  annotations} for the language, and this goal informs our analysis.  As we
collect our data, we keep in mind the idea that the data we produce should
clearly suggest what sorts of type annotations would reflect language usage
patterns.

%
%
%
%
\subsection{The Corpus}

\AT{I'm hesitant to go into too much detail here, as I'm not sure if we're quite done running our analysis.
I can imagine a scenario where we will try to rerun with a longer timeout just to get a little bit more data (because why not).}

We ran our analysis over a subset of CRAN, the Comprehensive R Archive Network.
We analyzed \AT{over 7500} packages, over half of all packages available on CRAN.
Our analysis had an explicit timeout of \AT{30 minutes} per package, as a number of R packages feature functions which have a nigh-eternal running time (e.g., machine learning packages and complex simulations) \AT{cite those packages/examples}.

\AT{Should we talk about some of the packages?
What else do we have to say about the corpus?
We can restate the numbers from the abstract.}

%
%
%
%
%
%
\section{Results}

\AT{Tables, graphs, code examples should go in here.}
In this section, we ... .

%
%
%
%
\subsection{Types in R}

\AT{may not be the correct place for this. i think i accidentally deleted it.}

R is a unityped language, and as such is trivially monomorphic, and this notion of monomorphism is far from useful.
We are interested in expanding the notion of ``type'' in R, and thankfully the language has some tools on offer.
\AT{this is discussed later on}

%
%
\subsubsection{What are possible type systems?}

Great question.
Some things:

\begin{itemize}

\item {\it The Obvious Type System (OTS)}: this type system corresponds directly with the type information that one can fetch using the {\tt typeof}, {\tt attributes}, and {\tt class} functions.
In this system, there is no distinction between vectors and scalars, and lists of differently-typed elements (e.g., a list of integers vs. a list of strings), and there is a distinction between integers and doubles, and the type of NULL and all other types.
This type system isn't particularly informative to a programmer, though it does reflect the treatment of values in the R runtime, for the {\tt typeof} a value is the type that the value has according to the R internals.

\item {\it The Fine-Grained Type System (FGTS)}: this type systems includes a number of relevant additions.
Chiefly, it distinguishes between vectors and scalars, as it is conceivable that boxing scalars as single-element vectors might be overkill.
In addition, list types are parameterized over the types of their elements (e.g., list<integer> vs. list<character>).
Finally, NAs ``in the wild'' are given the type {\tt raw\_NA}, as {\tt typeof(NA) = logical} in R.

\item {\it The Null-and-NA-Free Type System (NNTS)}: most languages consider NULLs to inhabit all types, and R does so for NAs \AT{elaborate}.
This gives us ... .

\end{itemize}

Of course, one could conceive of a more fine-grained type system than FGTS, but it's unclear that we would gain much in doing so.
Indeed, there is a space to explore in slightly less fine-grained type systems, which make modifications including:

\begin{itemize}

\item {\bf collapsing vectors and scalars}: this yields a type system slightly closer to OTS.

\item {\bf rolling NULL into other types}: in most languages, NULL is an inhabitant of all types, and this adjustment seems interesting.
Note that this goes beyond even OTS, as in R {\tt typeof(NULL) = NULL}.

\item {\bf rolling NA into other types}: \AT{i should try this}
similarly, NA is an inhabitant of all types, though NA standing alone has type {\tt logical}.
FGTS distinguishes such NAs with the {\tt raw\_NA} type, and if instead we consider NA to inhabit all types we may \AT{win}.

\item {\bf combining integers and doubles}: as in most languages, doubles and ints can stand-in for each other in nearly every scenario.
Even if not, it is trivial to convert from one to the other, particularly converting ints to doubles.
Aside, in R a double can be used to index a list, and R will floor it when performing the lookup.

\end{itemize}

%
%
%
%
\subsection{Usage Patterns}

In this section, we will discuss function usage patterns which arose in our analysis.
We will start by looking at the morphicity of functions.

\subsubsection{Function Argument and Return Morphicity}

First and foremost, we would like to know how often R programmers create polymorphic functions.
Recall that we define a polymorphic function to be a function with at least one polymorphic argument, or a polymorphic return.
We will turn our attention now to the data in \todo{Figure}, and go through each entry in the table.

Here, we see that the vast majority of function arguments are indeed monomorphic.
Monomorphic arguments are easy to annotate, as the {\tt typeof} the arguments is exactly the most precise annotation we could give, at least in terms of \textit{type}.
That said, types alone don't always paint the whole picture:
recall that \textit{attributes} are an R language feature which allows programmers to stick metadata onto values.
So in which circumstances \textit{do} types paint the whole picture?

The second data point in \textbf{TODO FIGURE} indicates that a
\isit{majority} of function arguments are monomorphic in type \textit{and}
have no attributes.  These represent arguments which are truly trivial to
annotate, as the {\tt typeof} an argument perfectly describes the usage of
that argument.  That said, some attributes arise naturally in R: For
instance, names in a named list (e.g. {\tt x} and {\tt y} in {\tt list(x=0,
  y=0)}) appear as an attribute on the value.  In
Section~\ref{sec:method:attributes}, we outlined these naturally-occurring
attribute patterns, and the third data point in \textbf{TODO FIGURE} shows
that a nontrivial amount of functions are monomorphic in type with said
natural attribute patterns.

Another facet of type information in R is in the {\tt class} attribute.
Recall that values have a \textit{class} in addition to a type: For example,
a {\tt data.frame} has type {\tt list} and class {\tt data.frame}.
\todo{Plug classes earlier} R has a number of built-in classes, such as
\todo{X, Y, and Z}, but users are free to redefine the class of any value at
runtime, and easily define new classes.  The next data point in
\todo{Figure} shows that user-defined classes don't appear altogether often,
though they do indeed feature.  \AT{In a following section, we will discuss
  how the usage of these classes manifests itself.}

\AT{Separate section for functions? Right now, this is at argument granularity.}

\subsubsection{Type Signatures}

In the last section, we presented a high-level overview of the morphicity ... .

\subsubsection{Attribute Signatures}

Recall that attributes are a way for programmers to store metadata on values in R.
What are the common attribute patterns?
And how often is the attribute pattern polymorphic?

\subsubsection{Takeaways}

\begin{itemize}
    \item the vast majority of arguments are monomorphic in type;
    \item of those, over 60\% have no attribute information;
    \item of the 40\% with attribute information, roughly 1/2 have fairly
      simple attributes corresponding to base R constructs (named lists and
      vectors, matrices, and data frames);
    \item now, of those arguments which are polymorphic, a sizable chunk (well over half) have defensible signatures (e.g., double and character for named list indexing, double and integer for obvious reasons, etc.).
\end{itemize}

In short, it looks like R (package) programmers are reasonable.  I'd
conjecture that a lot of the polymorphism (e.g., double and list) is coming
from how easy it is to use either type in a given situation (e.g.,
converting from vector of doubles to list or vice versa is simple).

%
%
%
%
%
%
\section{Synthesis}

In this section, we will discuss the conclusions that we draw from our data.


%
%
\subsection{Suggested Annotations}

\AT{How can we capture these patterns with annotations?}

Some possible annotations:

\begin{itemize}
    \item \textit{real}: for \textit{double} and \textit{integer} values
    \item \textit{function}: for \textit{closure}, \textit{special}, and \textit{builtin} values
    \item \textit{vector}: to indicate that something should be vectorized
    \item \textit{scalar}: to indicate that something should \textbf{not} be vectorized
    \item \textit{index}: for \textit{real} and \textit{character} values
\end{itemize}

%
%
\subsubsection{Struct-Like Attribute Declarations}

\AT{What's a convenient way to annotate attributes?  Should investigate how
  often attributes are consistent.}

%
\subsubsection{Coverage}

\AT{What is the coverage of these new annotations?}

\section{Conclusions and Future Work}

\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

\end{document}
